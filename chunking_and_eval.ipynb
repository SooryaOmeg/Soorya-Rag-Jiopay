{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd76abc",
   "metadata": {},
   "source": [
    "# Creating Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483f45d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages installed successfully!\n",
      "âœ… Required models downloaded!\n",
      "ðŸ“ Directory structure created:\n",
      "   data/\n",
      "   data/raw/\n",
      "   data/processed/\n",
      "   data/chunks/\n",
      "   data/chunks/fixed/\n",
      "   data/chunks/semantic/\n",
      "   data/chunks/structural/\n",
      "   data/chunks/recursive/\n",
      "   data/chunks/llm_based/\n",
      "   results/\n",
      "   results/ablations/\n",
      "   results/metrics/\n",
      "   models/\n",
      "   config/\n",
      "\n",
      "ðŸ” Verifying installation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All core libraries imported successfully!\n",
      "\n",
      "ðŸš€ Setup complete! Ready for chunking implementation.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Basic Setup, Installation, and Folder Creation\n",
    "# RAG Chatbot - JioPay Customer Support Chunking Implementation\n",
    "# =============================================================================\n",
    "\n",
    "# # Install required packages\n",
    "# !pip install -q transformers torch sentence-transformers\n",
    "# !pip install -q tiktoken google-generativeai\n",
    "# !pip install -q beautifulsoup4 requests\n",
    "# !pip install -q nltk spacy\n",
    "# !pip install -q scikit-learn numpy pandas\n",
    "# !pip install -q langchain langchain-google-genai\n",
    "\n",
    "# # Download required models/data\n",
    "# import nltk\n",
    "# nltk.download('punkt', quiet=True)\n",
    "# nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# # Download spacy model for text processing\n",
    "# !python -m spacy download en_core_web_sm --quiet\n",
    "\n",
    "# Create directory structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create main directories\n",
    "directories = [\n",
    "    'data',\n",
    "    'data/raw',           # Original scraped data\n",
    "    'data/processed',     # Cleaned data \n",
    "    'data/chunks',        # All chunking results\n",
    "    'data/chunks/fixed',  # Fixed chunking results\n",
    "    'data/chunks/semantic',   # Semantic chunking results\n",
    "    'data/chunks/structural', # Structural chunking results\n",
    "    'data/chunks/recursive',  # Recursive chunking results\n",
    "    'data/chunks/llm_based',  # LLM-based chunking results\n",
    "    'results',\n",
    "    'results/ablations',  # Ablation study results\n",
    "    'results/metrics',    # Evaluation metrics\n",
    "    'models',            # Downloaded/cached models\n",
    "    'config'             # Configuration files\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")\n",
    "print(\"âœ… Required models downloaded!\")\n",
    "print(\"ðŸ“ Directory structure created:\")\n",
    "for directory in directories:\n",
    "    print(f\"   {directory}/\")\n",
    "\n",
    "# Verify setup\n",
    "print(\"\\nðŸ” Verifying installation...\")\n",
    "try:\n",
    "    import transformers\n",
    "    import tiktoken  \n",
    "    import nltk\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import google.generativeai as genai\n",
    "    print(\"âœ… All core libraries imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "\n",
    "print(\"\\nðŸš€ Setup complete! Ready for chunking implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffa420",
   "metadata": {},
   "source": [
    "# Fixed Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf999b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded 139 documents from final.json\n",
      "ðŸ“Š Total documents loaded: 139\n",
      "ðŸ“„ First document: annexure_form_A.pdf\n",
      "ðŸ“ Content length: 4283 characters\n",
      "ðŸ“ˆ Total content: 637,794 characters, ~141,210 tokens\n",
      "\n",
      "ðŸ”„ Processing documents with fixed chunking strategies...\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Processing: Fixed_256_0\n",
      "   Chunk size: 256 tokens\n",
      "   Overlap: 0 tokens\n",
      "   Document 1: Generated 4 chunks\n",
      "   Document 2: Generated 2 chunks\n",
      "   Document 3: Generated 8 chunks\n",
      "   Document 4: Generated 20 chunks\n",
      "   Document 5: Generated 20 chunks\n",
      "   Document 6: Generated 25 chunks\n",
      "   Document 7: Generated 22 chunks\n",
      "   Document 8: Generated 3 chunks\n",
      "   Document 9: Generated 90 chunks\n",
      "   Document 10: Generated 5 chunks\n",
      "   Document 11: Generated 6 chunks\n",
      "   Document 12: Generated 6 chunks\n",
      "   Document 13: Generated 31 chunks\n",
      "   Document 14: Generated 4 chunks\n",
      "   Document 15: Generated 3 chunks\n",
      "   Document 16: Generated 2 chunks\n",
      "   Document 17: Generated 93 chunks\n",
      "   Document 18: Generated 3 chunks\n",
      "   Document 19: Generated 4 chunks\n",
      "   Document 20: Generated 4 chunks\n",
      "   Document 21: Generated 6 chunks\n",
      "   Document 22: Generated 5 chunks\n",
      "   Document 23: Generated 5 chunks\n",
      "   Document 24: Generated 4 chunks\n",
      "   Document 25: Generated 5 chunks\n",
      "   Document 26: Generated 5 chunks\n",
      "   Document 27: Generated 4 chunks\n",
      "   Document 28: Generated 5 chunks\n",
      "   Document 29: Generated 5 chunks\n",
      "   Document 30: Generated 5 chunks\n",
      "   Document 31: Generated 4 chunks\n",
      "   Document 32: Generated 4 chunks\n",
      "   Document 33: Generated 4 chunks\n",
      "   Document 34: Generated 4 chunks\n",
      "   Document 35: Generated 5 chunks\n",
      "   Document 36: Generated 5 chunks\n",
      "   Document 37: Generated 5 chunks\n",
      "   Document 38: Generated 4 chunks\n",
      "   Document 39: Generated 12 chunks\n",
      "   Document 40: Generated 70 chunks\n",
      "   Document 41: Generated 6 chunks\n",
      "   Document 42: Generated 17 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 1 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 1 chunks\n",
      "   Document 48: Generated 1 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 1 chunks\n",
      "   Document 81: Generated 1 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 1 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 2 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 642 chunks\n",
      "   â±ï¸  Processing time: 0.26s\n",
      "   ðŸ“Š Average tokens per chunk: 219.9\n",
      "   ðŸ’¾ Saved to: data/chunks/fixed/Fixed_256_0.json\n",
      "\n",
      "ðŸ“ Processing: Fixed_512_64\n",
      "   Chunk size: 512 tokens\n",
      "   Overlap: 64 tokens\n",
      "   Document 1: Generated 2 chunks\n",
      "   Document 2: Generated 1 chunks\n",
      "   Document 3: Generated 5 chunks\n",
      "   Document 4: Generated 12 chunks\n",
      "   Document 5: Generated 12 chunks\n",
      "   Document 6: Generated 14 chunks\n",
      "   Document 7: Generated 12 chunks\n",
      "   Document 8: Generated 2 chunks\n",
      "   Document 9: Generated 52 chunks\n",
      "   Document 10: Generated 3 chunks\n",
      "   Document 11: Generated 3 chunks\n",
      "   Document 12: Generated 3 chunks\n",
      "   Document 13: Generated 18 chunks\n",
      "   Document 14: Generated 3 chunks\n",
      "   Document 15: Generated 2 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 53 chunks\n",
      "   Document 18: Generated 2 chunks\n",
      "   Document 19: Generated 3 chunks\n",
      "   Document 20: Generated 2 chunks\n",
      "   Document 21: Generated 3 chunks\n",
      "   Document 22: Generated 3 chunks\n",
      "   Document 23: Generated 3 chunks\n",
      "   Document 24: Generated 3 chunks\n",
      "   Document 25: Generated 3 chunks\n",
      "   Document 26: Generated 3 chunks\n",
      "   Document 27: Generated 3 chunks\n",
      "   Document 28: Generated 3 chunks\n",
      "   Document 29: Generated 3 chunks\n",
      "   Document 30: Generated 3 chunks\n",
      "   Document 31: Generated 3 chunks\n",
      "   Document 32: Generated 3 chunks\n",
      "   Document 33: Generated 2 chunks\n",
      "   Document 34: Generated 3 chunks\n",
      "   Document 35: Generated 3 chunks\n",
      "   Document 36: Generated 3 chunks\n",
      "   Document 37: Generated 3 chunks\n",
      "   Document 38: Generated 3 chunks\n",
      "   Document 39: Generated 7 chunks\n",
      "   Document 40: Generated 40 chunks\n",
      "   Document 41: Generated 3 chunks\n",
      "   Document 42: Generated 10 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 1 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 1 chunks\n",
      "   Document 48: Generated 1 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 1 chunks\n",
      "   Document 81: Generated 1 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 1 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 1 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 415 chunks\n",
      "   â±ï¸  Processing time: 0.31s\n",
      "   ðŸ“Š Average tokens per chunk: 382.8\n",
      "   ðŸ’¾ Saved to: data/chunks/fixed/Fixed_512_64.json\n",
      "\n",
      "ðŸ“ Processing: Fixed_1024_128\n",
      "   Chunk size: 1024 tokens\n",
      "   Overlap: 128 tokens\n",
      "   Document 1: Generated 1 chunks\n",
      "   Document 2: Generated 1 chunks\n",
      "   Document 3: Generated 3 chunks\n",
      "   Document 4: Generated 6 chunks\n",
      "   Document 5: Generated 6 chunks\n",
      "   Document 6: Generated 7 chunks\n",
      "   Document 7: Generated 6 chunks\n",
      "   Document 8: Generated 1 chunks\n",
      "   Document 9: Generated 26 chunks\n",
      "   Document 10: Generated 2 chunks\n",
      "   Document 11: Generated 2 chunks\n",
      "   Document 12: Generated 2 chunks\n",
      "   Document 13: Generated 9 chunks\n",
      "   Document 14: Generated 1 chunks\n",
      "   Document 15: Generated 1 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 27 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 1 chunks\n",
      "   Document 20: Generated 1 chunks\n",
      "   Document 21: Generated 2 chunks\n",
      "   Document 22: Generated 2 chunks\n",
      "   Document 23: Generated 2 chunks\n",
      "   Document 24: Generated 1 chunks\n",
      "   Document 25: Generated 2 chunks\n",
      "   Document 26: Generated 2 chunks\n",
      "   Document 27: Generated 1 chunks\n",
      "   Document 28: Generated 2 chunks\n",
      "   Document 29: Generated 2 chunks\n",
      "   Document 30: Generated 2 chunks\n",
      "   Document 31: Generated 1 chunks\n",
      "   Document 32: Generated 1 chunks\n",
      "   Document 33: Generated 1 chunks\n",
      "   Document 34: Generated 1 chunks\n",
      "   Document 35: Generated 2 chunks\n",
      "   Document 36: Generated 2 chunks\n",
      "   Document 37: Generated 2 chunks\n",
      "   Document 38: Generated 1 chunks\n",
      "   Document 39: Generated 4 chunks\n",
      "   Document 40: Generated 20 chunks\n",
      "   Document 41: Generated 2 chunks\n",
      "   Document 42: Generated 5 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 1 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 1 chunks\n",
      "   Document 48: Generated 1 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 1 chunks\n",
      "   Document 81: Generated 1 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 1 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 1 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 262 chunks\n",
      "   â±ï¸  Processing time: 0.29s\n",
      "   ðŸ“Š Average tokens per chunk: 599.0\n",
      "   ðŸ’¾ Saved to: data/chunks/fixed/Fixed_1024_128.json\n",
      "\n",
      "============================================================\n",
      "âœ… Fixed chunking completed for all strategies!\n",
      "\n",
      "ðŸ“ˆ FIXED CHUNKING SUMMARY:\n",
      "================================================================================\n",
      "      Strategy  Chunk Size  Overlap  Total Chunks Avg Tokens/Chunk Processing Time (s)\n",
      "   Fixed_256_0         256        0           642            219.9                0.26\n",
      "  Fixed_512_64         512       64           415            382.8                0.31\n",
      "Fixed_1024_128        1024      128           262            599.0                0.29\n",
      "\n",
      "ðŸ’¾ Summary saved to: results/ablations/fixed_chunking_summary.csv\n",
      "\n",
      "ðŸ” SAMPLE CHUNKS (first chunk from each strategy):\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ Fixed_256_0:\n",
      "   Tokens: 256\n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME \n",
      "NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Fixed_512_64:\n",
      "   Tokens: 512\n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME \n",
      "NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Fixed_1024_128:\n",
      "   Tokens: 906\n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME \n",
      "NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not...\n",
      "   Source: annexure_form_A.pdf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Fixed Chunking Implementation\n",
    "# Strategy 1: Fixed-size chunks with specified token sizes and overlaps\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata for each chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    source_url: str\n",
    "    source_title: str\n",
    "    chunk_index: int\n",
    "    token_count: int\n",
    "    char_count: int\n",
    "    strategy: str\n",
    "    strategy_params: Dict\n",
    "\n",
    "class FixedChunker:\n",
    "    \"\"\"Implementation of fixed-size chunking with overlaps\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_by_tokens(self, text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into fixed-size chunks based on token count\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            chunk_size: Target size of each chunk in tokens\n",
    "            overlap: Number of overlapping tokens between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        # Encode text to tokens\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        if len(tokens) <= chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        while start_idx < len(tokens):\n",
    "            # Get chunk tokens\n",
    "            end_idx = min(start_idx + chunk_size, len(tokens))\n",
    "            chunk_tokens = tokens[start_idx:end_idx]\n",
    "            \n",
    "            # Decode back to text\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text.strip())\n",
    "            \n",
    "            # Move start position (accounting for overlap)\n",
    "            if end_idx >= len(tokens):\n",
    "                break\n",
    "            start_idx = end_idx - overlap\n",
    "            \n",
    "            # Ensure we don't get stuck in infinite loop\n",
    "            if start_idx <= 0:\n",
    "                start_idx = end_idx\n",
    "                \n",
    "        return chunks\n",
    "    \n",
    "    def process_document(self, doc: Dict, chunk_size: int, overlap: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document with fixed chunking\n",
    "        \n",
    "        Args:\n",
    "            doc: Document dictionary with url, title, content\n",
    "            chunk_size: Target chunk size in tokens\n",
    "            overlap: Overlap size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with metadata\n",
    "        \"\"\"\n",
    "        content = doc.get('content', '')\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        chunks = self.chunk_by_tokens(content, chunk_size, overlap)\n",
    "        \n",
    "        chunk_objects = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():  # Skip empty chunks\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{doc['url']}#chunk_{i}\",\n",
    "                    'source_url': doc['url'],\n",
    "                    'source_title': doc.get('title', ''),\n",
    "                    'content': chunk_text.strip(),\n",
    "                    'chunk_index': i,\n",
    "                    'token_count': self.count_tokens(chunk_text),\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'strategy': 'fixed',\n",
    "                    'strategy_params': {\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'overlap': overlap\n",
    "                    },\n",
    "                    'metadata': ChunkMetadata(\n",
    "                        chunk_id=f\"{doc['url']}#chunk_{i}\",\n",
    "                        source_url=doc['url'],\n",
    "                        source_title=doc.get('title', ''),\n",
    "                        chunk_index=i,\n",
    "                        token_count=self.count_tokens(chunk_text),\n",
    "                        char_count=len(chunk_text),\n",
    "                        strategy='fixed',\n",
    "                        strategy_params={'chunk_size': chunk_size, 'overlap': overlap}\n",
    "                    )\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects\n",
    "\n",
    "def load_scraped_data() -> List[Dict]:\n",
    "    \"\"\"Load your actual scraped JioPay data from final.json\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('final.json', 'r', encoding='utf-8') as f:\n",
    "            scraped_data = json.load(f)\n",
    "        \n",
    "        # Check if data needs format conversion\n",
    "        formatted_data = []\n",
    "        for item in scraped_data:\n",
    "            # Handle different possible formats\n",
    "            if 'content' in item:\n",
    "                # Already in correct format\n",
    "                formatted_item = item\n",
    "            elif 'text' in item:\n",
    "                # Convert 'text' field to 'content' field\n",
    "                formatted_item = {\n",
    "                    \"url\": item.get(\"url\", \"unknown\"),\n",
    "                    \"title\": item.get(\"title\", item.get(\"url\", \"unknown\")),\n",
    "                    \"content\": item[\"text\"]\n",
    "                }\n",
    "            else:\n",
    "                # Skip items without text content\n",
    "                continue\n",
    "                \n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded {len(formatted_data)} documents from final.json\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ final.json not found in current directory\")\n",
    "        print(\"ðŸ“ Please ensure final.json is in the same directory as this notebook\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ Error parsing final.json: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading final.json: {e}\")\n",
    "        return []\n",
    "\n",
    "# Initialize the fixed chunker\n",
    "fixed_chunker = FixedChunker()\n",
    "\n",
    "# Load scraped data from final.json\n",
    "documents = load_scraped_data()\n",
    "\n",
    "if not documents:\n",
    "    print(\"âŒ No documents loaded. Please check your final.json file.\")\n",
    "else:\n",
    "    print(f\"ðŸ“Š Total documents loaded: {len(documents)}\")\n",
    "    print(f\"ðŸ“„ First document: {documents[0]['title']}\")\n",
    "    print(f\"ðŸ“ Content length: {len(documents[0]['content'])} characters\")\n",
    "    \n",
    "    # Show total content stats\n",
    "    total_chars = sum(len(doc['content']) for doc in documents)\n",
    "    total_tokens = sum(fixed_chunker.count_tokens(doc['content']) for doc in documents)\n",
    "    print(f\"ðŸ“ˆ Total content: {total_chars:,} characters, ~{total_tokens:,} tokens\")\n",
    "\n",
    "# Define the three fixed chunking strategies as specified\n",
    "strategies = [\n",
    "    {\"chunk_size\": 256, \"overlap\": 0, \"name\": \"Fixed_256_0\"},\n",
    "    {\"chunk_size\": 512, \"overlap\": 64, \"name\": \"Fixed_512_64\"}, \n",
    "    {\"chunk_size\": 1024, \"overlap\": 128, \"name\": \"Fixed_1024_128\"}\n",
    "]\n",
    "\n",
    "# Process documents with each strategy\n",
    "results = {}\n",
    "\n",
    "print(\"\\nðŸ”„ Processing documents with fixed chunking strategies...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_name = strategy[\"name\"]\n",
    "    chunk_size = strategy[\"chunk_size\"]\n",
    "    overlap = strategy[\"overlap\"]\n",
    "    \n",
    "    print(f\"\\nðŸ“ Processing: {strategy_name}\")\n",
    "    print(f\"   Chunk size: {chunk_size} tokens\")\n",
    "    print(f\"   Overlap: {overlap} tokens\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        chunks = fixed_chunker.process_document(doc, chunk_size, overlap)\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"   Document {doc_idx + 1}: Generated {len(chunks)} chunks\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_chunks = len(all_chunks)\n",
    "    total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "    avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    results[strategy_name] = {\n",
    "        'chunks': all_chunks,\n",
    "        'total_chunks': total_chunks,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "        'processing_time': processing_time,\n",
    "        'strategy_params': strategy\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Generated {total_chunks} chunks\")\n",
    "    print(f\"   â±ï¸  Processing time: {processing_time:.2f}s\")\n",
    "    print(f\"   ðŸ“Š Average tokens per chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "    \n",
    "    # Save chunks to file\n",
    "    output_file = f\"data/chunks/fixed/{strategy_name}.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "    print(f\"   ðŸ’¾ Saved to: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Fixed chunking completed for all strategies!\")\n",
    "\n",
    "# Create summary comparison\n",
    "print(\"\\nðŸ“ˆ FIXED CHUNKING SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': name,\n",
    "        'Chunk Size': results[name]['strategy_params']['chunk_size'],\n",
    "        'Overlap': results[name]['strategy_params']['overlap'],\n",
    "        'Total Chunks': results[name]['total_chunks'],\n",
    "        'Avg Tokens/Chunk': f\"{results[name]['avg_tokens_per_chunk']:.1f}\",\n",
    "        'Processing Time (s)': f\"{results[name]['processing_time']:.2f}\"\n",
    "    }\n",
    "    for name in results.keys()\n",
    "])\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv('results/ablations/fixed_chunking_summary.csv', index=False)\n",
    "print(f\"\\nðŸ’¾ Summary saved to: results/ablations/fixed_chunking_summary.csv\")\n",
    "\n",
    "# Show sample chunks from each strategy\n",
    "print(\"\\nðŸ” SAMPLE CHUNKS (first chunk from each strategy):\")\n",
    "print(\"=\" * 80)\n",
    "for strategy_name, result in results.items():\n",
    "    if result['chunks']:\n",
    "        sample_chunk = result['chunks'][0]\n",
    "        print(f\"\\nðŸ“„ {strategy_name}:\")\n",
    "        print(f\"   Tokens: {sample_chunk['token_count']}\")\n",
    "        print(f\"   Content preview: {sample_chunk['content'][:200]}...\")\n",
    "        print(f\"   Source: {sample_chunk['source_title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706c7d6",
   "metadata": {},
   "source": [
    "# Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f99609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Semantic Chunker...\n",
      "ðŸ”„ Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 282\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Initialize semantic chunker\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Initializing Semantic Chunker...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m semantic_chunker = \u001b[43mSemanticChunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m    285\u001b[39m documents = load_scraped_data()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mSemanticChunker.__init__\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03mInitialize semantic chunker with embedding model\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    model_name: Sentence transformer model name\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ”„ Loading embedding model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer  \u001b[38;5;66;03m# From previous cell\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2184\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2181\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2183\u001b[39m \u001b[38;5;66;03m# Load the modules of sentence transformer\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m modules_json_path = \u001b[43mload_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2186\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodules.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(modules_json_path, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[32m   2193\u001b[39m     modules_config = json.load(fIn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\sentence_transformers\\util\\file_io.py:96\u001b[39m, in \u001b[36mload_file_path\u001b[39m\u001b[34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[39m\n\u001b[32m     94\u001b[39m file_path = Path(subfolder, filename)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1073\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1097\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1099\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1544\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1545\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1550\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1551\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1460\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1474\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hf_raise_for_status(response)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Main Files\\Soorya\\7thSem\\LLM_Prod\\Exp2\\venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Anaconda3\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Semantic Chunking Implementation\n",
    "# Strategy 2: Semantic-based chunking using sentence embeddings and similarity\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "class SemanticChunker:\n",
    "    \"\"\"Implementation of semantic chunking using sentence embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize semantic chunker with embedding model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Sentence transformer model name\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ”„ Loading embedding model: {model_name}\")\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer  # From previous cell\n",
    "        print(f\"âœ… Embedding model loaded successfully\")\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using NLTK\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Clean and filter sentences\n",
    "        cleaned_sentences = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if len(sent) > 10:  # Filter out very short sentences\n",
    "                cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "    \n",
    "    def compute_sentence_embeddings(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute embeddings for list of sentences\"\"\"\n",
    "        if not sentences:\n",
    "            return np.array([])\n",
    "        return self.embedding_model.encode(sentences)\n",
    "    \n",
    "    def compute_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute cosine similarity matrix between sentence embeddings\"\"\"\n",
    "        if len(embeddings) == 0:\n",
    "            return np.array([])\n",
    "        return cosine_similarity(embeddings)\n",
    "    \n",
    "    def find_semantic_boundaries(self, similarities: np.ndarray, threshold: float = 0.7) -> List[int]:\n",
    "        \"\"\"\n",
    "        Find semantic boundaries based on similarity drops\n",
    "        \n",
    "        Args:\n",
    "            similarities: Similarity matrix between consecutive sentences\n",
    "            threshold: Similarity threshold below which to create boundaries\n",
    "            \n",
    "        Returns:\n",
    "            List of sentence indices where semantic boundaries occur\n",
    "        \"\"\"\n",
    "        if len(similarities) <= 1:\n",
    "            return []\n",
    "        \n",
    "        boundaries = [0]  # Always start with first sentence\n",
    "        \n",
    "        # Look at consecutive sentence similarities\n",
    "        for i in range(len(similarities) - 1):\n",
    "            # Similarity between current and next sentence\n",
    "            current_similarity = similarities[i][i + 1]\n",
    "            \n",
    "            # If similarity drops below threshold, create boundary\n",
    "            if current_similarity < threshold:\n",
    "                boundaries.append(i + 1)\n",
    "        \n",
    "        # Always end with last sentence index\n",
    "        if boundaries[-1] != len(similarities):\n",
    "            boundaries.append(len(similarities))\n",
    "            \n",
    "        return boundaries\n",
    "    \n",
    "    def merge_small_chunks(self, chunks: List[str], min_size: int = 100, max_size: int = 1500) -> List[str]:\n",
    "        \"\"\"\n",
    "        Merge chunks that are too small and split chunks that are too large\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text chunks\n",
    "            min_size: Minimum chunk size in tokens\n",
    "            max_size: Maximum chunk size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of adjusted chunks\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        adjusted_chunks = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            current_chunk = chunks[i]\n",
    "            current_tokens = self.tokenizer.encode(current_chunk)\n",
    "            \n",
    "            # If chunk is too small, try to merge with next chunk\n",
    "            if len(current_tokens) < min_size and i + 1 < len(chunks):\n",
    "                next_chunk = chunks[i + 1]\n",
    "                merged = current_chunk + \" \" + next_chunk\n",
    "                merged_tokens = self.tokenizer.encode(merged)\n",
    "                \n",
    "                # If merged chunk is not too large, use it\n",
    "                if len(merged_tokens) <= max_size:\n",
    "                    adjusted_chunks.append(merged)\n",
    "                    i += 2  # Skip next chunk as it's been merged\n",
    "                else:\n",
    "                    # Keep original chunk even if small\n",
    "                    adjusted_chunks.append(current_chunk)\n",
    "                    i += 1\n",
    "            \n",
    "            # If chunk is too large, split it using fixed chunking\n",
    "            elif len(current_tokens) > max_size:\n",
    "                # Split large chunk into smaller pieces\n",
    "                sub_chunks = self.split_large_chunk(current_chunk, max_size)\n",
    "                adjusted_chunks.extend(sub_chunks)\n",
    "                i += 1\n",
    "            \n",
    "            else:\n",
    "                # Chunk size is acceptable\n",
    "                adjusted_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "        \n",
    "        return adjusted_chunks\n",
    "    \n",
    "    def split_large_chunk(self, text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"Split a large chunk into smaller chunks\"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return [text]\n",
    "        \n",
    "        # Use simple token-based splitting for large chunks\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            end = min(start + max_tokens, len(tokens))\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text.strip())\n",
    "            start = end\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, similarity_threshold: float = 0.7,\n",
    "                           min_chunk_size: int = 100, max_chunk_size: int = 1500) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform semantic chunking on text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            similarity_threshold: Threshold for semantic similarity\n",
    "            min_chunk_size: Minimum chunk size in tokens\n",
    "            max_chunk_size: Maximum chunk size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of semantically coherent chunks\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        # Step 1: Split into sentences\n",
    "        sentences = self.split_into_sentences(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return [text] if text.strip() else []\n",
    "        \n",
    "        # Step 2: Compute sentence embeddings\n",
    "        embeddings = self.compute_sentence_embeddings(sentences)\n",
    "        \n",
    "        # Step 3: Compute similarity matrix\n",
    "        similarity_matrix = self.compute_similarity_matrix(embeddings)\n",
    "        \n",
    "        # Step 4: Find semantic boundaries\n",
    "        boundaries = self.find_semantic_boundaries(similarity_matrix, similarity_threshold)\n",
    "        \n",
    "        # Step 5: Create chunks based on boundaries\n",
    "        chunks = []\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            start_idx = boundaries[i]\n",
    "            end_idx = boundaries[i + 1]\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "            chunk_text = \" \".join(chunk_sentences)\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(chunk_text.strip())\n",
    "        \n",
    "        # Step 6: Adjust chunk sizes\n",
    "        adjusted_chunks = self.merge_small_chunks(chunks, min_chunk_size, max_chunk_size)\n",
    "        \n",
    "        return adjusted_chunks\n",
    "    \n",
    "    def process_document(self, doc: Dict, similarity_threshold: float = 0.7,\n",
    "                        min_chunk_size: int = 100, max_chunk_size: int = 1500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document with semantic chunking\n",
    "        \n",
    "        Args:\n",
    "            doc: Document dictionary with url, title, content\n",
    "            similarity_threshold: Similarity threshold for chunking\n",
    "            min_chunk_size: Minimum chunk size in tokens\n",
    "            max_chunk_size: Maximum chunk size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with metadata\n",
    "        \"\"\"\n",
    "        content = doc.get('content', '')\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        chunks = self.semantic_chunk_text(content, similarity_threshold, min_chunk_size, max_chunk_size)\n",
    "        \n",
    "        chunk_objects = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{doc['url']}#semantic_chunk_{i}\",\n",
    "                    'source_url': doc['url'],\n",
    "                    'source_title': doc.get('title', ''),\n",
    "                    'content': chunk_text.strip(),\n",
    "                    'chunk_index': i,\n",
    "                    'token_count': len(self.tokenizer.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'strategy': 'semantic',\n",
    "                    'strategy_params': {\n",
    "                        'similarity_threshold': similarity_threshold,\n",
    "                        'min_chunk_size': min_chunk_size,\n",
    "                        'max_chunk_size': max_chunk_size,\n",
    "                        'embedding_model': self.model_name\n",
    "                    }\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects\n",
    "\n",
    "def load_scraped_data() -> List[Dict]:\n",
    "    \"\"\"Load scraped data from final.json\"\"\"\n",
    "    try:\n",
    "        with open('final.json', 'r', encoding='utf-8') as f:\n",
    "            scraped_data = json.load(f)\n",
    "        \n",
    "        formatted_data = []\n",
    "        for item in scraped_data:\n",
    "            if 'content' in item:\n",
    "                formatted_item = item\n",
    "            elif 'text' in item:\n",
    "                formatted_item = {\n",
    "                    \"url\": item.get(\"url\", \"unknown\"),\n",
    "                    \"title\": item.get(\"title\", item.get(\"url\", \"unknown\")),\n",
    "                    \"content\": item[\"text\"]\n",
    "                }\n",
    "            else:\n",
    "                continue\n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        return formatted_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ final.json not found\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading final.json: {e}\")\n",
    "        return []\n",
    "\n",
    "# Initialize semantic chunker\n",
    "print(\"ðŸš€ Initializing Semantic Chunker...\")\n",
    "semantic_chunker = SemanticChunker(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load data\n",
    "documents = load_scraped_data()\n",
    "\n",
    "if not documents:\n",
    "    print(\"âŒ No documents loaded. Please check your final.json file.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Loaded {len(documents)} documents for semantic chunking\")\n",
    "    \n",
    "    # Define semantic chunking configurations to test\n",
    "    semantic_configs = [\n",
    "        {\n",
    "            \"name\": \"Semantic_High_Sim\", \n",
    "            \"similarity_threshold\": 0.8, \n",
    "            \"min_size\": 100, \n",
    "            \"max_size\": 1200\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Semantic_Med_Sim\", \n",
    "            \"similarity_threshold\": 0.7, \n",
    "            \"min_size\": 150, \n",
    "            \"max_size\": 1500\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Semantic_Low_Sim\", \n",
    "            \"similarity_threshold\": 0.6, \n",
    "            \"min_size\": 200, \n",
    "            \"max_size\": 1800\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process documents with each semantic configuration\n",
    "    semantic_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ”„ Processing documents with semantic chunking strategies...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for config in semantic_configs:\n",
    "        config_name = config[\"name\"]\n",
    "        threshold = config[\"similarity_threshold\"]\n",
    "        min_size = config[\"min_size\"]\n",
    "        max_size = config[\"max_size\"]\n",
    "        \n",
    "        print(f\"\\nðŸ“ Processing: {config_name}\")\n",
    "        print(f\"   Similarity threshold: {threshold}\")\n",
    "        print(f\"   Min size: {min_size} tokens\")\n",
    "        print(f\"   Max size: {max_size} tokens\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            chunks = semantic_chunker.process_document(\n",
    "                doc, threshold, min_size, max_size\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"   Document {doc_idx + 1}: Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chunks = len(all_chunks)\n",
    "        total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "        avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        semantic_results[config_name] = {\n",
    "            'chunks': all_chunks,\n",
    "            'total_chunks': total_chunks,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "            'processing_time': processing_time,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Generated {total_chunks} chunks\")\n",
    "        print(f\"   â±ï¸  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"   ðŸ“Š Average tokens per chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "        \n",
    "        # Save chunks to file\n",
    "        output_file = f\"data/chunks/semantic/{config_name}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"   ðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… Semantic chunking completed for all configurations!\")\n",
    "    \n",
    "    # Create summary comparison\n",
    "    print(\"\\nðŸ“ˆ SEMANTIC CHUNKING SUMMARY:\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    semantic_summary = []\n",
    "    for name, result in semantic_results.items():\n",
    "        config = result['config']\n",
    "        semantic_summary.append({\n",
    "            'Strategy': name,\n",
    "            'Similarity Threshold': config['similarity_threshold'],\n",
    "            'Min Size': config['min_size'],\n",
    "            'Max Size': config['max_size'],\n",
    "            'Total Chunks': result['total_chunks'],\n",
    "            'Avg Tokens/Chunk': f\"{result['avg_tokens_per_chunk']:.1f}\",\n",
    "            'Processing Time (s)': f\"{result['processing_time']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    semantic_df = pd.DataFrame(semantic_summary)\n",
    "    print(semantic_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    semantic_df.to_csv('results/ablations/semantic_chunking_summary.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Summary saved to: results/ablations/semantic_chunking_summary.csv\")\n",
    "    \n",
    "    # Show sample chunks\n",
    "    print(\"\\nðŸ” SAMPLE CHUNKS (first chunk from each strategy):\")\n",
    "    print(\"=\" * 90)\n",
    "    for strategy_name, result in semantic_results.items():\n",
    "        if result['chunks']:\n",
    "            sample_chunk = result['chunks'][0]\n",
    "            print(f\"\\nðŸ“„ {strategy_name}:\")\n",
    "            print(f\"   Tokens: {sample_chunk['token_count']}\")\n",
    "            print(f\"   Similarity threshold: {result['config']['similarity_threshold']}\")\n",
    "            print(f\"   Content preview: {sample_chunk['content'][:200]}...\")\n",
    "            print(f\"   Source: {sample_chunk['source_title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1a478",
   "metadata": {},
   "source": [
    "# Structural Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Structural Chunker...\n",
      "âœ… Structural chunker initialized\n",
      "\n",
      "âœ… Loaded 139 documents for structural chunking\n",
      "\n",
      "ðŸ”„ Processing documents with structural chunking strategies...\n",
      "===========================================================================\n",
      "\n",
      "ðŸ“ Processing: Structural_Hierarchical\n",
      "   Preserve hierarchy: True\n",
      "   Min tokens: 100\n",
      "   Max tokens: 1200\n",
      "   Document 1: Generated 2 chunks\n",
      "   Document 2: Generated 5 chunks\n",
      "   Document 3: Generated 14 chunks\n",
      "   Document 4: Generated 8 chunks\n",
      "   Document 5: Generated 8 chunks\n",
      "   Document 6: Generated 5 chunks\n",
      "   Document 7: Generated 16 chunks\n",
      "   Document 8: Generated 3 chunks\n",
      "   Document 9: Generated 33 chunks\n",
      "   Document 10: Generated 6 chunks\n",
      "   Document 11: Generated 4 chunks\n",
      "   Document 12: Generated 4 chunks\n",
      "   Document 13: Generated 26 chunks\n",
      "   Document 14: Generated 4 chunks\n",
      "   Document 15: Generated 23 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 28 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 2 chunks\n",
      "   Document 20: Generated 2 chunks\n",
      "   Document 21: Generated 2 chunks\n",
      "   Document 22: Generated 2 chunks\n",
      "   Document 23: Generated 2 chunks\n",
      "   Document 24: Generated 2 chunks\n",
      "   Document 25: Generated 2 chunks\n",
      "   Document 26: Generated 2 chunks\n",
      "   Document 27: Generated 2 chunks\n",
      "   Document 28: Generated 2 chunks\n",
      "   Document 29: Generated 2 chunks\n",
      "   Document 30: Generated 2 chunks\n",
      "   Document 31: Generated 2 chunks\n",
      "   Document 32: Generated 2 chunks\n",
      "   Document 33: Generated 2 chunks\n",
      "   Document 34: Generated 2 chunks\n",
      "   Document 35: Generated 2 chunks\n",
      "   Document 36: Generated 2 chunks\n",
      "   Document 37: Generated 2 chunks\n",
      "   Document 38: Generated 2 chunks\n",
      "   Document 39: Generated 3 chunks\n",
      "   Document 40: Generated 13 chunks\n",
      "   Document 41: Generated 2 chunks\n",
      "   Document 42: Generated 4 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 2 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 2 chunks\n",
      "   Document 48: Generated 2 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 3 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 2 chunks\n",
      "   Document 69: Generated 3 chunks\n",
      "   Document 70: Generated 3 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 2 chunks\n",
      "   Document 74: Generated 3 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 3 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 1 chunks\n",
      "   Document 81: Generated 1 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 4 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 2 chunks\n",
      "   Document 90: Generated 3 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 4 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 2 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 3 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 2 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 3 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 380 chunks\n",
      "   â±ï¸  Processing time: 0.30s\n",
      "   ðŸ“Š Average tokens per chunk: 227.7\n",
      "   ðŸ—ï¸  Structure types: ['caps_heading', 'header_merged', 'header', 'alphabetic_merged', 'alphabetic', 'caps_heading_merged', 'numbered_section', 'numbered_section_merged', 'numbered_section_split', 'caps_heading_split', 'bullet_merged', 'topic_based', 'header_split', 'bullet', 'roman_merged', 'no_structure']\n",
      "   ðŸ“‹ Topics found: ['general', 'kyc_documents', 'merchant_onboarding', 'payment_processing', 'technical_requirements', 'bank_account']\n",
      "   ðŸ’¾ Saved to: data/chunks/structural/Structural_Hierarchical.json\n",
      "\n",
      "ðŸ“ Processing: Structural_Balanced\n",
      "   Preserve hierarchy: True\n",
      "   Min tokens: 150\n",
      "   Max tokens: 1500\n",
      "   Document 1: Generated 2 chunks\n",
      "   Document 2: Generated 5 chunks\n",
      "   Document 3: Generated 14 chunks\n",
      "   Document 4: Generated 7 chunks\n",
      "   Document 5: Generated 7 chunks\n",
      "   Document 6: Generated 5 chunks\n",
      "   Document 7: Generated 16 chunks\n",
      "   Document 8: Generated 3 chunks\n",
      "   Document 9: Generated 31 chunks\n",
      "   Document 10: Generated 6 chunks\n",
      "   Document 11: Generated 4 chunks\n",
      "   Document 12: Generated 4 chunks\n",
      "   Document 13: Generated 26 chunks\n",
      "   Document 14: Generated 3 chunks\n",
      "   Document 15: Generated 23 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 24 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 2 chunks\n",
      "   Document 20: Generated 2 chunks\n",
      "   Document 21: Generated 2 chunks\n",
      "   Document 22: Generated 2 chunks\n",
      "   Document 23: Generated 2 chunks\n",
      "   Document 24: Generated 2 chunks\n",
      "   Document 25: Generated 2 chunks\n",
      "   Document 26: Generated 2 chunks\n",
      "   Document 27: Generated 2 chunks\n",
      "   Document 28: Generated 2 chunks\n",
      "   Document 29: Generated 2 chunks\n",
      "   Document 30: Generated 2 chunks\n",
      "   Document 31: Generated 2 chunks\n",
      "   Document 32: Generated 2 chunks\n",
      "   Document 33: Generated 2 chunks\n",
      "   Document 34: Generated 2 chunks\n",
      "   Document 35: Generated 2 chunks\n",
      "   Document 36: Generated 2 chunks\n",
      "   Document 37: Generated 2 chunks\n",
      "   Document 38: Generated 2 chunks\n",
      "   Document 39: Generated 3 chunks\n",
      "   Document 40: Generated 11 chunks\n",
      "   Document 41: Generated 2 chunks\n",
      "   Document 42: Generated 3 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 2 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 2 chunks\n",
      "   Document 48: Generated 2 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 3 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 2 chunks\n",
      "   Document 69: Generated 3 chunks\n",
      "   Document 70: Generated 3 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 2 chunks\n",
      "   Document 74: Generated 3 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 3 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 1 chunks\n",
      "   Document 81: Generated 1 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 4 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 2 chunks\n",
      "   Document 90: Generated 3 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 4 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 2 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 3 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 2 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 3 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 368 chunks\n",
      "   â±ï¸  Processing time: 0.32s\n",
      "   ðŸ“Š Average tokens per chunk: 235.1\n",
      "   ðŸ—ï¸  Structure types: ['caps_heading', 'header_merged', 'header', 'alphabetic_merged', 'alphabetic', 'caps_heading_merged', 'numbered_section', 'numbered_section_merged', 'numbered_section_split', 'caps_heading_split', 'bullet_merged', 'topic_based', 'header_split', 'bullet', 'roman_merged', 'no_structure']\n",
      "   ðŸ“‹ Topics found: ['general', 'kyc_documents', 'merchant_onboarding', 'payment_processing', 'technical_requirements', 'bank_account']\n",
      "   ðŸ’¾ Saved to: data/chunks/structural/Structural_Balanced.json\n",
      "\n",
      "ðŸ“ Processing: Structural_Large\n",
      "   Preserve hierarchy: False\n",
      "   Min tokens: 200\n",
      "   Max tokens: 1800\n",
      "   Document 1: Generated 2 chunks\n",
      "   Document 2: Generated 8 chunks\n",
      "   Document 3: Generated 24 chunks\n",
      "   Document 4: Generated 10 chunks\n",
      "   Document 5: Generated 10 chunks\n",
      "   Document 6: Generated 6 chunks\n",
      "   Document 7: Generated 30 chunks\n",
      "   Document 8: Generated 5 chunks\n",
      "   Document 9: Generated 35 chunks\n",
      "   Document 10: Generated 10 chunks\n",
      "   Document 11: Generated 8 chunks\n",
      "   Document 12: Generated 5 chunks\n",
      "   Document 13: Generated 43 chunks\n",
      "   Document 14: Generated 5 chunks\n",
      "   Document 15: Generated 46 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 22 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 2 chunks\n",
      "   Document 20: Generated 2 chunks\n",
      "   Document 21: Generated 2 chunks\n",
      "   Document 22: Generated 2 chunks\n",
      "   Document 23: Generated 2 chunks\n",
      "   Document 24: Generated 2 chunks\n",
      "   Document 25: Generated 2 chunks\n",
      "   Document 26: Generated 2 chunks\n",
      "   Document 27: Generated 2 chunks\n",
      "   Document 28: Generated 2 chunks\n",
      "   Document 29: Generated 2 chunks\n",
      "   Document 30: Generated 2 chunks\n",
      "   Document 31: Generated 2 chunks\n",
      "   Document 32: Generated 2 chunks\n",
      "   Document 33: Generated 2 chunks\n",
      "   Document 34: Generated 2 chunks\n",
      "   Document 35: Generated 3 chunks\n",
      "   Document 36: Generated 2 chunks\n",
      "   Document 37: Generated 2 chunks\n",
      "   Document 38: Generated 2 chunks\n",
      "   Document 39: Generated 2 chunks\n",
      "   Document 40: Generated 2 chunks\n",
      "   Document 41: Generated 2 chunks\n",
      "   Document 42: Generated 2 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 3 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 3 chunks\n",
      "   Document 48: Generated 3 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 2 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 5 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 2 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 4 chunks\n",
      "   Document 69: Generated 5 chunks\n",
      "   Document 70: Generated 6 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 3 chunks\n",
      "   Document 74: Generated 6 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 5 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 2 chunks\n",
      "   Document 81: Generated 2 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 7 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 4 chunks\n",
      "   Document 90: Generated 5 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 8 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 3 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 6 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 3 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 2 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 5 chunks\n",
      "   Document 135: Generated 2 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 489 chunks\n",
      "   â±ï¸  Processing time: 0.14s\n",
      "   ðŸ“Š Average tokens per chunk: 175.8\n",
      "   ðŸ—ï¸  Structure types: ['caps_heading', 'header', 'alphabetic', 'numbered_section', 'bullet', 'topic_based', 'roman', 'no_structure']\n",
      "   ðŸ“‹ Topics found: ['general', 'kyc_documents', 'payment_processing', 'merchant_onboarding', 'legal_declaration', 'technical_requirements', 'bank_account']\n",
      "   ðŸ’¾ Saved to: data/chunks/structural/Structural_Large.json\n",
      "\n",
      "===========================================================================\n",
      "âœ… Structural chunking completed for all configurations!\n",
      "\n",
      "ðŸ“ˆ STRUCTURAL CHUNKING SUMMARY:\n",
      "====================================================================================================\n",
      "               Strategy  Preserve Hierarchy  Min Tokens  Max Tokens  Total Chunks Avg Tokens/Chunk Processing Time (s)  Structure Types\n",
      "Structural_Hierarchical                True         100        1200           380            227.7                0.30               16\n",
      "    Structural_Balanced                True         150        1500           368            235.1                0.32               16\n",
      "       Structural_Large               False         200        1800           489            175.8                0.14                8\n",
      "\n",
      "ðŸ’¾ Summary saved to: results/ablations/structural_chunking_summary.csv\n",
      "\n",
      "ðŸ” SAMPLE CHUNKS WITH STRUCTURE INFO:\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“„ Structural_Hierarchical:\n",
      "   Tokens: 160\n",
      "   Structure Type: caps_heading\n",
      "   Level: 2\n",
      "   Heading: NOT MENTIONED ON PROOF OF BUSINESS\n",
      "   Content preview: NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not mention on Proof \n",
      "of business \n",
      " This Declaration to be valid only in case of Sole P...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Structural_Balanced:\n",
      "   Tokens: 160\n",
      "   Structure Type: caps_heading\n",
      "   Level: 2\n",
      "   Heading: NOT MENTIONED ON PROOF OF BUSINESS\n",
      "   Content preview: NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not mention on Proof \n",
      "of business \n",
      " This Declaration to be valid only in case of Sole P...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Structural_Large:\n",
      "   Tokens: 160\n",
      "   Structure Type: caps_heading\n",
      "   Level: 2\n",
      "   Heading: NOT MENTIONED ON PROOF OF BUSINESS\n",
      "   Content preview: NOT MENTIONED ON PROOF OF BUSINESS \n",
      " \n",
      "JPSL Merchant declaration Requirement where owner name or business name is not mention on Proof \n",
      "of business \n",
      " This Declaration to be valid only in case of Sole P...\n",
      "   Source: annexure_form_A.pdf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Structural Chunking Implementation (FIXED)\n",
    "# Strategy 3: Structure-based chunking using headings, HTML tags, and document hierarchy\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Mock tokenizer for demonstration - replace with your actual tokenizer\n",
    "class MockTokenizer:\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Simple mock tokenizer - replace with actual tokenizer\"\"\"\n",
    "        return text.split()  # Simple word-based tokenization\n",
    "    \n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"Simple mock decoder\"\"\"\n",
    "        return \" \".join(str(t) for t in tokens)\n",
    "\n",
    "# Initialize mock tokenizer - replace with your actual tokenizer\n",
    "tokenizer = MockTokenizer()\n",
    "\n",
    "class StructuralChunker:\n",
    "    \"\"\"Implementation of structural chunking using headings and HTML structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize structural chunker\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Define structural patterns to look for\n",
    "        self.heading_patterns = [\n",
    "            r'^#{1,6}\\s+(.+)$',  # Markdown headings (# ## ### etc.)\n",
    "            r'^(.+)\\n={3,}$',    # Underlined headings with ===\n",
    "            r'^(.+)\\n-{3,}$',    # Underlined headings with ---\n",
    "            r'^\\d+\\.\\s+(.+)$',   # Numbered sections (1. 2. 3.)\n",
    "            r'^[A-Z][A-Z\\s]{5,}$',  # ALL CAPS headings\n",
    "        ]\n",
    "        \n",
    "        # HTML tags that indicate structure\n",
    "        self.structural_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'section', 'article', 'p']\n",
    "        \n",
    "        print(\"âœ… Structural chunker initialized\")\n",
    "    \n",
    "    def detect_html_content(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains HTML tags\"\"\"\n",
    "        return bool(re.search(r'<[^>]+>', text))\n",
    "    \n",
    "    def detect_content_patterns(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Detect structural patterns in plain text content to identify potential section breaks\n",
    "        and hierarchical structure based on common formatting patterns.\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        patterns = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Pattern 1: Numbered sections (1. 2. 3. a) b) etc.)\n",
    "            if re.match(r'^\\d+\\.\\s+.+', line):\n",
    "                patterns.append({\n",
    "                    'type': 'numbered_section',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'numeric',\n",
    "                    'level': 3\n",
    "                })\n",
    "                \n",
    "            # Pattern 2: Alphabetic subsections (a) b) c) or (a. b. c.))\n",
    "            elif re.match(r'^[a-z]\\)\\s+.+|^\\([a-z]\\)\\s+.+|^\\([a-z]\\.\\s+.+', line, re.IGNORECASE):\n",
    "                patterns.append({\n",
    "                    'type': 'alphabetic_subsection',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'alphabetic',\n",
    "                    'level': 4\n",
    "                })\n",
    "                \n",
    "            # Pattern 3: Roman numerals (i. ii. iii. or I. II. III.)\n",
    "            elif re.match(r'^[IVXivx]+\\.\\s+.+', line):\n",
    "                patterns.append({\n",
    "                    'type': 'roman_numeral',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'roman',\n",
    "                    'level': 3\n",
    "                })\n",
    "                \n",
    "            # Pattern 4: Bullet points (â€¢ - *)\n",
    "            elif line.startswith(('â€¢', '-', '*', 'â†’', 'â–ª', 'â—‹')):\n",
    "                patterns.append({\n",
    "                    'type': 'bullet_point',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'bullet',\n",
    "                    'level': 5\n",
    "                })\n",
    "                \n",
    "            # Pattern 5: Headers (ALL CAPS or Title Case followed by colon)\n",
    "            elif (line.isupper() and len(line) > 4) or re.match(r'^[A-Z][^.!?]*:', line):\n",
    "                patterns.append({\n",
    "                    'type': 'header',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'header',\n",
    "                    'level': 2\n",
    "                })\n",
    "                \n",
    "            # Pattern 6: Indented content (suggests hierarchical structure)\n",
    "            elif line.startswith('    ') or line.startswith('\\t'):\n",
    "                patterns.append({\n",
    "                    'type': 'indented_content',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'indent',\n",
    "                    'level': 6\n",
    "                })\n",
    "                \n",
    "            # Pattern 7: Question-Answer format (Q: or Question: followed by A: or Answer:)\n",
    "            elif re.match(r'^(Q:|Question:|A:|Answer:)\\s+.+', line, re.IGNORECASE):\n",
    "                patterns.append({\n",
    "                    'type': 'qa_format',\n",
    "                    'line_num': i,\n",
    "                    'text': line,\n",
    "                    'pattern': 'qa',\n",
    "                    'level': 4\n",
    "                })\n",
    "\n",
    "        return patterns\n",
    "    \n",
    "    def detect_topic_keywords(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Categorize content by topic using keyword matching\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Define topic keywords - customize these for your domain\n",
    "        topic_keywords = {\n",
    "            'kyc_documents': ['kyc', 'documents', 'verification', 'identity', 'proof', 'aadhaar', 'pan', 'declaration'],\n",
    "            'merchant_onboarding': ['merchant', 'onboarding', 'registration', 'business', 'proprietor', 'settlement'],\n",
    "            'payment_processing': ['payment', 'transaction', 'gateway', 'processing', 'refund', 'settlement'],\n",
    "            'bank_account': ['bank account', 'settlement', 'beneficiary', 'passbook', 'cheque', 'utr'],\n",
    "            'business_verification': ['business', 'proof', 'verification', 'certificate', 'legal name'],\n",
    "            'legal_declaration': ['declaration', 'affirm', 'solemnly', 'proprietor', 'legal'],\n",
    "            'technical_requirements': ['api', 'integration', 'technical', 'sdk', 'endpoint'],\n",
    "            'general': []\n",
    "        }\n",
    "        \n",
    "        best_topic = 'general'\n",
    "        best_score = 0\n",
    "        \n",
    "        for topic, keywords in topic_keywords.items():\n",
    "            if topic == 'general':\n",
    "                continue\n",
    "            score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_topic = topic\n",
    "        \n",
    "        return best_topic\n",
    "    \n",
    "    def extract_html_structure(self, html_text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract structural elements from HTML content\n",
    "        \n",
    "        Returns:\n",
    "            List of structural elements with their hierarchy and content\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        elements = []\n",
    "        \n",
    "        # Find all structural elements\n",
    "        for tag in soup.find_all(self.structural_tags):\n",
    "            # Skip if tag has no text content\n",
    "            text_content = tag.get_text().strip()\n",
    "            if not text_content:\n",
    "                continue\n",
    "                \n",
    "            # Determine hierarchy level\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                level = int(tag.name[1])  # Extract number from h1, h2, etc.\n",
    "                element_type = 'heading'\n",
    "            else:\n",
    "                level = 10  # Give non-heading elements lower priority\n",
    "                element_type = 'content'\n",
    "            \n",
    "            elements.append({\n",
    "                'tag': tag.name,\n",
    "                'level': level,\n",
    "                'type': element_type,\n",
    "                'text': text_content,\n",
    "                'start_pos': html_text.find(str(tag)) if str(tag) in html_text else 0\n",
    "            })\n",
    "        \n",
    "        # Sort by position in document\n",
    "        elements.sort(key=lambda x: x['start_pos'])\n",
    "        return elements\n",
    "    \n",
    "    def detect_markdown_headings(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Detect markdown-style headings and other structural patterns\n",
    "        \n",
    "        Returns:\n",
    "            List of heading elements with positions and levels\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        headings = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check markdown headings (# ## ### etc.)\n",
    "            markdown_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            if markdown_match:\n",
    "                level = len(markdown_match.group(1))\n",
    "                title = markdown_match.group(2).strip()\n",
    "                headings.append({\n",
    "                    'line_num': i,\n",
    "                    'level': level,\n",
    "                    'type': 'markdown_heading',\n",
    "                    'text': title,\n",
    "                    'full_line': line\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Check underlined headings\n",
    "            if i + 1 < len(lines):\n",
    "                next_line = lines[i + 1].strip()\n",
    "                if re.match(r'^={3,}$', next_line):  # Underlined with ===\n",
    "                    headings.append({\n",
    "                        'line_num': i,\n",
    "                        'level': 1,\n",
    "                        'type': 'underlined_heading',\n",
    "                        'text': line,\n",
    "                        'full_line': line\n",
    "                    })\n",
    "                    continue\n",
    "                elif re.match(r'^-{3,}$', next_line):  # Underlined with ---\n",
    "                    headings.append({\n",
    "                        'line_num': i,\n",
    "                        'level': 2,\n",
    "                        'type': 'underlined_heading',\n",
    "                        'text': line,\n",
    "                        'full_line': line\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Check numbered sections (1. 2. 3.)\n",
    "            if re.match(r'^\\d+\\.\\s+(.+)$', line):\n",
    "                headings.append({\n",
    "                    'line_num': i,\n",
    "                    'level': 3,\n",
    "                    'type': 'numbered_section',\n",
    "                    'text': line,\n",
    "                    'full_line': line\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Check ALL CAPS headings (at least 5 characters)\n",
    "            if re.match(r'^[A-Z][A-Z\\s]{4,}$', line) and len(line.split()) <= 6:\n",
    "                headings.append({\n",
    "                    'line_num': i,\n",
    "                    'level': 2,\n",
    "                    'type': 'caps_heading',\n",
    "                    'text': line,\n",
    "                    'full_line': line\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        return headings\n",
    "    \n",
    "    def chunk_by_content_patterns(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk text based on detected content patterns (for documents without HTML/markdown)\n",
    "        \"\"\"\n",
    "        patterns = self.detect_content_patterns(text)\n",
    "        \n",
    "        if not patterns:\n",
    "            # No patterns found - use topic-based chunking\n",
    "            return self.chunk_by_topic_content(text)\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        chunks = []\n",
    "        \n",
    "        for i, pattern in enumerate(patterns):\n",
    "            # Determine start and end positions for this chunk\n",
    "            start_line = pattern['line_num']\n",
    "            \n",
    "            # Find end line (next pattern or end of document)\n",
    "            if i + 1 < len(patterns):\n",
    "                end_line = patterns[i + 1]['line_num']\n",
    "            else:\n",
    "                end_line = len(lines)\n",
    "            \n",
    "            # Extract content for this section\n",
    "            section_lines = lines[start_line:end_line]\n",
    "            content = '\\n'.join(section_lines).strip()\n",
    "            \n",
    "            if content:\n",
    "                topic = self.detect_topic_keywords(content)\n",
    "                chunks.append({\n",
    "                    'content': content,\n",
    "                    'structure_type': pattern['pattern'],\n",
    "                    'level': pattern['level'],\n",
    "                    'heading': pattern['text'][:100],  # Limit heading length\n",
    "                    'topic': topic\n",
    "                })\n",
    "        \n",
    "        return chunks if chunks else self.chunk_by_topic_content(text)\n",
    "    \n",
    "    def chunk_by_topic_content(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fallback chunking by content topics when no structural patterns exist\n",
    "        \"\"\"\n",
    "        # Split into logical paragraphs first\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk_paras = []\n",
    "        current_tokens = 0\n",
    "        max_tokens_per_chunk = 800  # Reasonable size for topic-based chunks\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            para_tokens = len(self.tokenizer.encode(para))\n",
    "            \n",
    "            # If adding this paragraph would exceed limit, finalize current chunk\n",
    "            if current_tokens + para_tokens > max_tokens_per_chunk and current_chunk_paras:\n",
    "                chunk_content = '\\n\\n'.join(current_chunk_paras)\n",
    "                topic = self.detect_topic_keywords(chunk_content)\n",
    "                \n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'structure_type': 'topic_based',\n",
    "                    'level': 5,  # Low priority level\n",
    "                    'heading': f'Topic: {topic.title()}',\n",
    "                    'topic': topic\n",
    "                })\n",
    "                \n",
    "                current_chunk_paras = [para]\n",
    "                current_tokens = para_tokens\n",
    "            else:\n",
    "                current_chunk_paras.append(para)\n",
    "                current_tokens += para_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_paras:\n",
    "            chunk_content = '\\n\\n'.join(current_chunk_paras)\n",
    "            topic = self.detect_topic_keywords(chunk_content)\n",
    "            \n",
    "            chunks.append({\n",
    "                'content': chunk_content,\n",
    "                'structure_type': 'topic_based',\n",
    "                'level': 5,\n",
    "                'heading': f'Topic: {topic.title()}',\n",
    "                'topic': topic\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_html_structure(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk text based on HTML structure\"\"\"\n",
    "        elements = self.extract_html_structure(text)\n",
    "        \n",
    "        if not elements:\n",
    "            return [{'content': text, 'structure_type': 'no_structure', 'level': 0, 'heading': ''}]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_level = 0\n",
    "        current_heading = \"\"\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            if element['type'] == 'heading':\n",
    "                # Save previous chunk if it has content\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append({\n",
    "                        'content': current_chunk.strip(),\n",
    "                        'structure_type': 'html_section',\n",
    "                        'level': current_level,\n",
    "                        'heading': current_heading\n",
    "                    })\n",
    "                \n",
    "                # Start new chunk with this heading\n",
    "                current_chunk = element['text'] + \"\\n\"\n",
    "                current_level = element['level']\n",
    "                current_heading = element['text']\n",
    "            else:\n",
    "                # Add content to current chunk\n",
    "                current_chunk += element['text'] + \"\\n\"\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'content': current_chunk.strip(),\n",
    "                'structure_type': 'html_section',\n",
    "                'level': current_level,\n",
    "                'heading': current_heading\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_markdown_structure(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk text based on markdown headings and structure\"\"\"\n",
    "        headings = self.detect_markdown_headings(text)\n",
    "        \n",
    "        if not headings:\n",
    "            return [{'content': text, 'structure_type': 'no_headings', 'level': 0, 'heading': ''}]\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        chunks = []\n",
    "        \n",
    "        for i, heading in enumerate(headings):\n",
    "            # Determine start and end positions for this chunk\n",
    "            start_line = heading['line_num']\n",
    "            \n",
    "            # Find end line (next heading or end of document)\n",
    "            if i + 1 < len(headings):\n",
    "                end_line = headings[i + 1]['line_num']\n",
    "            else:\n",
    "                end_line = len(lines)\n",
    "            \n",
    "            # Extract content for this section\n",
    "            section_lines = lines[start_line:end_line]\n",
    "            content = '\\n'.join(section_lines).strip()\n",
    "            \n",
    "            if content:\n",
    "                chunks.append({\n",
    "                    'content': content,\n",
    "                    'structure_type': heading['type'],\n",
    "                    'level': heading['level'],\n",
    "                    'heading': heading['text']\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_paragraph_structure(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk text based on paragraph breaks\"\"\"\n",
    "        # Split by double newlines (paragraph breaks)\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        chunks = []\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            content = paragraph.strip()\n",
    "            if content:\n",
    "                chunks.append({\n",
    "                    'content': content,\n",
    "                    'structure_type': 'paragraph',\n",
    "                    'level': 5,  # Lower priority level\n",
    "                    'heading': f'Paragraph {i+1}'\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def merge_small_chunks(self, chunks: List[Dict], min_tokens: int = 100, max_tokens: int = 1500) -> List[Dict]:\n",
    "        \"\"\"Merge chunks that are too small and split chunks that are too large\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        adjusted_chunks = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            current_chunk = chunks[i]\n",
    "            current_tokens = len(self.tokenizer.encode(current_chunk['content']))\n",
    "            \n",
    "            # If chunk is too small, try to merge with next chunk\n",
    "            if current_tokens < min_tokens and i + 1 < len(chunks):\n",
    "                next_chunk = chunks[i + 1]\n",
    "                \n",
    "                # Only merge if they're at similar structural levels\n",
    "                if abs(current_chunk['level'] - next_chunk['level']) <= 1:\n",
    "                    merged_content = current_chunk['content'] + \"\\n\\n\" + next_chunk['content']\n",
    "                    merged_tokens = len(self.tokenizer.encode(merged_content))\n",
    "                    \n",
    "                    if merged_tokens <= max_tokens:\n",
    "                        merged_chunk = {\n",
    "                            'content': merged_content,\n",
    "                            'structure_type': f\"{current_chunk['structure_type']}_merged\",\n",
    "                            'level': min(current_chunk['level'], next_chunk['level']),\n",
    "                            'heading': f\"{current_chunk['heading']} + {next_chunk['heading']}\"\n",
    "                        }\n",
    "                        adjusted_chunks.append(merged_chunk)\n",
    "                        i += 2\n",
    "                        continue\n",
    "                \n",
    "                # Can't merge, keep original\n",
    "                adjusted_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "            \n",
    "            # If chunk is too large, split it\n",
    "            elif current_tokens > max_tokens:\n",
    "                sub_chunks = self.split_large_chunk(current_chunk, max_tokens)\n",
    "                adjusted_chunks.extend(sub_chunks)\n",
    "                i += 1\n",
    "            \n",
    "            else:\n",
    "                # Chunk size is acceptable\n",
    "                adjusted_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "        \n",
    "        return adjusted_chunks\n",
    "    \n",
    "    def split_large_chunk(self, chunk: Dict, max_tokens: int) -> List[Dict]:\n",
    "        \"\"\"Split a large chunk into smaller pieces\"\"\"\n",
    "        content = chunk['content']\n",
    "        tokens = self.tokenizer.encode(content)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return [chunk]\n",
    "        \n",
    "        # Split by sentences first, then by tokens if needed\n",
    "        sentences = re.split(r'[.!?]+', content)\n",
    "        sub_chunks = []\n",
    "        current_content = \"\"\n",
    "        part_num = 1\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            # Check if adding this sentence would exceed limit\n",
    "            test_content = current_content + \". \" + sentence if current_content else sentence\n",
    "            test_tokens = len(self.tokenizer.encode(test_content))\n",
    "            \n",
    "            if test_tokens > max_tokens and current_content:\n",
    "                # Save current chunk and start new one\n",
    "                sub_chunks.append({\n",
    "                    'content': current_content.strip(),\n",
    "                    'structure_type': f\"{chunk['structure_type']}_split\",\n",
    "                    'level': chunk['level'],\n",
    "                    'heading': f\"{chunk['heading']} (Part {part_num})\"\n",
    "                })\n",
    "                current_content = sentence\n",
    "                part_num += 1\n",
    "            else:\n",
    "                current_content = test_content\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_content.strip():\n",
    "            sub_chunks.append({\n",
    "                'content': current_content.strip(),\n",
    "                'structure_type': f\"{chunk['structure_type']}_split\",\n",
    "                'level': chunk['level'],\n",
    "                'heading': f\"{chunk['heading']} (Part {part_num})\"\n",
    "            })\n",
    "        \n",
    "        return sub_chunks if sub_chunks else [chunk]\n",
    "    \n",
    "    def structural_chunk_text(self, text: str, preserve_hierarchy: bool = True,\n",
    "                            min_chunk_tokens: int = 100, max_chunk_tokens: int = 1500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform structural chunking on text (adapted for plain text without HTML/markdown)\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            preserve_hierarchy: Whether to maintain structural hierarchy\n",
    "            min_chunk_tokens: Minimum chunk size in tokens\n",
    "            max_chunk_tokens: Maximum chunk size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of structurally coherent chunks\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        # Try different structural approaches in order of preference\n",
    "        chunks = []\n",
    "        \n",
    "        # 1. Try HTML structure first\n",
    "        if self.detect_html_content(text):\n",
    "            chunks = self.chunk_by_html_structure(text)\n",
    "        \n",
    "        # 2. Try markdown structure\n",
    "        elif self.detect_markdown_headings(text):\n",
    "            chunks = self.chunk_by_markdown_structure(text)\n",
    "        \n",
    "        # 3. Try content pattern detection (for plain text documents)\n",
    "        else:\n",
    "            chunks = self.chunk_by_content_patterns(text)\n",
    "        \n",
    "        # 4. If no meaningful chunks, fall back to topic-based chunking\n",
    "        if not chunks or (len(chunks) == 1 and chunks[0].get('structure_type') == 'topic_based'):\n",
    "            chunks = self.chunk_by_topic_content(text)\n",
    "        \n",
    "        # Adjust chunk sizes if needed\n",
    "        if preserve_hierarchy:\n",
    "            chunks = self.merge_small_chunks(chunks, min_chunk_tokens, max_chunk_tokens)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_document(self, doc: Dict, preserve_hierarchy: bool = True,\n",
    "                        min_chunk_tokens: int = 100, max_chunk_tokens: int = 1500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document with structural chunking\n",
    "        \n",
    "        Args:\n",
    "            doc: Document dictionary with url, title, content\n",
    "            preserve_hierarchy: Whether to maintain structural hierarchy\n",
    "            min_chunk_tokens: Minimum chunk size in tokens\n",
    "            max_chunk_tokens: Maximum chunk size in tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with metadata\n",
    "        \"\"\"\n",
    "        content = doc.get('content', '')\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        structural_chunks = self.structural_chunk_text(\n",
    "            content, preserve_hierarchy, min_chunk_tokens, max_chunk_tokens\n",
    "        )\n",
    "        \n",
    "        chunk_objects = []\n",
    "        for i, chunk_data in enumerate(structural_chunks):\n",
    "            chunk_text = chunk_data['content']\n",
    "            if chunk_text.strip():\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{doc['url']}#structural_chunk_{i}\",\n",
    "                    'source_url': doc['url'],\n",
    "                    'source_title': doc.get('title', ''),\n",
    "                    'content': chunk_text.strip(),\n",
    "                    'chunk_index': i,\n",
    "                    'token_count': len(self.tokenizer.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'strategy': 'structural',\n",
    "                    'strategy_params': {\n",
    "                        'preserve_hierarchy': preserve_hierarchy,\n",
    "                        'min_chunk_tokens': min_chunk_tokens,\n",
    "                        'max_chunk_tokens': max_chunk_tokens\n",
    "                    },\n",
    "                    'structural_info': {\n",
    "                        'structure_type': chunk_data['structure_type'],\n",
    "                        'level': chunk_data['level'],\n",
    "                        'heading': chunk_data['heading'],\n",
    "                        'topic': chunk_data.get('topic', 'general')\n",
    "                    }\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects\n",
    "\n",
    "\n",
    "def load_scraped_data() -> List[Dict]:\n",
    "    \"\"\"Load scraped data from final.json\"\"\"\n",
    "    try:\n",
    "        with open('final.json', 'r', encoding='utf-8') as f:\n",
    "            scraped_data = json.load(f)\n",
    "        \n",
    "        formatted_data = []\n",
    "        for item in scraped_data:\n",
    "            if 'content' in item:\n",
    "                formatted_item = item\n",
    "            elif 'text' in item:\n",
    "                formatted_item = {\n",
    "                    \"url\": item.get(\"url\", \"unknown\"),\n",
    "                    \"title\": item.get(\"title\", item.get(\"url\", \"unknown\")),\n",
    "                    \"content\": item[\"text\"]\n",
    "                }\n",
    "            else:\n",
    "                continue\n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        return formatted_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ final.json not found\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading final.json: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    directories = [\n",
    "        'data/chunks/structural',\n",
    "        'results/ablations'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Create output directories\n",
    "    create_output_directories()\n",
    "    \n",
    "    # Initialize structural chunker\n",
    "    print(\"ðŸš€ Initializing Structural Chunker...\")\n",
    "    structural_chunker = StructuralChunker()\n",
    "\n",
    "    # Load data\n",
    "    documents = load_scraped_data()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âŒ No documents loaded. Please check your final.json file.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ… Loaded {len(documents)} documents for structural chunking\")\n",
    "    \n",
    "    # Define structural chunking configurations to test\n",
    "    structural_configs = [\n",
    "        {\n",
    "            \"name\": \"Structural_Hierarchical\",\n",
    "            \"preserve_hierarchy\": True,\n",
    "            \"min_tokens\": 100,\n",
    "            \"max_tokens\": 1200\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Structural_Balanced\", \n",
    "            \"preserve_hierarchy\": True,\n",
    "            \"min_tokens\": 150,\n",
    "            \"max_tokens\": 1500\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Structural_Large\",\n",
    "            \"preserve_hierarchy\": False,  # Allow breaking hierarchy for size control\n",
    "            \"min_tokens\": 200,\n",
    "            \"max_tokens\": 1800\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process documents with each structural configuration\n",
    "    structural_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ”„ Processing documents with structural chunking strategies...\")\n",
    "    print(\"=\" * 75)\n",
    "    \n",
    "    for config in structural_configs:\n",
    "        config_name = config[\"name\"]\n",
    "        preserve_hierarchy = config[\"preserve_hierarchy\"]\n",
    "        min_tokens = config[\"min_tokens\"]\n",
    "        max_tokens = config[\"max_tokens\"]\n",
    "        \n",
    "        print(f\"\\nðŸ“ Processing: {config_name}\")\n",
    "        print(f\"   Preserve hierarchy: {preserve_hierarchy}\")\n",
    "        print(f\"   Min tokens: {min_tokens}\")\n",
    "        print(f\"   Max tokens: {max_tokens}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            chunks = structural_chunker.process_document(\n",
    "                doc, preserve_hierarchy, min_tokens, max_tokens\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"   Document {doc_idx + 1}: Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chunks = len(all_chunks)\n",
    "        total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "        avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        # Count different structure types and topics\n",
    "        structure_types = {}\n",
    "        topics = {}\n",
    "        for chunk in all_chunks:\n",
    "            struct_info = chunk.get('structural_info', {})\n",
    "            struct_type = struct_info.get('structure_type', 'unknown')\n",
    "            topic = struct_info.get('topic', 'general')\n",
    "            \n",
    "            structure_types[struct_type] = structure_types.get(struct_type, 0) + 1\n",
    "            topics[topic] = topics.get(topic, 0) + 1\n",
    "        \n",
    "        structural_results[config_name] = {\n",
    "            'chunks': all_chunks,\n",
    "            'total_chunks': total_chunks,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "            'processing_time': processing_time,\n",
    "            'config': config,\n",
    "            'structure_types': structure_types,\n",
    "            'topics': topics\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Generated {total_chunks} chunks\")\n",
    "        print(f\"   â±ï¸  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"   ðŸ“Š Average tokens per chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "        print(f\"   ðŸ—ï¸  Structure types: {list(structure_types.keys())}\")\n",
    "        print(f\"   ðŸ“‹ Topics found: {list(topics.keys())}\")\n",
    "        \n",
    "        # Save chunks to file\n",
    "        output_file = f\"data/chunks/structural/{config_name}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"   ðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 75)\n",
    "    print(\"âœ… Structural chunking completed for all configurations!\")\n",
    "    \n",
    "    # Create summary comparison\n",
    "    print(\"\\nðŸ“ˆ STRUCTURAL CHUNKING SUMMARY:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    structural_summary = []\n",
    "    for name, result in structural_results.items():\n",
    "        config = result['config']\n",
    "        structural_summary.append({\n",
    "            'Strategy': name,\n",
    "            'Preserve Hierarchy': config['preserve_hierarchy'],\n",
    "            'Min Tokens': config['min_tokens'],\n",
    "            'Max Tokens': config['max_tokens'],\n",
    "            'Total Chunks': result['total_chunks'],\n",
    "            'Avg Tokens/Chunk': f\"{result['avg_tokens_per_chunk']:.1f}\",\n",
    "            'Processing Time (s)': f\"{result['processing_time']:.2f}\",\n",
    "            'Structure Types': len(result['structure_types'])\n",
    "        })\n",
    "    \n",
    "    structural_df = pd.DataFrame(structural_summary)\n",
    "    print(structural_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    structural_df.to_csv('results/ablations/structural_chunking_summary.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Summary saved to: results/ablations/structural_chunking_summary.csv\")\n",
    "    \n",
    "    # Show sample chunks with structural information\n",
    "    print(\"\\nðŸ” SAMPLE CHUNKS WITH STRUCTURE INFO:\")\n",
    "    print(\"=\" * 100)\n",
    "    for strategy_name, result in structural_results.items():\n",
    "        if result['chunks']:\n",
    "            sample_chunk = result['chunks'][0]\n",
    "            structural_info = sample_chunk.get('structural_info', {})\n",
    "            print(f\"\\nðŸ“„ {strategy_name}:\")\n",
    "            print(f\"   Tokens: {sample_chunk['token_count']}\")\n",
    "            print(f\"   Structure Type: {structural_info.get('structure_type', 'N/A')}\")\n",
    "            print(f\"   Level: {structural_info.get('level', 'N/A')}\")\n",
    "            print(f\"   Heading: {structural_info.get('heading', 'N/A')}\")\n",
    "            print(f\"   Content preview: {sample_chunk['content'][:200]}...\")\n",
    "            print(f\"   Source: {sample_chunk['source_title']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f59c3",
   "metadata": {},
   "source": [
    "# Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e0c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Recursive Chunker...\n",
      "âœ… Recursive chunker initialized\n",
      "\n",
      "âœ… Loaded 139 documents for recursive chunking\n",
      "\n",
      "ðŸ”„ Processing documents with recursive chunking strategies...\n",
      "===========================================================================\n",
      "\n",
      "ðŸ“ Processing: Recursive_Balanced\n",
      "   Max tokens: 1024\n",
      "   Semantic max tokens: 512\n",
      "   Fixed chunk size: 256\n",
      "   Fixed overlap: 64\n",
      "   Document 1: Generated 5 chunks\n",
      "   Document 2: Generated 2 chunks\n",
      "   Document 3: Generated 31 chunks\n",
      "   Document 4: Generated 35 chunks\n",
      "   Document 5: Generated 35 chunks\n",
      "   Document 6: Generated 31 chunks\n",
      "   Document 7: Generated 50 chunks\n",
      "   Document 8: Generated 4 chunks\n",
      "   Document 9: Generated 87 chunks\n",
      "   Document 10: Generated 12 chunks\n",
      "   Document 11: Generated 11 chunks\n",
      "   Document 12: Generated 14 chunks\n",
      "   Document 13: Generated 49 chunks\n",
      "   Document 14: Generated 11 chunks\n",
      "   Document 15: Generated 1 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 62 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 1 chunks\n",
      "   Document 20: Generated 1 chunks\n",
      "   Document 21: Generated 1 chunks\n",
      "   Document 22: Generated 1 chunks\n",
      "   Document 23: Generated 1 chunks\n",
      "   Document 24: Generated 1 chunks\n",
      "   Document 25: Generated 1 chunks\n",
      "   Document 26: Generated 1 chunks\n",
      "   Document 27: Generated 1 chunks\n",
      "   Document 28: Generated 1 chunks\n",
      "   Document 29: Generated 1 chunks\n",
      "   Document 30: Generated 1 chunks\n",
      "   Document 31: Generated 1 chunks\n",
      "   Document 32: Generated 1 chunks\n",
      "   Document 33: Generated 1 chunks\n",
      "   Document 34: Generated 1 chunks\n",
      "   Document 35: Generated 1 chunks\n",
      "   Document 36: Generated 1 chunks\n",
      "   Document 37: Generated 1 chunks\n",
      "   Document 38: Generated 1 chunks\n",
      "   Document 39: Generated 5 chunks\n",
      "   Document 40: Generated 31 chunks\n",
      "   Document 41: Generated 1 chunks\n",
      "   Document 42: Generated 7 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 4 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 4 chunks\n",
      "   Document 48: Generated 4 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 3 chunks\n",
      "   Document 81: Generated 3 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 4 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 1 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 619 chunks\n",
      "   â±ï¸  Processing time: 0.12s\n",
      "   ðŸ“Š Average tokens per chunk: 158.3\n",
      "   ðŸ”„ Processing levels: ['structural', 'semantic']\n",
      "   ðŸ“‹ Chunk types: ['text_structural', 'semantic_paragraph', 'semantic_sentence', 'html_no_structure']\n",
      "   ðŸ’¾ Saved to: data/chunks/recursive/Recursive_Balanced.json\n",
      "\n",
      "ðŸ“ Processing: Recursive_Large\n",
      "   Max tokens: 1536\n",
      "   Semantic max tokens: 768\n",
      "   Fixed chunk size: 384\n",
      "   Fixed overlap: 96\n",
      "   Document 1: Generated 5 chunks\n",
      "   Document 2: Generated 2 chunks\n",
      "   Document 3: Generated 31 chunks\n",
      "   Document 4: Generated 35 chunks\n",
      "   Document 5: Generated 35 chunks\n",
      "   Document 6: Generated 31 chunks\n",
      "   Document 7: Generated 48 chunks\n",
      "   Document 8: Generated 4 chunks\n",
      "   Document 9: Generated 84 chunks\n",
      "   Document 10: Generated 12 chunks\n",
      "   Document 11: Generated 11 chunks\n",
      "   Document 12: Generated 14 chunks\n",
      "   Document 13: Generated 49 chunks\n",
      "   Document 14: Generated 11 chunks\n",
      "   Document 15: Generated 1 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 62 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 1 chunks\n",
      "   Document 20: Generated 1 chunks\n",
      "   Document 21: Generated 1 chunks\n",
      "   Document 22: Generated 1 chunks\n",
      "   Document 23: Generated 1 chunks\n",
      "   Document 24: Generated 1 chunks\n",
      "   Document 25: Generated 1 chunks\n",
      "   Document 26: Generated 1 chunks\n",
      "   Document 27: Generated 1 chunks\n",
      "   Document 28: Generated 1 chunks\n",
      "   Document 29: Generated 1 chunks\n",
      "   Document 30: Generated 1 chunks\n",
      "   Document 31: Generated 1 chunks\n",
      "   Document 32: Generated 1 chunks\n",
      "   Document 33: Generated 1 chunks\n",
      "   Document 34: Generated 1 chunks\n",
      "   Document 35: Generated 1 chunks\n",
      "   Document 36: Generated 1 chunks\n",
      "   Document 37: Generated 1 chunks\n",
      "   Document 38: Generated 1 chunks\n",
      "   Document 39: Generated 4 chunks\n",
      "   Document 40: Generated 20 chunks\n",
      "   Document 41: Generated 1 chunks\n",
      "   Document 42: Generated 5 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 4 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 4 chunks\n",
      "   Document 48: Generated 4 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 3 chunks\n",
      "   Document 81: Generated 3 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 4 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 1 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 600 chunks\n",
      "   â±ï¸  Processing time: 0.15s\n",
      "   ðŸ“Š Average tokens per chunk: 163.3\n",
      "   ðŸ”„ Processing levels: ['structural', 'semantic']\n",
      "   ðŸ“‹ Chunk types: ['text_structural', 'semantic_sentence', 'html_no_structure']\n",
      "   ðŸ’¾ Saved to: data/chunks/recursive/Recursive_Large.json\n",
      "\n",
      "ðŸ“ Processing: Recursive_Small\n",
      "   Max tokens: 768\n",
      "   Semantic max tokens: 384\n",
      "   Fixed chunk size: 192\n",
      "   Fixed overlap: 48\n",
      "   Document 1: Generated 5 chunks\n",
      "   Document 2: Generated 2 chunks\n",
      "   Document 3: Generated 31 chunks\n",
      "   Document 4: Generated 35 chunks\n",
      "   Document 5: Generated 35 chunks\n",
      "   Document 6: Generated 31 chunks\n",
      "   Document 7: Generated 50 chunks\n",
      "   Document 8: Generated 4 chunks\n",
      "   Document 9: Generated 90 chunks\n",
      "   Document 10: Generated 12 chunks\n",
      "   Document 11: Generated 11 chunks\n",
      "   Document 12: Generated 14 chunks\n",
      "   Document 13: Generated 49 chunks\n",
      "   Document 14: Generated 11 chunks\n",
      "   Document 15: Generated 1 chunks\n",
      "   Document 16: Generated 1 chunks\n",
      "   Document 17: Generated 62 chunks\n",
      "   Document 18: Generated 1 chunks\n",
      "   Document 19: Generated 1 chunks\n",
      "   Document 20: Generated 1 chunks\n",
      "   Document 21: Generated 3 chunks\n",
      "   Document 22: Generated 3 chunks\n",
      "   Document 23: Generated 1 chunks\n",
      "   Document 24: Generated 1 chunks\n",
      "   Document 25: Generated 1 chunks\n",
      "   Document 26: Generated 1 chunks\n",
      "   Document 27: Generated 1 chunks\n",
      "   Document 28: Generated 1 chunks\n",
      "   Document 29: Generated 1 chunks\n",
      "   Document 30: Generated 3 chunks\n",
      "   Document 31: Generated 1 chunks\n",
      "   Document 32: Generated 1 chunks\n",
      "   Document 33: Generated 1 chunks\n",
      "   Document 34: Generated 1 chunks\n",
      "   Document 35: Generated 3 chunks\n",
      "   Document 36: Generated 3 chunks\n",
      "   Document 37: Generated 3 chunks\n",
      "   Document 38: Generated 1 chunks\n",
      "   Document 39: Generated 7 chunks\n",
      "   Document 40: Generated 43 chunks\n",
      "   Document 41: Generated 4 chunks\n",
      "   Document 42: Generated 10 chunks\n",
      "   Document 43: Generated 1 chunks\n",
      "   Document 44: Generated 1 chunks\n",
      "   Document 45: Generated 4 chunks\n",
      "   Document 46: Generated 1 chunks\n",
      "   Document 47: Generated 4 chunks\n",
      "   Document 48: Generated 4 chunks\n",
      "   Document 49: Generated 1 chunks\n",
      "   Document 50: Generated 1 chunks\n",
      "   Document 51: Generated 1 chunks\n",
      "   Document 52: Generated 1 chunks\n",
      "   Document 53: Generated 1 chunks\n",
      "   Document 54: Generated 1 chunks\n",
      "   Document 55: Generated 1 chunks\n",
      "   Document 56: Generated 1 chunks\n",
      "   Document 57: Generated 1 chunks\n",
      "   Document 58: Generated 1 chunks\n",
      "   Document 59: Generated 1 chunks\n",
      "   Document 60: Generated 1 chunks\n",
      "   Document 61: Generated 1 chunks\n",
      "   Document 62: Generated 1 chunks\n",
      "   Document 63: Generated 1 chunks\n",
      "   Document 64: Generated 1 chunks\n",
      "   Document 65: Generated 1 chunks\n",
      "   Document 66: Generated 1 chunks\n",
      "   Document 67: Generated 1 chunks\n",
      "   Document 68: Generated 1 chunks\n",
      "   Document 69: Generated 1 chunks\n",
      "   Document 70: Generated 1 chunks\n",
      "   Document 71: Generated 1 chunks\n",
      "   Document 72: Generated 1 chunks\n",
      "   Document 73: Generated 1 chunks\n",
      "   Document 74: Generated 1 chunks\n",
      "   Document 75: Generated 1 chunks\n",
      "   Document 76: Generated 1 chunks\n",
      "   Document 77: Generated 1 chunks\n",
      "   Document 78: Generated 1 chunks\n",
      "   Document 79: Generated 1 chunks\n",
      "   Document 80: Generated 3 chunks\n",
      "   Document 81: Generated 3 chunks\n",
      "   Document 82: Generated 1 chunks\n",
      "   Document 83: Generated 1 chunks\n",
      "   Document 84: Generated 1 chunks\n",
      "   Document 85: Generated 1 chunks\n",
      "   Document 86: Generated 1 chunks\n",
      "   Document 87: Generated 1 chunks\n",
      "   Document 88: Generated 1 chunks\n",
      "   Document 89: Generated 1 chunks\n",
      "   Document 90: Generated 1 chunks\n",
      "   Document 91: Generated 1 chunks\n",
      "   Document 92: Generated 1 chunks\n",
      "   Document 93: Generated 1 chunks\n",
      "   Document 94: Generated 1 chunks\n",
      "   Document 95: Generated 1 chunks\n",
      "   Document 96: Generated 1 chunks\n",
      "   Document 97: Generated 1 chunks\n",
      "   Document 98: Generated 1 chunks\n",
      "   Document 99: Generated 1 chunks\n",
      "   Document 100: Generated 1 chunks\n",
      "   Document 101: Generated 1 chunks\n",
      "   Document 102: Generated 1 chunks\n",
      "   Document 103: Generated 1 chunks\n",
      "   Document 104: Generated 1 chunks\n",
      "   Document 105: Generated 1 chunks\n",
      "   Document 106: Generated 1 chunks\n",
      "   Document 107: Generated 1 chunks\n",
      "   Document 108: Generated 4 chunks\n",
      "   Document 109: Generated 1 chunks\n",
      "   Document 110: Generated 1 chunks\n",
      "   Document 111: Generated 1 chunks\n",
      "   Document 112: Generated 1 chunks\n",
      "   Document 113: Generated 1 chunks\n",
      "   Document 114: Generated 1 chunks\n",
      "   Document 115: Generated 1 chunks\n",
      "   Document 116: Generated 1 chunks\n",
      "   Document 117: Generated 1 chunks\n",
      "   Document 118: Generated 1 chunks\n",
      "   Document 119: Generated 1 chunks\n",
      "   Document 120: Generated 1 chunks\n",
      "   Document 121: Generated 1 chunks\n",
      "   Document 122: Generated 1 chunks\n",
      "   Document 123: Generated 1 chunks\n",
      "   Document 124: Generated 1 chunks\n",
      "   Document 125: Generated 1 chunks\n",
      "   Document 126: Generated 1 chunks\n",
      "   Document 127: Generated 1 chunks\n",
      "   Document 128: Generated 1 chunks\n",
      "   Document 129: Generated 1 chunks\n",
      "   Document 130: Generated 1 chunks\n",
      "   Document 131: Generated 1 chunks\n",
      "   Document 132: Generated 1 chunks\n",
      "   Document 133: Generated 1 chunks\n",
      "   Document 134: Generated 1 chunks\n",
      "   Document 135: Generated 1 chunks\n",
      "   Document 136: Generated 1 chunks\n",
      "   Document 137: Generated 1 chunks\n",
      "   Document 138: Generated 1 chunks\n",
      "   Document 139: Generated 1 chunks\n",
      "   âœ… Generated 654 chunks\n",
      "   â±ï¸  Processing time: 0.12s\n",
      "   ðŸ“Š Average tokens per chunk: 150.0\n",
      "   ðŸ”„ Processing levels: ['structural', 'semantic', 'fixed']\n",
      "   ðŸ“‹ Chunk types: ['text_structural', 'semantic_paragraph', 'semantic_sentence', 'fixed_size', 'html_no_structure']\n",
      "   ðŸ’¾ Saved to: data/chunks/recursive/Recursive_Small.json\n",
      "\n",
      "===========================================================================\n",
      "âœ… Recursive chunking completed for all configurations!\n",
      "\n",
      "ðŸ“ˆ RECURSIVE CHUNKING SUMMARY:\n",
      "====================================================================================================\n",
      "          Strategy  Max Tokens  Semantic Max  Fixed Size  Fixed Overlap  Total Chunks Avg Tokens/Chunk Processing Time (s)  Processing Levels\n",
      "Recursive_Balanced        1024           512         256             64           619            158.3                0.12                  2\n",
      "   Recursive_Large        1536           768         384             96           600            163.3                0.15                  2\n",
      "   Recursive_Small         768           384         192             48           654            150.0                0.12                  3\n",
      "\n",
      "ðŸ’¾ Summary saved to: results/ablations/recursive_chunking_summary.csv\n",
      "\n",
      "ðŸ” SAMPLE CHUNKS WITH RECURSIVE INFO:\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“„ Recursive_Balanced:\n",
      "   Tokens: 14\n",
      "   Processing Level: structural\n",
      "   Chunk Type: text_structural\n",
      "   Level: 10\n",
      "   Heading: Content Block\n",
      "   Parent Heading: \n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Recursive_Large:\n",
      "   Tokens: 14\n",
      "   Processing Level: structural\n",
      "   Chunk Type: text_structural\n",
      "   Level: 10\n",
      "   Heading: Content Block\n",
      "   Parent Heading: \n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME...\n",
      "   Source: annexure_form_A.pdf\n",
      "\n",
      "ðŸ“„ Recursive_Small:\n",
      "   Tokens: 14\n",
      "   Processing Level: structural\n",
      "   Chunk Type: text_structural\n",
      "   Level: 10\n",
      "   Heading: Content Block\n",
      "   Parent Heading: \n",
      "   Content preview: ANNEXURE I: FORMAT OF DECLARATION TO BE FURNISHED WHEN OWNER NAME OR BUSINESS NAME...\n",
      "   Source: annexure_form_A.pdf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RECURSIVE CHUNKING IMPLEMENTATION\n",
    "# Strategy 4: Hierarchical fallback from large structural blocks to smaller chunks\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Mock tokenizer for demonstration - replace with your actual tokenizer\n",
    "class MockTokenizer:\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Simple mock tokenizer - replace with actual tokenizer\"\"\"\n",
    "        return text.split()  # Simple word-based tokenization\n",
    "\n",
    "# Initialize mock tokenizer - replace with your actual tokenizer\n",
    "tokenizer = MockTokenizer()\n",
    "\n",
    "class RecursiveChunker:\n",
    "    \"\"\"\n",
    "    Implementation of recursive chunking with hierarchical fallback:\n",
    "    1. Try structural chunking first (headings, HTML tags)\n",
    "    2. If chunks are too large, fall back to semantic chunking\n",
    "    3. If still too large, fall back to fixed-size chunking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize recursive chunker\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Structural patterns (from structural chunker)\n",
    "        self.heading_patterns = [\n",
    "            r'^#{1,6}\\s+(.+)$',  # Markdown headings\n",
    "            r'^(.+)\\n={3,}$',    # Underlined headings with ===\n",
    "            r'^(.+)\\n-{3,}$',    # Underlined headings with ---\n",
    "            r'^\\d+\\.\\s+(.+)$',   # Numbered sections\n",
    "            r'^[A-Z][A-Z\\s]{5,}$',  # ALL CAPS headings\n",
    "        ]\n",
    "        \n",
    "        self.structural_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'section', 'article', 'p']\n",
    "        \n",
    "        print(\"âœ… Recursive chunker initialized\")\n",
    "    \n",
    "    def detect_html_content(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains HTML tags\"\"\"\n",
    "        return bool(re.search(r'<[^>]+>', text))\n",
    "    \n",
    "    def get_structural_chunks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract structural chunks using headings and HTML structure\n",
    "        Similar to structural chunker but returns raw chunks without size constraints\n",
    "        \"\"\"\n",
    "        if self.detect_html_content(text):\n",
    "            return self._extract_html_structural_chunks(text)\n",
    "        else:\n",
    "            return self._extract_text_structural_chunks(text)\n",
    "    \n",
    "    def _extract_html_structural_chunks(self, html_text: str) -> List[Dict]:\n",
    "        \"\"\"Extract chunks based on HTML structure\"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        chunks = []\n",
    "        \n",
    "        # Find all heading elements\n",
    "        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        \n",
    "        if not headings:\n",
    "            # No headings found, return whole text as one chunk\n",
    "            return [{\n",
    "                'content': soup.get_text().strip(),\n",
    "                'level': 0,\n",
    "                'heading': 'No Structure',\n",
    "                'chunk_type': 'html_no_structure'\n",
    "            }]\n",
    "        \n",
    "        # Process each heading section\n",
    "        for i, heading in enumerate(headings):\n",
    "            level = int(heading.name[1])  # Extract number from h1, h2, etc.\n",
    "            heading_text = heading.get_text().strip()\n",
    "            \n",
    "            # Find content until next heading of same or higher level\n",
    "            content_elements = []\n",
    "            current = heading.next_sibling\n",
    "            \n",
    "            while current:\n",
    "                if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                    current_level = int(current.name[1])\n",
    "                    if current_level <= level:  # Same or higher level heading\n",
    "                        break\n",
    "                \n",
    "                if hasattr(current, 'get_text'):\n",
    "                    text_content = current.get_text().strip()\n",
    "                    if text_content:\n",
    "                        content_elements.append(text_content)\n",
    "                elif isinstance(current, str) and current.strip():\n",
    "                    content_elements.append(current.strip())\n",
    "                \n",
    "                current = current.next_sibling\n",
    "            \n",
    "            # Combine heading and content\n",
    "            full_content = heading_text\n",
    "            if content_elements:\n",
    "                full_content += \"\\n\\n\" + \"\\n\".join(content_elements)\n",
    "            \n",
    "            if full_content.strip():\n",
    "                chunks.append({\n",
    "                    'content': full_content.strip(),\n",
    "                    'level': level,\n",
    "                    'heading': heading_text,\n",
    "                    'chunk_type': 'html_structural'\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_text_structural_chunks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract chunks based on text structure patterns\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk_lines = []\n",
    "        current_heading = \"\"\n",
    "        current_level = 10  # Default high level for non-structured content\n",
    "        \n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped:\n",
    "                if current_chunk_lines:  # Keep empty lines within chunks\n",
    "                    current_chunk_lines.append(line)\n",
    "                continue\n",
    "            \n",
    "            # Check for structural patterns\n",
    "            is_heading = False\n",
    "            heading_level = 10\n",
    "            \n",
    "            # Markdown headings (# ## ### etc.)\n",
    "            markdown_match = re.match(r'^(#{1,6})\\s+(.+)$', line_stripped)\n",
    "            if markdown_match:\n",
    "                is_heading = True\n",
    "                heading_level = len(markdown_match.group(1))\n",
    "                heading_text = markdown_match.group(2)\n",
    "            \n",
    "            # Numbered sections (1. 2. 3.)\n",
    "            elif re.match(r'^\\d+\\.\\s+(.+)$', line_stripped):\n",
    "                is_heading = True\n",
    "                heading_level = 3\n",
    "                heading_text = line_stripped\n",
    "            \n",
    "            # ALL CAPS headings\n",
    "            elif line_stripped.isupper() and len(line_stripped) > 4 and len(line_stripped.split()) <= 6:\n",
    "                is_heading = True\n",
    "                heading_level = 2\n",
    "                heading_text = line_stripped\n",
    "            \n",
    "            # Title case with colon\n",
    "            elif re.match(r'^[A-Z][^.!?]*:$', line_stripped):\n",
    "                is_heading = True\n",
    "                heading_level = 3\n",
    "                heading_text = line_stripped\n",
    "            \n",
    "            if is_heading:\n",
    "                # Save previous chunk if exists\n",
    "                if current_chunk_lines:\n",
    "                    chunk_content = '\\n'.join(current_chunk_lines).strip()\n",
    "                    if chunk_content:\n",
    "                        chunks.append({\n",
    "                            'content': chunk_content,\n",
    "                            'level': current_level,\n",
    "                            'heading': current_heading if current_heading else 'Content Block',\n",
    "                            'chunk_type': 'text_structural'\n",
    "                        })\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk_lines = [line]\n",
    "                current_heading = heading_text\n",
    "                current_level = heading_level\n",
    "            else:\n",
    "                current_chunk_lines.append(line)\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_lines:\n",
    "            chunk_content = '\\n'.join(current_chunk_lines).strip()\n",
    "            if chunk_content:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'level': current_level,\n",
    "                    'heading': current_heading if current_heading else 'Content Block',\n",
    "                    'chunk_type': 'text_structural'\n",
    "                })\n",
    "        \n",
    "        # If no structured chunks found, return whole text\n",
    "        if not chunks:\n",
    "            chunks = [{\n",
    "                'content': text.strip(),\n",
    "                'level': 10,\n",
    "                'heading': 'No Structure',\n",
    "                'chunk_type': 'text_no_structure'\n",
    "            }]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def semantic_split_chunk(self, text: str, max_tokens: int = 512) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Split text using semantic boundaries (sentences and paragraphs)\n",
    "        \"\"\"\n",
    "        # Split by paragraphs first\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            # Check if adding this paragraph would exceed limit\n",
    "            test_chunk = current_chunk + \"\\n\\n\" + para if current_chunk else para\n",
    "            test_tokens = len(self.tokenizer.encode(test_chunk))\n",
    "            \n",
    "            if test_tokens > max_tokens and current_chunk:\n",
    "                # Current chunk is full, save it\n",
    "                chunks.append({\n",
    "                    'content': current_chunk.strip(),\n",
    "                    'chunk_type': 'semantic_paragraph',\n",
    "                    'level': 5,\n",
    "                    'heading': 'Semantic Chunk'\n",
    "                })\n",
    "                current_chunk = para\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        # Handle remaining content\n",
    "        if current_chunk.strip():\n",
    "            current_tokens = len(self.tokenizer.encode(current_chunk))\n",
    "            if current_tokens > max_tokens:\n",
    "                # Still too large, split by sentences\n",
    "                sentence_chunks = self.sentence_split_chunk(current_chunk, max_tokens)\n",
    "                chunks.extend(sentence_chunks)\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    'content': current_chunk.strip(),\n",
    "                    'chunk_type': 'semantic_paragraph',\n",
    "                    'level': 5,\n",
    "                    'heading': 'Semantic Chunk'\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def sentence_split_chunk(self, text: str, max_tokens: int = 512) -> List[Dict]:\n",
    "        \"\"\"Split text by sentences when paragraph splitting isn't enough\"\"\"\n",
    "        # Split by sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            # Add sentence terminator back\n",
    "            sentence_with_punct = sentence + \".\"\n",
    "            test_chunk = current_chunk + \" \" + sentence_with_punct if current_chunk else sentence_with_punct\n",
    "            test_tokens = len(self.tokenizer.encode(test_chunk))\n",
    "            \n",
    "            if test_tokens > max_tokens and current_chunk:\n",
    "                # Current chunk is full, save it\n",
    "                chunks.append({\n",
    "                    'content': current_chunk.strip(),\n",
    "                    'chunk_type': 'semantic_sentence',\n",
    "                    'level': 6,\n",
    "                    'heading': 'Sentence Chunk'\n",
    "                })\n",
    "                current_chunk = sentence_with_punct\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'content': current_chunk.strip(),\n",
    "                'chunk_type': 'semantic_sentence',\n",
    "                'level': 6,\n",
    "                'heading': 'Sentence Chunk'\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def fixed_size_split_chunk(self, text: str, chunk_size: int = 512, overlap: int = 64) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Final fallback: split by fixed token size with overlap\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        chunks = []\n",
    "        \n",
    "        start = 0\n",
    "        chunk_num = 1\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            end = min(start + chunk_size, len(tokens))\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            \n",
    "            # Convert back to text (simplified for mock tokenizer)\n",
    "            if isinstance(chunk_tokens[0], str):\n",
    "                chunk_text = \" \".join(chunk_tokens)\n",
    "            else:\n",
    "                chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            \n",
    "            chunks.append({\n",
    "                'content': chunk_text.strip(),\n",
    "                'chunk_type': 'fixed_size',\n",
    "                'level': 7,  # Lowest priority\n",
    "                'heading': f'Fixed Chunk {chunk_num}'\n",
    "            })\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = end - overlap if end < len(tokens) else len(tokens)\n",
    "            chunk_num += 1\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def recursive_chunk_text(self, text: str, max_tokens: int = 1024, \n",
    "                           semantic_max_tokens: int = 512, \n",
    "                           fixed_chunk_size: int = 256,\n",
    "                           fixed_overlap: int = 64) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Main recursive chunking logic:\n",
    "        1. Try structural chunking\n",
    "        2. If chunks too large, apply semantic chunking\n",
    "        3. If still too large, apply fixed-size chunking\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            max_tokens: Maximum tokens for structural chunks before fallback\n",
    "            semantic_max_tokens: Maximum tokens for semantic chunks before fallback\n",
    "            fixed_chunk_size: Size for fixed chunking fallback\n",
    "            fixed_overlap: Overlap for fixed chunking\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks with metadata\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        # Step 1: Try structural chunking\n",
    "        structural_chunks = self.get_structural_chunks(text)\n",
    "        \n",
    "        final_chunks = []\n",
    "        \n",
    "        for chunk in structural_chunks:\n",
    "            chunk_content = chunk['content']\n",
    "            chunk_tokens = len(self.tokenizer.encode(chunk_content))\n",
    "            \n",
    "            if chunk_tokens <= max_tokens:\n",
    "                # Chunk is acceptable size, keep as is\n",
    "                final_chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'chunk_type': chunk['chunk_type'],\n",
    "                    'level': chunk['level'],\n",
    "                    'heading': chunk['heading'],\n",
    "                    'token_count': chunk_tokens,\n",
    "                    'processing_level': 'structural'\n",
    "                })\n",
    "            else:\n",
    "                # Chunk too large, apply semantic chunking\n",
    "                semantic_chunks = self.semantic_split_chunk(chunk_content, semantic_max_tokens)\n",
    "                \n",
    "                for sem_chunk in semantic_chunks:\n",
    "                    sem_tokens = len(self.tokenizer.encode(sem_chunk['content']))\n",
    "                    \n",
    "                    if sem_tokens <= semantic_max_tokens:\n",
    "                        # Semantic chunk is acceptable\n",
    "                        final_chunks.append({\n",
    "                            'content': sem_chunk['content'],\n",
    "                            'chunk_type': sem_chunk['chunk_type'],\n",
    "                            'level': sem_chunk['level'],\n",
    "                            'heading': f\"{chunk['heading']} - {sem_chunk['heading']}\",\n",
    "                            'token_count': sem_tokens,\n",
    "                            'processing_level': 'semantic',\n",
    "                            'parent_heading': chunk['heading']\n",
    "                        })\n",
    "                    else:\n",
    "                        # Still too large, apply fixed-size chunking\n",
    "                        fixed_chunks = self.fixed_size_split_chunk(\n",
    "                            sem_chunk['content'], fixed_chunk_size, fixed_overlap\n",
    "                        )\n",
    "                        \n",
    "                        for i, fix_chunk in enumerate(fixed_chunks):\n",
    "                            fix_tokens = len(self.tokenizer.encode(fix_chunk['content']))\n",
    "                            final_chunks.append({\n",
    "                                'content': fix_chunk['content'],\n",
    "                                'chunk_type': fix_chunk['chunk_type'],\n",
    "                                'level': fix_chunk['level'],\n",
    "                                'heading': f\"{chunk['heading']} - Part {i+1}\",\n",
    "                                'token_count': fix_tokens,\n",
    "                                'processing_level': 'fixed',\n",
    "                                'parent_heading': chunk['heading'],\n",
    "                                'semantic_parent': sem_chunk['heading']\n",
    "                            })\n",
    "        \n",
    "        return final_chunks\n",
    "    \n",
    "    def process_document(self, doc: Dict, max_tokens: int = 1024,\n",
    "                        semantic_max_tokens: int = 512,\n",
    "                        fixed_chunk_size: int = 256,\n",
    "                        fixed_overlap: int = 64) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document with recursive chunking\n",
    "        \n",
    "        Args:\n",
    "            doc: Document dictionary with url, title, content\n",
    "            max_tokens: Maximum tokens for structural chunks\n",
    "            semantic_max_tokens: Maximum tokens for semantic chunks  \n",
    "            fixed_chunk_size: Size for fixed chunking fallback\n",
    "            fixed_overlap: Overlap for fixed chunking\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with metadata\n",
    "        \"\"\"\n",
    "        content = doc.get('content', '')\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        recursive_chunks = self.recursive_chunk_text(\n",
    "            content, max_tokens, semantic_max_tokens, fixed_chunk_size, fixed_overlap\n",
    "        )\n",
    "        \n",
    "        chunk_objects = []\n",
    "        for i, chunk_data in enumerate(recursive_chunks):\n",
    "            chunk_text = chunk_data['content']\n",
    "            if chunk_text.strip():\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{doc['url']}#recursive_chunk_{i}\",\n",
    "                    'source_url': doc['url'],\n",
    "                    'source_title': doc.get('title', ''),\n",
    "                    'content': chunk_text.strip(),\n",
    "                    'chunk_index': i,\n",
    "                    'token_count': chunk_data.get('token_count', len(self.tokenizer.encode(chunk_text))),\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'strategy': 'recursive',\n",
    "                    'strategy_params': {\n",
    "                        'max_tokens': max_tokens,\n",
    "                        'semantic_max_tokens': semantic_max_tokens,\n",
    "                        'fixed_chunk_size': fixed_chunk_size,\n",
    "                        'fixed_overlap': fixed_overlap\n",
    "                    },\n",
    "                    'recursive_info': {\n",
    "                        'chunk_type': chunk_data['chunk_type'],\n",
    "                        'level': chunk_data['level'],\n",
    "                        'heading': chunk_data['heading'],\n",
    "                        'processing_level': chunk_data['processing_level'],\n",
    "                        'parent_heading': chunk_data.get('parent_heading', ''),\n",
    "                        'semantic_parent': chunk_data.get('semantic_parent', '')\n",
    "                    }\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects\n",
    "\n",
    "\n",
    "def load_scraped_data() -> List[Dict]:\n",
    "    \"\"\"Load scraped data from final.json\"\"\"\n",
    "    try:\n",
    "        with open('final.json', 'r', encoding='utf-8') as f:\n",
    "            scraped_data = json.load(f)\n",
    "        \n",
    "        formatted_data = []\n",
    "        for item in scraped_data:\n",
    "            if 'content' in item:\n",
    "                formatted_item = item\n",
    "            elif 'text' in item:\n",
    "                formatted_item = {\n",
    "                    \"url\": item.get(\"url\", \"unknown\"),\n",
    "                    \"title\": item.get(\"title\", item.get(\"url\", \"unknown\")),\n",
    "                    \"content\": item[\"text\"]\n",
    "                }\n",
    "            else:\n",
    "                continue\n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        return formatted_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ final.json not found\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading final.json: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    directories = [\n",
    "        'data/chunks/recursive',\n",
    "        'results/ablations'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Create output directories\n",
    "    create_output_directories()\n",
    "    \n",
    "    # Initialize recursive chunker\n",
    "    print(\"ðŸš€ Initializing Recursive Chunker...\")\n",
    "    recursive_chunker = RecursiveChunker()\n",
    "\n",
    "    # Load data\n",
    "    documents = load_scraped_data()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âŒ No documents loaded. Please check your final.json file.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ… Loaded {len(documents)} documents for recursive chunking\")\n",
    "    \n",
    "    # Define recursive chunking configurations to test\n",
    "    recursive_configs = [\n",
    "        {\n",
    "            \"name\": \"Recursive_Balanced\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"semantic_max_tokens\": 512,\n",
    "            \"fixed_chunk_size\": 256,\n",
    "            \"fixed_overlap\": 64\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Recursive_Large\", \n",
    "            \"max_tokens\": 1536,\n",
    "            \"semantic_max_tokens\": 768,\n",
    "            \"fixed_chunk_size\": 384,\n",
    "            \"fixed_overlap\": 96\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Recursive_Small\",\n",
    "            \"max_tokens\": 768,\n",
    "            \"semantic_max_tokens\": 384,\n",
    "            \"fixed_chunk_size\": 192,\n",
    "            \"fixed_overlap\": 48\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process documents with each recursive configuration\n",
    "    recursive_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ”„ Processing documents with recursive chunking strategies...\")\n",
    "    print(\"=\" * 75)\n",
    "    \n",
    "    for config in recursive_configs:\n",
    "        config_name = config[\"name\"]\n",
    "        max_tokens = config[\"max_tokens\"]\n",
    "        semantic_max_tokens = config[\"semantic_max_tokens\"] \n",
    "        fixed_chunk_size = config[\"fixed_chunk_size\"]\n",
    "        fixed_overlap = config[\"fixed_overlap\"]\n",
    "        \n",
    "        print(f\"\\nðŸ“ Processing: {config_name}\")\n",
    "        print(f\"   Max tokens: {max_tokens}\")\n",
    "        print(f\"   Semantic max tokens: {semantic_max_tokens}\")\n",
    "        print(f\"   Fixed chunk size: {fixed_chunk_size}\")\n",
    "        print(f\"   Fixed overlap: {fixed_overlap}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            chunks = recursive_chunker.process_document(\n",
    "                doc, max_tokens, semantic_max_tokens, fixed_chunk_size, fixed_overlap\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"   Document {doc_idx + 1}: Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chunks = len(all_chunks)\n",
    "        total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "        avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        # Count different processing levels and chunk types\n",
    "        processing_levels = {}\n",
    "        chunk_types = {}\n",
    "        for chunk in all_chunks:\n",
    "            recursive_info = chunk.get('recursive_info', {})\n",
    "            processing_level = recursive_info.get('processing_level', 'unknown')\n",
    "            chunk_type = recursive_info.get('chunk_type', 'unknown')\n",
    "            \n",
    "            processing_levels[processing_level] = processing_levels.get(processing_level, 0) + 1\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "        \n",
    "        recursive_results[config_name] = {\n",
    "            'chunks': all_chunks,\n",
    "            'total_chunks': total_chunks,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "            'processing_time': processing_time,\n",
    "            'config': config,\n",
    "            'processing_levels': processing_levels,\n",
    "            'chunk_types': chunk_types\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Generated {total_chunks} chunks\")\n",
    "        print(f\"   â±ï¸  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"   ðŸ“Š Average tokens per chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "        print(f\"   ðŸ”„ Processing levels: {list(processing_levels.keys())}\")\n",
    "        print(f\"   ðŸ“‹ Chunk types: {list(chunk_types.keys())}\")\n",
    "        \n",
    "        # Save chunks to file\n",
    "        output_file = f\"data/chunks/recursive/{config_name}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"   ðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 75)\n",
    "    print(\"âœ… Recursive chunking completed for all configurations!\")\n",
    "    \n",
    "    # Create summary comparison\n",
    "    print(\"\\nðŸ“ˆ RECURSIVE CHUNKING SUMMARY:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    recursive_summary = []\n",
    "    for name, result in recursive_results.items():\n",
    "        config = result['config']\n",
    "        recursive_summary.append({\n",
    "            'Strategy': name,\n",
    "            'Max Tokens': config['max_tokens'],\n",
    "            'Semantic Max': config['semantic_max_tokens'],\n",
    "            'Fixed Size': config['fixed_chunk_size'],\n",
    "            'Fixed Overlap': config['fixed_overlap'],\n",
    "            'Total Chunks': result['total_chunks'],\n",
    "            'Avg Tokens/Chunk': f\"{result['avg_tokens_per_chunk']:.1f}\",\n",
    "            'Processing Time (s)': f\"{result['processing_time']:.2f}\",\n",
    "            'Processing Levels': len(result['processing_levels'])\n",
    "        })\n",
    "    \n",
    "    recursive_df = pd.DataFrame(recursive_summary)\n",
    "    print(recursive_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    recursive_df.to_csv('results/ablations/recursive_chunking_summary.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Summary saved to: results/ablations/recursive_chunking_summary.csv\")\n",
    "    \n",
    "    # Show sample chunks with recursive information\n",
    "    print(\"\\nðŸ” SAMPLE CHUNKS WITH RECURSIVE INFO:\")\n",
    "    print(\"=\" * 100)\n",
    "    for strategy_name, result in recursive_results.items():\n",
    "        if result['chunks']:\n",
    "            sample_chunk = result['chunks'][0]\n",
    "            recursive_info = sample_chunk.get('recursive_info', {})\n",
    "            print(f\"\\nðŸ“„ {strategy_name}:\")\n",
    "            print(f\"   Tokens: {sample_chunk['token_count']}\")\n",
    "            print(f\"   Processing Level: {recursive_info.get('processing_level', 'N/A')}\")\n",
    "            print(f\"   Chunk Type: {recursive_info.get('chunk_type', 'N/A')}\")\n",
    "            print(f\"   Level: {recursive_info.get('level', 'N/A')}\")\n",
    "            print(f\"   Heading: {recursive_info.get('heading', 'N/A')}\")\n",
    "            print(f\"   Parent Heading: {recursive_info.get('parent_heading', 'N/A')}\")\n",
    "            print(f\"   Content preview: {sample_chunk['content'][:200]}...\")\n",
    "            print(f\"   Source: {sample_chunk['source_title']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292815f8",
   "metadata": {},
   "source": [
    "# LLM Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8476b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: google-generativeai in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (0.8.5)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (2.182.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (2.11.9)\n",
      "Requirement already satisfied: tqdm in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\main files\\soorya\\7thsem\\llm_prod\\exp2\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: python-dotenv, google-ai-generativelanguage\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "    Found existing installation: google-ai-generativelanguage 0.7.0\n",
      "    Uninstalling google-ai-generativelanguage-0.7.0:\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.7.0\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing LLM Chunker with Gemini 2.0 Flash-Lite...\n",
      "âœ… LLM chunker initialized with Gemini 2.0 Flash-Lite\n",
      "\n",
      "âœ… Loaded 139 documents for LLM-based chunking\n",
      "\n",
      "ðŸ”„ Processing documents with LLM-based chunking strategies...\n",
      "===========================================================================\n",
      "\n",
      "ðŸ“ Processing: LLM_Medium_Chunks\n",
      "   Max chunk tokens: 512\n",
      "   Processing document 1/139...\n",
      "   Sending request to Gemini (estimated 981 input tokens)...\n",
      "   âœ… Generated 4 chunks in 7.32s\n",
      "     Generated 4 chunks\n",
      "   Processing document 2/139...\n",
      "   Sending request to Gemini (estimated 547 input tokens)...\n",
      "   âœ… Generated 4 chunks in 6.26s\n",
      "     Generated 4 chunks\n",
      "   Processing document 3/139...\n",
      "   Sending request to Gemini (estimated 1680 input tokens)...\n",
      "   âœ… Generated 7 chunks in 9.77s\n",
      "     Generated 7 chunks\n",
      "   Processing document 4/139...\n",
      "   Sending request to Gemini (estimated 3138 input tokens)...\n",
      "   âœ… Generated 17 chunks in 21.16s\n",
      "     Generated 17 chunks\n",
      "   Processing document 5/139...\n",
      "   Sending request to Gemini (estimated 3137 input tokens)...\n",
      "   âœ… Generated 17 chunks in 19.51s\n",
      "     Generated 17 chunks\n",
      "   Processing document 6/139...\n",
      "   Sending request to Gemini (estimated 4066 input tokens)...\n",
      "   âœ… Generated 15 chunks in 19.71s\n",
      "     Generated 15 chunks\n",
      "   Processing document 7/139...\n",
      "   Sending request to Gemini (estimated 2939 input tokens)...\n",
      "   âœ… Generated 19 chunks in 25.75s\n",
      "     Generated 19 chunks\n",
      "   Processing document 8/139...\n",
      "   Sending request to Gemini (estimated 780 input tokens)...\n",
      "   âœ… Generated 5 chunks in 9.36s\n",
      "     Generated 5 chunks\n",
      "   Processing document 9/139...\n",
      "   Sending request to Gemini (estimated 16000 input tokens)...\n",
      "   âŒ JSON parsing error: Expecting ',' delimiter: line 340 column 78 (char 33960)\n",
      "   Response preview: ```json\n",
      "[\n",
      "  {\n",
      "    \"chunk_number\": 1,\n",
      "    \"heading\": \"Merchant Application Form Overview and Contact Information\",\n",
      "    \"content\": \"This document is the Jio Payment Solutions Limited (JPSL) Merchant Application Form (MAF). The registered address is 5th Floor, Court House, Lokmanya Tilak Marg, Dhobi Talao, Mumbai - 400002, Maharashtra, India. The communication address is DAKC Gate 4, Dhirubhai Ambani Knowledge City, MIDC Industrial Area, Building No. 25, B-Wing, 2nd Floor, Thane Belapur Road, Kopar...\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 10/139...\n",
      "   Sending request to Gemini (estimated 1118 input tokens)...\n",
      "   âœ… Generated 6 chunks in 10.05s\n",
      "     Generated 6 chunks\n",
      "   Processing document 11/139...\n",
      "   Sending request to Gemini (estimated 1288 input tokens)...\n",
      "   âœ… Generated 14 chunks in 14.91s\n",
      "     Generated 14 chunks\n",
      "   Processing document 12/139...\n",
      "   Sending request to Gemini (estimated 1205 input tokens)...\n",
      "   âœ… Generated 7 chunks in 10.09s\n",
      "     Generated 7 chunks\n",
      "   Processing document 13/139...\n",
      "   Sending request to Gemini (estimated 5320 input tokens)...\n",
      "   âœ… Generated 15 chunks in 21.90s\n",
      "     Generated 15 chunks\n",
      "   Processing document 14/139...\n",
      "   Sending request to Gemini (estimated 877 input tokens)...\n",
      "   âœ… Generated 6 chunks in 8.08s\n",
      "     Generated 6 chunks\n",
      "   Processing document 15/139...\n",
      "   Sending request to Gemini (estimated 708 input tokens)...\n",
      "   âœ… Generated 18 chunks in 17.80s\n",
      "     Generated 18 chunks\n",
      "   Processing document 16/139...\n",
      "   Sending request to Gemini (estimated 502 input tokens)...\n",
      "   âœ… Generated 3 chunks in 4.83s\n",
      "     Generated 3 chunks\n",
      "   Processing document 17/139...\n",
      "   Sending request to Gemini (estimated 16938 input tokens)...\n",
      "   âŒ JSON parsing error: Expecting ',' delimiter: line 360 column 81 (char 33350)\n",
      "   Response preview: ```json\n",
      "[\n",
      "  {\n",
      "    \"chunk_number\": 1,\n",
      "    \"heading\": \"Introduction and Acceptance of Terms and Conditions\",\n",
      "    \"content\": \"Jio Payments Bank Limited Terms and Conditions. Please read the Terms carefully before availing/registering for any Bank Services. By clicking on \\\"Submit\\\" and/or availing any Bank Services and/or accessing/using the Platform/Bank Services and/or downloading the Mobile App, you agree to the Terms set-out hereunder and shall be bound by the same. If you do not wish to avail ...\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 18/139...\n",
      "   Sending request to Gemini (estimated 755 input tokens)...\n",
      "   âœ… Generated 8 chunks in 8.94s\n",
      "     Generated 8 chunks\n",
      "   Processing document 19/139...\n",
      "   Sending request to Gemini (estimated 1013 input tokens)...\n",
      "   âœ… Generated 10 chunks in 14.82s\n",
      "     Generated 10 chunks\n",
      "   Processing document 20/139...\n",
      "   Sending request to Gemini (estimated 941 input tokens)...\n",
      "   âœ… Generated 5 chunks in 6.83s\n",
      "     Generated 5 chunks\n",
      "   Processing document 21/139...\n",
      "   Sending request to Gemini (estimated 1274 input tokens)...\n",
      "   âœ… Generated 7 chunks in 9.87s\n",
      "     Generated 7 chunks\n",
      "   Processing document 22/139...\n",
      "   Sending request to Gemini (estimated 1095 input tokens)...\n",
      "   âœ… Generated 6 chunks in 9.01s\n",
      "     Generated 6 chunks\n",
      "   Processing document 23/139...\n",
      "   Sending request to Gemini (estimated 1064 input tokens)...\n",
      "   âœ… Generated 6 chunks in 7.82s\n",
      "     Generated 6 chunks\n",
      "   Processing document 24/139...\n",
      "   Sending request to Gemini (estimated 1038 input tokens)...\n",
      "   âœ… Generated 10 chunks in 12.46s\n",
      "     Generated 10 chunks\n",
      "   Processing document 25/139...\n",
      "   Sending request to Gemini (estimated 1046 input tokens)...\n",
      "   âœ… Generated 7 chunks in 9.32s\n",
      "     Generated 7 chunks\n",
      "   Processing document 26/139...\n",
      "   Sending request to Gemini (estimated 1069 input tokens)...\n",
      "   âœ… Generated 12 chunks in 13.77s\n",
      "     Generated 12 chunks\n",
      "   Processing document 27/139...\n",
      "   Sending request to Gemini (estimated 1035 input tokens)...\n",
      "   âœ… Generated 11 chunks in 12.33s\n",
      "     Generated 11 chunks\n",
      "   Processing document 28/139...\n",
      "   Sending request to Gemini (estimated 1037 input tokens)...\n",
      "   âœ… Generated 11 chunks in 13.61s\n",
      "     Generated 11 chunks\n",
      "   Processing document 29/139...\n",
      "   Sending request to Gemini (estimated 1052 input tokens)...\n",
      "   âœ… Generated 11 chunks in 12.49s\n",
      "     Generated 11 chunks\n",
      "   Processing document 30/139...\n",
      "   Sending request to Gemini (estimated 1152 input tokens)...\n",
      "   âœ… Generated 13 chunks in 15.27s\n",
      "     Generated 13 chunks\n",
      "   Processing document 31/139...\n",
      "   Sending request to Gemini (estimated 1018 input tokens)...\n",
      "   âœ… Generated 10 chunks in 12.03s\n",
      "     Generated 10 chunks\n",
      "   Processing document 32/139...\n",
      "   Sending request to Gemini (estimated 1028 input tokens)...\n",
      "   âœ… Generated 11 chunks in 13.30s\n",
      "     Generated 11 chunks\n",
      "   Processing document 33/139...\n",
      "   Sending request to Gemini (estimated 998 input tokens)...\n",
      "   âœ… Generated 10 chunks in 12.43s\n",
      "     Generated 10 chunks\n",
      "   Processing document 34/139...\n",
      "   Sending request to Gemini (estimated 1053 input tokens)...\n",
      "   âœ… Generated 9 chunks in 11.75s\n",
      "     Generated 9 chunks\n",
      "   Processing document 35/139...\n",
      "   Sending request to Gemini (estimated 1159 input tokens)...\n",
      "   âœ… Generated 13 chunks in 16.19s\n",
      "     Generated 13 chunks\n",
      "   Processing document 36/139...\n",
      "   Sending request to Gemini (estimated 1105 input tokens)...\n",
      "   âœ… Generated 10 chunks in 11.68s\n",
      "     Generated 10 chunks\n",
      "   Processing document 37/139...\n",
      "   Sending request to Gemini (estimated 1126 input tokens)...\n",
      "   âœ… Generated 8 chunks in 9.30s\n",
      "     Generated 8 chunks\n",
      "   Processing document 38/139...\n",
      "   Sending request to Gemini (estimated 1049 input tokens)...\n",
      "   âœ… Generated 12 chunks in 13.93s\n",
      "     Generated 12 chunks\n",
      "   Processing document 39/139...\n",
      "   Sending request to Gemini (estimated 2581 input tokens)...\n",
      "   âœ… Generated 13 chunks in 15.05s\n",
      "     Generated 13 chunks\n",
      "   Processing document 40/139...\n",
      "   Sending request to Gemini (estimated 14450 input tokens)...\n",
      "   âœ… Generated 31 chunks in 39.77s\n",
      "     Generated 31 chunks\n",
      "   Processing document 41/139...\n",
      "   Sending request to Gemini (estimated 1329 input tokens)...\n",
      "   âœ… Generated 6 chunks in 9.19s\n",
      "     Generated 6 chunks\n",
      "   Processing document 42/139...\n",
      "   Sending request to Gemini (estimated 3590 input tokens)...\n",
      "   âœ… Generated 11 chunks in 13.50s\n",
      "     Generated 11 chunks\n",
      "   Processing document 43/139...\n",
      "   Sending request to Gemini (estimated 404 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.92s\n",
      "     Generated 1 chunks\n",
      "   Processing document 44/139...\n",
      "   Sending request to Gemini (estimated 368 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.88s\n",
      "     Generated 1 chunks\n",
      "   Processing document 45/139...\n",
      "   Sending request to Gemini (estimated 355 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.95s\n",
      "     Generated 1 chunks\n",
      "   Processing document 46/139...\n",
      "   Sending request to Gemini (estimated 346 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.89s\n",
      "     Generated 1 chunks\n",
      "   Processing document 47/139...\n",
      "   Sending request to Gemini (estimated 406 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.31s\n",
      "     Generated 1 chunks\n",
      "   Processing document 48/139...\n",
      "   Sending request to Gemini (estimated 390 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.53s\n",
      "     Generated 1 chunks\n",
      "   Processing document 49/139...\n",
      "   Sending request to Gemini (estimated 343 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.14s\n",
      "     Generated 1 chunks\n",
      "   Processing document 50/139...\n",
      "   Sending request to Gemini (estimated 414 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.92s\n",
      "     Generated 1 chunks\n",
      "   Processing document 51/139...\n",
      "   Sending request to Gemini (estimated 348 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.14s\n",
      "     Generated 1 chunks\n",
      "   Processing document 52/139...\n",
      "   Sending request to Gemini (estimated 411 input tokens)...\n",
      "   âœ… Generated 2 chunks in 4.01s\n",
      "     Generated 2 chunks\n",
      "   Processing document 53/139...\n",
      "   Sending request to Gemini (estimated 350 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.03s\n",
      "     Generated 1 chunks\n",
      "   Processing document 54/139...\n",
      "   Sending request to Gemini (estimated 381 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.67s\n",
      "     Generated 1 chunks\n",
      "   Processing document 55/139...\n",
      "   Sending request to Gemini (estimated 366 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.84s\n",
      "     Generated 1 chunks\n",
      "   Processing document 56/139...\n",
      "   Sending request to Gemini (estimated 351 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.77s\n",
      "     Generated 1 chunks\n",
      "   Processing document 57/139...\n",
      "   Sending request to Gemini (estimated 363 input tokens)...\n",
      "   âœ… Generated 1 chunks in 1.70s\n",
      "     Generated 1 chunks\n",
      "   Processing document 58/139...\n",
      "   Sending request to Gemini (estimated 345 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.02s\n",
      "     Generated 1 chunks\n",
      "   Processing document 59/139...\n",
      "   Sending request to Gemini (estimated 396 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 40.97139042s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 60/139...\n",
      "   Sending request to Gemini (estimated 349 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 40.590529135s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 61/139...\n",
      "   Sending request to Gemini (estimated 332 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 40.253327083s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 62/139...\n",
      "   Sending request to Gemini (estimated 369 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 39.922488564s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 39\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 63/139...\n",
      "   Sending request to Gemini (estimated 353 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 39.587475617s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 39\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 64/139...\n",
      "   Sending request to Gemini (estimated 362 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 39.200780155s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 39\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 65/139...\n",
      "   Sending request to Gemini (estimated 374 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 38.85786565s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 66/139...\n",
      "   Sending request to Gemini (estimated 325 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 38.502167554s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 67/139...\n",
      "   Sending request to Gemini (estimated 352 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 38.165092759s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 68/139...\n",
      "   Sending request to Gemini (estimated 370 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 37.746151332s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 37\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 69/139...\n",
      "   Sending request to Gemini (estimated 373 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 37.333501549s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 37\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 70/139...\n",
      "   Sending request to Gemini (estimated 403 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 36.917161967s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 71/139...\n",
      "   Sending request to Gemini (estimated 365 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 36.524276908s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 72/139...\n",
      "   Sending request to Gemini (estimated 366 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 36.064387553s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 73/139...\n",
      "   Sending request to Gemini (estimated 479 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 35.731760497s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 35\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 74/139...\n",
      "   Sending request to Gemini (estimated 434 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 35.364658754s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 35\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 75/139...\n",
      "   Sending request to Gemini (estimated 354 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 34.79177384s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 76/139...\n",
      "   Sending request to Gemini (estimated 390 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 34.461337632s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 77/139...\n",
      "   Sending request to Gemini (estimated 434 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 34.122135022s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 78/139...\n",
      "   Sending request to Gemini (estimated 371 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 33.800299651s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 79/139...\n",
      "   Sending request to Gemini (estimated 343 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 33.469235092s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 80/139...\n",
      "   Sending request to Gemini (estimated 362 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 33.004927746s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 81/139...\n",
      "   Sending request to Gemini (estimated 396 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 32.539946033s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 82/139...\n",
      "   Sending request to Gemini (estimated 371 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 32.124900174s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 83/139...\n",
      "   Sending request to Gemini (estimated 340 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 31.79771791s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 84/139...\n",
      "   Sending request to Gemini (estimated 365 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 31.419561048s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 85/139...\n",
      "   Sending request to Gemini (estimated 434 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 31.080435852s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 86/139...\n",
      "   Sending request to Gemini (estimated 366 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 30.748732821s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 87/139...\n",
      "   Sending request to Gemini (estimated 336 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 30.415832574s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 88/139...\n",
      "   Sending request to Gemini (estimated 413 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 30.046885162s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 89/139...\n",
      "   Sending request to Gemini (estimated 373 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 29.638555198s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 90/139...\n",
      "   Sending request to Gemini (estimated 377 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 29.312558522s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 91/139...\n",
      "   Sending request to Gemini (estimated 361 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 28.965213974s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 92/139...\n",
      "   Sending request to Gemini (estimated 349 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 28.62726328s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 93/139...\n",
      "   Sending request to Gemini (estimated 371 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 28.232007525s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 94/139...\n",
      "   Sending request to Gemini (estimated 344 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 27.90528006s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 95/139...\n",
      "   Sending request to Gemini (estimated 397 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 27.579623703s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 96/139...\n",
      "   Sending request to Gemini (estimated 431 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 27.161816987s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 97/139...\n",
      "   Sending request to Gemini (estimated 393 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 25.9093288s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 98/139...\n",
      "   Sending request to Gemini (estimated 354 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 25.505884436s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 99/139...\n",
      "   Sending request to Gemini (estimated 390 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 25.162829437s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 100/139...\n",
      "   Sending request to Gemini (estimated 336 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 24.256656348s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 101/139...\n",
      "   Sending request to Gemini (estimated 333 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 23.905439163s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 102/139...\n",
      "   Sending request to Gemini (estimated 344 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 23.533086091s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 103/139...\n",
      "   Sending request to Gemini (estimated 346 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 22.313534881s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 104/139...\n",
      "   Sending request to Gemini (estimated 345 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 21.885761448s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 105/139...\n",
      "   Sending request to Gemini (estimated 381 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 20.725640303s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 20\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 106/139...\n",
      "   Sending request to Gemini (estimated 327 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 20.26757301s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 20\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 107/139...\n",
      "   Sending request to Gemini (estimated 387 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 19.9390035s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 108/139...\n",
      "   Sending request to Gemini (estimated 371 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 19.573004291s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 109/139...\n",
      "   Sending request to Gemini (estimated 332 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 19.263719677s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 110/139...\n",
      "   Sending request to Gemini (estimated 327 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 18.928375223s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 111/139...\n",
      "   Sending request to Gemini (estimated 363 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.00s\n",
      "     Generated 1 chunks\n",
      "   Processing document 112/139...\n",
      "   Sending request to Gemini (estimated 340 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 16.595744875s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 113/139...\n",
      "   Sending request to Gemini (estimated 333 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 16.263374199s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 114/139...\n",
      "   Sending request to Gemini (estimated 333 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 15.871805864s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 115/139...\n",
      "   Sending request to Gemini (estimated 328 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 15.439803705s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 116/139...\n",
      "   Sending request to Gemini (estimated 427 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 15.038754792s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 117/139...\n",
      "   Sending request to Gemini (estimated 331 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 14.631498326s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 118/139...\n",
      "   Sending request to Gemini (estimated 346 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 14.194516785s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 119/139...\n",
      "   Sending request to Gemini (estimated 354 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 13.819874052s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 120/139...\n",
      "   Sending request to Gemini (estimated 365 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 13.492875654s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 121/139...\n",
      "   Sending request to Gemini (estimated 439 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.66s\n",
      "     Generated 1 chunks\n",
      "   Processing document 122/139...\n",
      "   Sending request to Gemini (estimated 337 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 10.425599172s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 123/139...\n",
      "   Sending request to Gemini (estimated 336 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 10.101765015s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 124/139...\n",
      "   Sending request to Gemini (estimated 349 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 9.77361375s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 9\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 125/139...\n",
      "   Sending request to Gemini (estimated 384 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 9.400375163s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 9\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 126/139...\n",
      "   Sending request to Gemini (estimated 369 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 8.996666995s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 127/139...\n",
      "   Sending request to Gemini (estimated 378 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 8.66553862s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 128/139...\n",
      "   Sending request to Gemini (estimated 332 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 8.313875974s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 129/139...\n",
      "   Sending request to Gemini (estimated 380 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 7.847222237s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 130/139...\n",
      "   Sending request to Gemini (estimated 384 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 7.477120175s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 131/139...\n",
      "   Sending request to Gemini (estimated 368 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 5.648448843s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 132/139...\n",
      "   Sending request to Gemini (estimated 343 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 5.263458682s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 133/139...\n",
      "   Sending request to Gemini (estimated 344 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.15s\n",
      "     Generated 1 chunks\n",
      "   Processing document 134/139...\n",
      "   Sending request to Gemini (estimated 491 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 2.717782795s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 135/139...\n",
      "   Sending request to Gemini (estimated 407 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.12s\n",
      "     Generated 1 chunks\n",
      "   Processing document 136/139...\n",
      "   Sending request to Gemini (estimated 343 input tokens)...\n",
      "   âœ… Generated 1 chunks in 2.48s\n",
      "     Generated 1 chunks\n",
      "   Processing document 137/139...\n",
      "   Sending request to Gemini (estimated 356 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 57.757579789s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 138/139...\n",
      "   Sending request to Gemini (estimated 429 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 57.325124423s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   Processing document 139/139...\n",
      "   Sending request to Gemini (estimated 411 input tokens)...\n",
      "   âŒ Error calling Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\n",
      "Please retry in 56.999843115s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "   ðŸ”„ LLM chunking failed, using fallback method...\n",
      "     Generated 1 chunks\n",
      "   âœ… Generated 518 chunks (440 LLM, 78 fallback)\n",
      "   â±ï¸  Processing time: 709.86s\n",
      "   ðŸ“Š Average tokens per chunk: 125.7\n",
      "   ðŸ’° Estimated cost: $0.0371\n",
      "   ðŸ“‹ Topic categories: ['kyc', 'KYC', 'policies', 'troubleshooting', 'general', 'onboarding', 'payments', 'security', 'procedural', 'Policies', 'General Information', 'Products', 'Legal & Contact', 'support', 'legal', 'compliance']\n",
      "   ðŸ·ï¸  Chunk types: ['declarative', 'procedural', 'reference', 'troubleshooting', 'fallback']\n",
      "   ðŸ’¾ Saved to: data/chunks/llm_based/LLM_Medium_Chunks.json\n",
      "\n",
      "===========================================================================\n",
      "âœ… LLM-based chunking completed for all configurations!\n",
      "\n",
      "ðŸ“ˆ LLM CHUNKING SUMMARY:\n",
      "====================================================================================================\n",
      "         Strategy  Max Tokens  Total Chunks  LLM Generated  Fallback Used Avg Tokens/Chunk Processing Time (s) Total Cost ($) Cost per Chunk ($)\n",
      "LLM_Medium_Chunks         512           518            440             78            125.7              709.86         0.0371           0.000072\n",
      "\n",
      "ðŸ’¾ Summary saved to: results/ablations/llm_chunking_summary.csv\n",
      "\n",
      "ðŸ” SAMPLE CHUNKS WITH LLM INFO:\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“„ LLM_Medium_Chunks:\n",
      "   Tokens: 176\n",
      "   LLM Generated: True\n",
      "   Topic Category: kyc\n",
      "   Chunk Type: declarative\n",
      "   Heading: Declaration Requirements for Sole Proprietor Merchants - Overview\n",
      "   Key Concepts: ['declaration', 'sole proprietor', 'POB', 'DKYC', 'settlement bank account', 'POI']\n",
      "   Content preview: This document outlines the declaration requirements for Jio Payment Solutions Ltd (JPSL) merchant partners who are sole proprietors. This declaration is specifically required when the owner's name or ...\n",
      "   Source: annexure_form_A.pdf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LLM-BASED CHUNKING IMPLEMENTATION\n",
    "# Strategy 5: Instruction-aware segmentation using Gemini 2.0 Flash-Lite\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Mock tokenizer for demonstration - replace with your actual tokenizer\n",
    "class MockTokenizer:\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Simple mock tokenizer - replace with actual tokenizer\"\"\"\n",
    "        return text.split()  # Simple word-based tokenization\n",
    "\n",
    "# Initialize mock tokenizer - replace with your actual tokenizer\n",
    "tokenizer = MockTokenizer()\n",
    "\n",
    "class LLMChunker:\n",
    "    \"\"\"\n",
    "    Implementation of LLM-based chunking using Gemini 2.0 Flash-Lite\n",
    "    for instruction-aware segmentation with cost vs quality analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize LLM chunker with Gemini API key\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=api_key)\n",
    "        \n",
    "        # Initialize Gemini 2.0 Flash-Lite model\n",
    "        self.model = genai.GenerativeModel(\n",
    "        \"gemini-2.0-flash-lite-001\",  # consistent with test script\n",
    "        generation_config={\n",
    "            \"temperature\": 0.1,   # Low temperature for consistent chunking\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40,\n",
    "            \"max_output_tokens\": 8192,\n",
    "        },\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "    )\n",
    "        \n",
    "        # Cost tracking\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_requests = 0\n",
    "        \n",
    "        print(\"âœ… LLM chunker initialized with Gemini 2.0 Flash-Lite\")\n",
    "    \n",
    "    def create_chunking_prompt(self, document_content: str, document_title: str = \"\", \n",
    "                              max_chunk_tokens: int = 512) -> str:\n",
    "        \"\"\"\n",
    "        Create a comprehensive chunking prompt using the specified format:\n",
    "        Persona, Instruction, Context, Format, Audience, Tone, Data\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"**PERSONA:**\n",
    "You are an expert document processing specialist with deep expertise in information architecture, content analysis, and knowledge management systems. You have extensive experience in preparing documents for retrieval-augmented generation (RAG) systems, particularly for customer support chatbots in the fintech domain.\n",
    "\n",
    "**INSTRUCTION:**\n",
    "Analyze the provided JioPay customer support document and intelligently segment it into coherent, semantically meaningful chunks. Each chunk should:\n",
    "1. Maintain complete contextual meaning and be self-contained\n",
    "2. Preserve logical flow and relationships between concepts\n",
    "3. Respect natural topic boundaries and information hierarchies  \n",
    "4. Optimize for retrieval relevance in customer support scenarios\n",
    "5. Stay within approximately {max_chunk_tokens} tokens per chunk\n",
    "6. Include sufficient context to be understood independently\n",
    "\n",
    "**CONTEXT:**\n",
    "This document contains JioPay customer support information including onboarding procedures, payment processes, KYC requirements, API documentation, troubleshooting guides, and policy information. The chunks will be used in a RAG system to answer customer queries about JioPay services. Each chunk must be retrievable and provide complete answers to potential customer questions.\n",
    "\n",
    "**FORMAT:**\n",
    "Return your response as a valid JSON array where each chunk object contains:\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"chunk_number\": 1,\n",
    "    \"heading\": \"Descriptive heading that captures the main topic\",\n",
    "    \"content\": \"The actual chunk content with complete sentences and context\",\n",
    "    \"topic_category\": \"Primary category (onboarding/payments/kyc/api/security/troubleshooting/policies)\",\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\", \"concept3\"],\n",
    "    \"chunk_type\": \"information_type (procedural/declarative/troubleshooting/reference)\",\n",
    "    \"estimated_tokens\": 450,\n",
    "    \"relevance_keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"]\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\n",
    "**AUDIENCE:**\n",
    "The chunks will serve customer support representatives and automated systems answering queries from JioPay business customers who need specific, actionable information about account setup, payment processing, compliance requirements, and technical integration.\n",
    "\n",
    "**TONE:**\n",
    "Maintain a professional, precise, and analytical approach. Focus on clarity and accuracy. Preserve the original document's authoritative tone while ensuring each chunk provides complete, actionable information.\n",
    "\n",
    "**DATA:**\n",
    "Document Title: \"{document_title}\"\n",
    "Document Content:\n",
    "---\n",
    "{document_content}\n",
    "---\n",
    "\n",
    "Please analyze the above document and create optimal chunks following the specified format. Ensure each chunk is self-contained, semantically coherent, and valuable for customer support retrieval.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count for cost calculation\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_with_llm(self, document_content: str, document_title: str = \"\",\n",
    "                       max_chunk_tokens: int = 512) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Use Gemini to intelligently chunk the document content\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chunks_list, cost_info)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the chunking prompt\n",
    "            prompt = self.create_chunking_prompt(document_content, document_title, max_chunk_tokens)\n",
    "            \n",
    "            # Estimate input cost\n",
    "            input_tokens = self.estimate_tokens(prompt)\n",
    "            self.total_input_tokens += input_tokens\n",
    "            \n",
    "            print(f\"   Sending request to Gemini (estimated {input_tokens} input tokens)...\")\n",
    "            \n",
    "            # Send request to Gemini\n",
    "            start_time = time.time()\n",
    "            response = self.model.generate_content(prompt)  # prompt is string\n",
    "            request_time = time.time() - start_time\n",
    "            \n",
    "            self.total_requests += 1\n",
    "            \n",
    "            if not response.text:\n",
    "                print(\"   âŒ Empty response from Gemini\")\n",
    "                return [], {\"error\": \"Empty response\", \"request_time\": request_time}\n",
    "            \n",
    "            # Estimate output tokens\n",
    "            output_tokens = self.estimate_tokens(response.text)\n",
    "            self.total_output_tokens += output_tokens\n",
    "            \n",
    "            # Parse JSON response\n",
    "            try:\n",
    "                # Clean response text and extract JSON\n",
    "                response_text = response.text.strip()\n",
    "                \n",
    "                # Find JSON array in response\n",
    "                json_start = response_text.find('[')\n",
    "                json_end = response_text.rfind(']') + 1\n",
    "                \n",
    "                if json_start == -1 or json_end == 0:\n",
    "                    print(\"   âŒ No JSON array found in response\")\n",
    "                    return [], {\"error\": \"No JSON found\", \"request_time\": request_time}\n",
    "                \n",
    "                json_text = response_text[json_start:json_end]\n",
    "                chunks_data = json.loads(json_text)\n",
    "                \n",
    "                # Validate and process chunks\n",
    "                processed_chunks = []\n",
    "                for i, chunk in enumerate(chunks_data):\n",
    "                    if not isinstance(chunk, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    processed_chunk = {\n",
    "                        'content': chunk.get('content', '').strip(),\n",
    "                        'heading': chunk.get('heading', f'Chunk {i+1}'),\n",
    "                        'topic_category': chunk.get('topic_category', 'general'),\n",
    "                        'key_concepts': chunk.get('key_concepts', []),\n",
    "                        'chunk_type': chunk.get('chunk_type', 'declarative'),\n",
    "                        'estimated_tokens': chunk.get('estimated_tokens', 0),\n",
    "                        'relevance_keywords': chunk.get('relevance_keywords', []),\n",
    "                        'level': 1,  # LLM-determined chunks get high priority\n",
    "                        'llm_generated': True\n",
    "                    }\n",
    "                    \n",
    "                    if processed_chunk['content']:\n",
    "                        processed_chunks.append(processed_chunk)\n",
    "                \n",
    "                cost_info = {\n",
    "                    'input_tokens': input_tokens,\n",
    "                    'output_tokens': output_tokens,\n",
    "                    'request_time': request_time,\n",
    "                    'chunks_generated': len(processed_chunks),\n",
    "                    'success': True\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… Generated {len(processed_chunks)} chunks in {request_time:.2f}s\")\n",
    "                return processed_chunks, cost_info\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"   âŒ JSON parsing error: {e}\")\n",
    "                print(f\"   Response preview: {response.text[:500]}...\")\n",
    "                return [], {\"error\": f\"JSON parsing: {e}\", \"request_time\": request_time}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error calling Gemini: {e}\")\n",
    "            return [], {\"error\": str(e), \"request_time\": 0}\n",
    "    \n",
    "    def fallback_chunking(self, text: str, max_tokens: int = 512) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fallback chunking when LLM fails (simple paragraph-based)\n",
    "        \"\"\"\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        chunk_num = 1\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            test_chunk = current_chunk + \"\\n\\n\" + para if current_chunk else para\n",
    "            test_tokens = len(self.tokenizer.encode(test_chunk))\n",
    "            \n",
    "            if test_tokens > max_tokens and current_chunk:\n",
    "                chunks.append({\n",
    "                    'content': current_chunk.strip(),\n",
    "                    'heading': f'Fallback Chunk {chunk_num}',\n",
    "                    'topic_category': 'general',\n",
    "                    'key_concepts': [],\n",
    "                    'chunk_type': 'fallback',\n",
    "                    'estimated_tokens': len(self.tokenizer.encode(current_chunk)),\n",
    "                    'relevance_keywords': [],\n",
    "                    'level': 5,  # Lower priority for fallback\n",
    "                    'llm_generated': False\n",
    "                })\n",
    "                current_chunk = para\n",
    "                chunk_num += 1\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'content': current_chunk.strip(),\n",
    "                'heading': f'Fallback Chunk {chunk_num}',\n",
    "                'topic_category': 'general',\n",
    "                'key_concepts': [],\n",
    "                'chunk_type': 'fallback',\n",
    "                'estimated_tokens': len(self.tokenizer.encode(current_chunk)),\n",
    "                'relevance_keywords': [],\n",
    "                'level': 5,\n",
    "                'llm_generated': False\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def llm_chunk_text(self, text: str, title: str = \"\", max_chunk_tokens: int = 512) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Main LLM chunking method\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            title: Document title for context\n",
    "            max_chunk_tokens: Maximum tokens per chunk\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (chunks_list, cost_info)\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return [], {\"error\": \"Empty text\", \"cost\": 0}\n",
    "        \n",
    "        # Try LLM chunking first\n",
    "        chunks, cost_info = self.chunk_with_llm(text, title, max_chunk_tokens)\n",
    "        \n",
    "        if not chunks or not cost_info.get('success', False):\n",
    "            print(\"   ðŸ”„ LLM chunking failed, using fallback method...\")\n",
    "            chunks = self.fallback_chunking(text, max_chunk_tokens)\n",
    "            cost_info['fallback_used'] = True\n",
    "        \n",
    "        return chunks, cost_info\n",
    "    \n",
    "    def process_document(self, doc: Dict, max_chunk_tokens: int = 512) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document with LLM-based chunking\n",
    "        \n",
    "        Args:\n",
    "            doc: Document dictionary with url, title, content\n",
    "            max_chunk_tokens: Maximum tokens per chunk\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (chunk_objects_list, cost_info)\n",
    "        \"\"\"\n",
    "        content = doc.get('content', '')\n",
    "        title = doc.get('title', '')\n",
    "        \n",
    "        if not content.strip():\n",
    "            return [], {\"error\": \"Empty content\", \"cost\": 0}\n",
    "        \n",
    "        llm_chunks, cost_info = self.llm_chunk_text(content, title, max_chunk_tokens)\n",
    "        \n",
    "        chunk_objects = []\n",
    "        for i, chunk_data in enumerate(llm_chunks):\n",
    "            chunk_text = chunk_data['content']\n",
    "            if chunk_text.strip():\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{doc['url']}#llm_chunk_{i}\",\n",
    "                    'source_url': doc['url'],\n",
    "                    'source_title': doc.get('title', ''),\n",
    "                    'content': chunk_text.strip(),\n",
    "                    'chunk_index': i,\n",
    "                    'token_count': len(self.tokenizer.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'strategy': 'llm_based',\n",
    "                    'strategy_params': {\n",
    "                        'max_chunk_tokens': max_chunk_tokens,\n",
    "                        'model': 'gemini-2.0-flash-exp',\n",
    "                        'temperature': 0.1\n",
    "                    },\n",
    "                    'llm_info': {\n",
    "                        'heading': chunk_data['heading'],\n",
    "                        'topic_category': chunk_data['topic_category'],\n",
    "                        'key_concepts': chunk_data['key_concepts'],\n",
    "                        'chunk_type': chunk_data['chunk_type'],\n",
    "                        'estimated_tokens': chunk_data['estimated_tokens'],\n",
    "                        'relevance_keywords': chunk_data['relevance_keywords'],\n",
    "                        'level': chunk_data['level'],\n",
    "                        'llm_generated': chunk_data['llm_generated']\n",
    "                    }\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects, cost_info\n",
    "    \n",
    "    def get_cost_summary(self) -> Dict:\n",
    "        \"\"\"Get comprehensive cost analysis\"\"\"\n",
    "        # Gemini pricing (approximate - update with actual pricing)\n",
    "        input_cost_per_1k = 0.000125  # $0.000125 per 1K input tokens\n",
    "        output_cost_per_1k = 0.000375  # $0.000375 per 1K output tokens\n",
    "        \n",
    "        input_cost = (self.total_input_tokens / 1000) * input_cost_per_1k\n",
    "        output_cost = (self.total_output_tokens / 1000) * output_cost_per_1k\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        return {\n",
    "            'total_requests': self.total_requests,\n",
    "            'total_input_tokens': self.total_input_tokens,\n",
    "            'total_output_tokens': self.total_output_tokens,\n",
    "            'total_tokens': self.total_input_tokens + self.total_output_tokens,\n",
    "            'input_cost_usd': input_cost,\n",
    "            'output_cost_usd': output_cost,\n",
    "            'total_cost_usd': total_cost,\n",
    "            'avg_tokens_per_request': (self.total_input_tokens + self.total_output_tokens) / max(1, self.total_requests),\n",
    "            'cost_per_request_usd': total_cost / max(1, self.total_requests)\n",
    "        }\n",
    "\n",
    "\n",
    "def load_scraped_data() -> List[Dict]:\n",
    "    \"\"\"Load scraped data from final.json\"\"\"\n",
    "    try:\n",
    "        with open('final.json', 'r', encoding='utf-8') as f:\n",
    "            scraped_data = json.load(f)\n",
    "        \n",
    "        formatted_data = []\n",
    "        for item in scraped_data:\n",
    "            if 'content' in item:\n",
    "                formatted_item = item\n",
    "            elif 'text' in item:\n",
    "                formatted_item = {\n",
    "                    \"url\": item.get(\"url\", \"unknown\"),\n",
    "                    \"title\": item.get(\"title\", item.get(\"url\", \"unknown\")),\n",
    "                    \"content\": item[\"text\"]\n",
    "                }\n",
    "            else:\n",
    "                continue\n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        return formatted_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ final.json not found\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading final.json: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    directories = [\n",
    "        'data/chunks/llm_based',\n",
    "        'results/ablations'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Load API key from .env file\n",
    "    api_key = os.getenv('GEMINI_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âŒ GEMINI_API_KEY not found in .env file\")\n",
    "        print(\"   Please add your API key to .env file: GEMINI_API_KEY=your-api-key-here\")\n",
    "        return\n",
    "    \n",
    "    # Create output directories\n",
    "    create_output_directories()\n",
    "    \n",
    "    # Initialize LLM chunker\n",
    "    print(\"ðŸš€ Initializing LLM Chunker with Gemini 2.0 Flash-Lite...\")\n",
    "    llm_chunker = LLMChunker(api_key)\n",
    "\n",
    "    # Load data\n",
    "    documents = load_scraped_data()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âŒ No documents loaded. Please check your final.json file.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ… Loaded {len(documents)} documents for LLM-based chunking\")\n",
    "    \n",
    "    # Define LLM chunking configurations to test\n",
    "    llm_configs = [\n",
    "        # {\n",
    "        #     \"name\": \"LLM_Small_Chunks\",\n",
    "        #     \"max_chunk_tokens\": 256\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"LLM_Medium_Chunks\", \n",
    "            \"max_chunk_tokens\": 512\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\": \"LLM_Large_Chunks\",\n",
    "        #     \"max_chunk_tokens\": 768\n",
    "        # }\n",
    "    ]\n",
    "    \n",
    "    # Process documents with each LLM configuration\n",
    "    llm_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ”„ Processing documents with LLM-based chunking strategies...\")\n",
    "    print(\"=\" * 75)\n",
    "    \n",
    "    for config in llm_configs:\n",
    "        config_name = config[\"name\"]\n",
    "        max_chunk_tokens = config[\"max_chunk_tokens\"]\n",
    "        \n",
    "        print(f\"\\nðŸ“ Processing: {config_name}\")\n",
    "        print(f\"   Max chunk tokens: {max_chunk_tokens}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_chunks = []\n",
    "        total_cost_info = {\n",
    "            'total_input_tokens': 0,\n",
    "            'total_output_tokens': 0,\n",
    "            'total_requests': 0,\n",
    "            'fallback_used_count': 0,\n",
    "            'successful_requests': 0\n",
    "        }\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            print(f\"   Processing document {doc_idx + 1}/{len(documents)}...\")\n",
    "            chunks, cost_info = llm_chunker.process_document(doc, max_chunk_tokens)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Aggregate cost info\n",
    "            total_cost_info['total_input_tokens'] += cost_info.get('input_tokens', 0)\n",
    "            total_cost_info['total_output_tokens'] += cost_info.get('output_tokens', 0)\n",
    "            total_cost_info['total_requests'] += 1\n",
    "            if cost_info.get('fallback_used'):\n",
    "                total_cost_info['fallback_used_count'] += 1\n",
    "            if cost_info.get('success'):\n",
    "                total_cost_info['successful_requests'] += 1\n",
    "            \n",
    "            print(f\"     Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chunks = len(all_chunks)\n",
    "        total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "        avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        # Count different categories and types\n",
    "        topic_categories = {}\n",
    "        chunk_types = {}\n",
    "        llm_generated_count = 0\n",
    "        \n",
    "        for chunk in all_chunks:\n",
    "            llm_info = chunk.get('llm_info', {})\n",
    "            topic_category = llm_info.get('topic_category', 'unknown')\n",
    "            chunk_type = llm_info.get('chunk_type', 'unknown')\n",
    "            \n",
    "            topic_categories[topic_category] = topic_categories.get(topic_category, 0) + 1\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "            \n",
    "            if llm_info.get('llm_generated', False):\n",
    "                llm_generated_count += 1\n",
    "        \n",
    "        # Get cost summary\n",
    "        cost_summary = llm_chunker.get_cost_summary()\n",
    "        \n",
    "        llm_results[config_name] = {\n",
    "            'chunks': all_chunks,\n",
    "            'total_chunks': total_chunks,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "            'processing_time': processing_time,\n",
    "            'config': config,\n",
    "            'topic_categories': topic_categories,\n",
    "            'chunk_types': chunk_types,\n",
    "            'llm_generated_count': llm_generated_count,\n",
    "            'fallback_count': total_chunks - llm_generated_count,\n",
    "            'cost_info': total_cost_info,\n",
    "            'cost_summary': cost_summary\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Generated {total_chunks} chunks ({llm_generated_count} LLM, {total_chunks - llm_generated_count} fallback)\")\n",
    "        print(f\"   â±ï¸  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"   ðŸ“Š Average tokens per chunk: {avg_tokens_per_chunk:.1f}\")\n",
    "        print(f\"   ðŸ’° Estimated cost: ${cost_summary['total_cost_usd']:.4f}\")\n",
    "        print(f\"   ðŸ“‹ Topic categories: {list(topic_categories.keys())}\")\n",
    "        print(f\"   ðŸ·ï¸  Chunk types: {list(chunk_types.keys())}\")\n",
    "        \n",
    "        # Save chunks to file\n",
    "        output_file = f\"data/chunks/llm_based/{config_name}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"   ðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 75)\n",
    "    print(\"âœ… LLM-based chunking completed for all configurations!\")\n",
    "    \n",
    "    # Create summary comparison\n",
    "    print(\"\\nðŸ“ˆ LLM CHUNKING SUMMARY:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    llm_summary = []\n",
    "    for name, result in llm_results.items():\n",
    "        config = result['config']\n",
    "        cost_summary = result['cost_summary']\n",
    "        llm_summary.append({\n",
    "            'Strategy': name,\n",
    "            'Max Tokens': config['max_chunk_tokens'],\n",
    "            'Total Chunks': result['total_chunks'],\n",
    "            'LLM Generated': result['llm_generated_count'],\n",
    "            'Fallback Used': result['fallback_count'],\n",
    "            'Avg Tokens/Chunk': f\"{result['avg_tokens_per_chunk']:.1f}\",\n",
    "            'Processing Time (s)': f\"{result['processing_time']:.2f}\",\n",
    "            'Total Cost ($)': f\"{cost_summary['total_cost_usd']:.4f}\",\n",
    "            'Cost per Chunk ($)': f\"{cost_summary['total_cost_usd']/max(1, result['total_chunks']):.6f}\"\n",
    "        })\n",
    "    \n",
    "    llm_df = pd.DataFrame(llm_summary)\n",
    "    print(llm_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    llm_df.to_csv('results/ablations/llm_chunking_summary.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Summary saved to: results/ablations/llm_chunking_summary.csv\")\n",
    "    \n",
    "    # Show sample chunks with LLM information\n",
    "    print(\"\\nðŸ” SAMPLE CHUNKS WITH LLM INFO:\")\n",
    "    print(\"=\" * 100)\n",
    "    for strategy_name, result in llm_results.items():\n",
    "        if result['chunks']:\n",
    "            sample_chunk = result['chunks'][0]\n",
    "            llm_info = sample_chunk.get('llm_info', {})\n",
    "            print(f\"\\nðŸ“„ {strategy_name}:\")\n",
    "            print(f\"   Tokens: {sample_chunk['token_count']}\")\n",
    "            print(f\"   LLM Generated: {llm_info.get('llm_generated', False)}\")\n",
    "            print(f\"   Topic Category: {llm_info.get('topic_category', 'N/A')}\")\n",
    "            print(f\"   Chunk Type: {llm_info.get('chunk_type', 'N/A')}\")\n",
    "            print(f\"   Heading: {llm_info.get('heading', 'N/A')}\")\n",
    "            print(f\"   Key Concepts: {llm_info.get('key_concepts', [])}\")\n",
    "            print(f\"   Content preview: {sample_chunk['content'][:200]}...\")\n",
    "            print(f\"   Source: {sample_chunk['source_title']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed864893",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3f84b",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00d6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIFIED CHUNKING EVALUATION TEST SUITE\n",
    "# Combines retrieval-focused metrics with domain-specific RAG evaluation\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"results/unified_evaluation\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac16d2f",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11af4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_broken_sentences(text: str) -> int:\n",
    "    \"\"\"Count sentences that are incomplete or broken\"\"\"\n",
    "    sentences = text.split('.')\n",
    "    broken_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and len(sentence) < 10:  # Very short fragments\n",
    "            broken_count += 1\n",
    "        elif sentence and not sentence[0].isupper():  # Doesn't start with capital\n",
    "            broken_count += 1\n",
    "\n",
    "    return broken_count\n",
    "\n",
    "def measure_topic_consistency(text: str) -> float:\n",
    "    \"\"\"Measure if text maintains consistent topic/theme\"\"\"\n",
    "    # JioPay domain keywords\n",
    "    domain_topics = {\n",
    "        'payment': ['payment', 'transaction', 'money', 'amount', 'pay', 'refund'],\n",
    "        'app': ['app', 'application', 'download', 'install', 'mobile', 'android', 'ios'],\n",
    "        'account': ['account', 'login', 'password', 'signin', 'logout', 'profile'],\n",
    "        'business': ['business', 'merchant', 'kyc', 'documents', 'verification'],\n",
    "        'support': ['help', 'support', 'issue', 'problem', 'error', 'troubleshoot'],\n",
    "        'settlement': ['settlement', 'bank', 'transfer', 'utr', 'payout'],\n",
    "        'collect_links': ['collect link', 'payment link', 'link validity', 'bulk collect', 'partial payment'],\n",
    "        'voicebox': ['voicebox', 'voice box', 'audio', 'announcement', 'replay'],\n",
    "        'transactions': ['transaction', 'payment', 'refund', 'failed'],\n",
    "        'repeat_payments': ['repeat', 'recurring', 'subscription', 'mandate'],\n",
    "        'campaigns': ['campaign', 'offer', 'create campaign', 'edit campaign'],\n",
    "        'user_management': ['sub user', 'user management', 'block user'],\n",
    "        'dqr': ['DQR', 'dynamic QR', 'store manager'],\n",
    "        'partner_program': ['partner', 'commission', 'earning'],\n",
    "        'p2pm_merchants': ['P2PM', 'merchant limit', 'upgrade'],\n",
    "        'payment_gateway': ['payment', 'gateway', 'transaction', 'processing', 'checkout'],\n",
    "        'app_usage': ['app', 'download', 'install', 'mobile', 'android', 'ios'],\n",
    "        'business_setup': ['business', 'setup', 'merchant', 'onboarding', 'registration'],\n",
    "        'technical_issues': ['error', 'issue', 'problem', 'troubleshoot', 'fix', 'bug'],\n",
    "        'refunds': ['refund', 'return', 'cancel', 'reverse', 'chargeback'],\n",
    "        'kyc_documents': ['kyc', 'documents', 'verification', 'identity', 'proof'],\n",
    "        'fees_pricing': ['fee', 'charge', 'cost', 'price', 'rate', 'commission'],\n",
    "        'general': []  # fallback category\n",
    "    }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    topic_scores = {}\n",
    "\n",
    "    for topic, keywords in domain_topics.items():\n",
    "        score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        if score > 0:\n",
    "            topic_scores[topic] = score\n",
    "\n",
    "    if not topic_scores:\n",
    "        return 0.5  # Neutral if no topics found\n",
    "\n",
    "    # Calculate consistency (higher if dominated by one topic)\n",
    "    max_score = max(topic_scores.values())\n",
    "    total_score = sum(topic_scores.values())\n",
    "\n",
    "    consistency = max_score / total_score if total_score > 0 else 0\n",
    "    return consistency\n",
    "\n",
    "def measure_thought_completeness(text: str) -> float:\n",
    "    \"\"\"Measure if text contains complete thoughts/ideas\"\"\"\n",
    "    # Check for complete Q&A pairs\n",
    "    has_question = bool(re.search(r'[Qq]:', text) or '?' in text)\n",
    "    has_answer = bool(re.search(r'[Aa]:', text) or len(text.split()) > 15)\n",
    "\n",
    "    if has_question and has_answer:\n",
    "        return 1.0\n",
    "    elif has_question or has_answer:\n",
    "        return 0.7\n",
    "\n",
    "    # Check for complete procedures/instructions\n",
    "    procedure_indicators = ['step', 'follow', 'click', 'enter', 'select', 'go to']\n",
    "    has_procedure = any(indicator in text.lower() for indicator in procedure_indicators)\n",
    "\n",
    "    if has_procedure and len(text.split()) > 20:\n",
    "        return 0.9\n",
    "    elif has_procedure:\n",
    "        return 0.6\n",
    "\n",
    "    # Default based on length and structure\n",
    "    sentences = text.split('.')\n",
    "    complete_sentences = [s for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "    if len(complete_sentences) >= 2:\n",
    "        return 0.8\n",
    "    elif len(complete_sentences) == 1:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389071b0",
   "metadata": {},
   "source": [
    "## Domain Specific Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2de44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jiopay_keywords(text: str) -> int:\n",
    "    \"\"\"Count JioPay-specific keywords and terms\"\"\"\n",
    "    jiopay_keywords = [\n",
    "        'jiopay', 'jio pay', 'jiomoney', 'jio money',\n",
    "        'payment gateway', 'upi', 'wallet', 'digital payment',\n",
    "        'merchant', 'business account', 'kyc', 'settlement',\n",
    "        'transaction', 'refund', 'chargeback', 'dispute',\n",
    "        'collect link', 'payment link', 'qr code', 'dynamic qr',\n",
    "        'voicebox', 'voice box', 'announcement', 'replay',\n",
    "        'api', 'integration', 'webhook', 'callback'\n",
    "    ]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for keyword in jiopay_keywords if keyword in text_lower)\n",
    "\n",
    "def count_procedure_words(text: str) -> int:\n",
    "    \"\"\"Count procedural/instructional words\"\"\"\n",
    "    procedure_words = [\n",
    "        'click', 'tap', 'select', 'choose', 'enter', 'input',\n",
    "        'go to', 'navigate', 'open', 'close', 'save', 'submit',\n",
    "        'step', 'follow', 'complete', 'finish', 'start', 'begin'\n",
    "    ]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for word in procedure_words if word in text_lower)\n",
    "\n",
    "# Mock embedding function\n",
    "def get_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Mock embedding function using TF-IDF\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=384, stop_words='english')\n",
    "    if len(texts) == 0:\n",
    "        return np.array([])\n",
    "    embeddings = vectorizer.fit_transform(texts).toarray()\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a04208",
   "metadata": {},
   "source": [
    "## Test Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ee30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 15 test queries\n",
      "Ground truth mappings: 15\n"
     ]
    }
   ],
   "source": [
    "def create_test_queries() -> List[Dict]:\n",
    "    \"\"\"Create comprehensive test queries for JioPay domain\"\"\"\n",
    "    return [\n",
    "        # Onboarding queries\n",
    "        {\n",
    "            \"id\": \"q1\",\n",
    "            \"query\": \"How do I register for a JioPay business account?\",\n",
    "            \"category\": \"onboarding\",\n",
    "            \"expected_topics\": [\"registration\", \"business\", \"account\", \"setup\"],\n",
    "            \"complexity\": \"simple\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q2\", \n",
    "            \"query\": \"What documents are required for KYC verification during business onboarding?\",\n",
    "            \"category\": \"onboarding\",\n",
    "            \"expected_topics\": [\"kyc\", \"documents\", \"verification\", \"business\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        \n",
    "        # Payment processing queries\n",
    "        {\n",
    "            \"id\": \"q3\",\n",
    "            \"query\": \"How does payment processing work for merchants?\",\n",
    "            \"category\": \"payments\",\n",
    "            \"expected_topics\": [\"payment\", \"processing\", \"merchant\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q4\",\n",
    "            \"query\": \"What are the settlement timelines and how do refunds work?\",\n",
    "            \"category\": \"payments\", \n",
    "            \"expected_topics\": [\"settlement\", \"refund\", \"timeline\"],\n",
    "            \"complexity\": \"complex\"\n",
    "        },\n",
    "        \n",
    "        # KYC and compliance\n",
    "        {\n",
    "            \"id\": \"q5\",\n",
    "            \"query\": \"What identity documents are accepted for KYC?\",\n",
    "            \"category\": \"kyc\",\n",
    "            \"expected_topics\": [\"kyc\", \"identity\", \"documents\"],\n",
    "            \"complexity\": \"simple\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q6\",\n",
    "            \"query\": \"How long does the KYC verification process take?\",\n",
    "            \"category\": \"kyc\",\n",
    "            \"expected_topics\": [\"kyc\", \"verification\", \"timeline\"],\n",
    "            \"complexity\": \"simple\"\n",
    "        },\n",
    "        \n",
    "        # API and integration\n",
    "        {\n",
    "            \"id\": \"q7\",\n",
    "            \"query\": \"How do I integrate JioPay payment gateway API?\",\n",
    "            \"category\": \"api\",\n",
    "            \"expected_topics\": [\"api\", \"integration\", \"gateway\"],\n",
    "            \"complexity\": \"complex\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q8\",\n",
    "            \"query\": \"What are the API endpoints for payment processing?\",\n",
    "            \"category\": \"api\",\n",
    "            \"expected_topics\": [\"api\", \"endpoints\", \"payment\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        \n",
    "        # Security and troubleshooting\n",
    "        {\n",
    "            \"id\": \"q9\",\n",
    "            \"query\": \"What security measures are in place for transactions?\",\n",
    "            \"category\": \"security\",\n",
    "            \"expected_topics\": [\"security\", \"transaction\", \"protection\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q10\",\n",
    "            \"query\": \"How do I resolve failed payment transactions?\",\n",
    "            \"category\": \"troubleshooting\",\n",
    "            \"expected_topics\": [\"troubleshooting\", \"failed\", \"payment\"],\n",
    "            \"complexity\": \"complex\"\n",
    "        },\n",
    "        \n",
    "        # Pricing and fees\n",
    "        {\n",
    "            \"id\": \"q11\", \n",
    "            \"query\": \"What are the transaction fees for different payment methods?\",\n",
    "            \"category\": \"pricing\",\n",
    "            \"expected_topics\": [\"fees\", \"pricing\", \"transaction\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q12\",\n",
    "            \"query\": \"Are there any setup or monthly maintenance charges?\",\n",
    "            \"category\": \"pricing\",\n",
    "            \"expected_topics\": [\"charges\", \"setup\", \"maintenance\"],\n",
    "            \"complexity\": \"simple\"\n",
    "        },\n",
    "        \n",
    "        # JioPay specific features\n",
    "        {\n",
    "            \"id\": \"q13\",\n",
    "            \"query\": \"How do I use collect links for payment collection?\",\n",
    "            \"category\": \"features\",\n",
    "            \"expected_topics\": [\"collect\", \"link\", \"payment\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q14\",\n",
    "            \"query\": \"What is voicebox and how do I configure announcements?\",\n",
    "            \"category\": \"features\",\n",
    "            \"expected_topics\": [\"voicebox\", \"announcement\", \"audio\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q15\",\n",
    "            \"query\": \"How do I create and manage dynamic QR codes?\",\n",
    "            \"category\": \"features\",\n",
    "            \"expected_topics\": [\"qr\", \"dynamic\", \"code\"],\n",
    "            \"complexity\": \"medium\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def create_ground_truth(test_queries) -> Dict:\n",
    "    \"\"\"Create ground truth relevance mappings\"\"\"\n",
    "    return {\n",
    "        query[\"id\"]: query[\"expected_topics\"] \n",
    "        for query in test_queries\n",
    "    }\n",
    "\n",
    "# Initialize test data\n",
    "test_queries = create_test_queries()\n",
    "ground_truth = create_ground_truth(test_queries)\n",
    "\n",
    "print(f\"Initialized {len(test_queries)} test queries\")\n",
    "print(f\"Ground truth mappings: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99440cde",
   "metadata": {},
   "source": [
    "## Core Evaluator Class - Part 1 (Initialization & Data Loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ff1c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Chunking Evaluator initialized\n",
      "Test queries: 15\n",
      "Ground truth mappings: 15\n"
     ]
    }
   ],
   "source": [
    "class UnifiedChunkingEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation framework combining:\n",
    "    1. Retrieval-focused metrics (precision, recall, latency)\n",
    "    2. Domain-specific RAG metrics (semantic coherence, content quality)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_queries=None, ground_truth=None):\n",
    "        \"\"\"Initialize the unified evaluator\"\"\"\n",
    "        self.test_queries = test_queries or create_test_queries()\n",
    "        self.ground_truth = ground_truth or create_ground_truth(self.test_queries)\n",
    "        self.evaluation_results = {}\n",
    "        \n",
    "        print(\"Unified Chunking Evaluator initialized\")\n",
    "        print(f\"Test queries: {len(self.test_queries)}\")\n",
    "        print(f\"Ground truth mappings: {len(self.ground_truth)}\")\n",
    "    \n",
    "    def load_chunked_data(self, strategy_name: str, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load chunked data from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different data formats\n",
    "            if isinstance(data, dict) and 'chunks' in data:\n",
    "                chunks = data['chunks']\n",
    "            elif isinstance(data, list):\n",
    "                chunks = data\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected data format in {file_path}\")\n",
    "                return []\n",
    "            \n",
    "            # Normalize chunk format\n",
    "            normalized_chunks = []\n",
    "            for chunk in chunks:\n",
    "                if isinstance(chunk, dict):\n",
    "                    # Handle different chunk formats\n",
    "                    content = chunk.get('content', chunk.get('text', ''))\n",
    "                    if content:\n",
    "                        normalized_chunk = {\n",
    "                            'content': content,\n",
    "                            'text': content,  # For compatibility\n",
    "                            'token_count': chunk.get('token_count', len(content.split())),\n",
    "                            'char_count': chunk.get('char_count', len(content)),\n",
    "                            'type': chunk.get('type', 'unknown'),\n",
    "                            'source': chunk.get('source', 'unknown'),\n",
    "                            'chunk_id': chunk.get('chunk_id', chunk.get('id', len(normalized_chunks)))\n",
    "                        }\n",
    "                        normalized_chunks.append(normalized_chunk)\n",
    "                        \n",
    "            print(f\"Loaded {len(normalized_chunks)} chunks for {strategy_name}\")\n",
    "            return normalized_chunks\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = UnifiedChunkingEvaluator(test_queries, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97006c29",
   "metadata": {},
   "source": [
    "## Core Evaluator Class - Part 2 (RAG Quality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ebc009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_semantic_coherence(self, chunks: List[Dict]) -> float:\n",
    "    \"\"\"Measure how well chunks preserve semantic meaning\"\"\"\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "\n",
    "    coherence_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get('content', chunk.get('text', ''))\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Check for broken sentences\n",
    "        broken_sentence_penalty = count_broken_sentences(text) * 0.1\n",
    "\n",
    "        # Check for topic consistency\n",
    "        topic_consistency = measure_topic_consistency(text)\n",
    "\n",
    "        # Check for complete thoughts\n",
    "        completeness_score = measure_thought_completeness(text)\n",
    "\n",
    "        chunk_score = (topic_consistency + completeness_score) / 2 - broken_sentence_penalty\n",
    "        coherence_scores.append(max(0, min(1, chunk_score)))\n",
    "\n",
    "    return np.mean(coherence_scores) if coherence_scores else 0.0\n",
    "\n",
    "def evaluate_context_completeness(self, chunks: List[Dict]) -> float:\n",
    "    \"\"\"Measure if chunks contain complete contextual information\"\"\"\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "\n",
    "    context_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get('content', chunk.get('text', ''))\n",
    "        chunk_type = chunk.get('type', 'unknown')\n",
    "\n",
    "        if 'faq' in chunk_type.lower():\n",
    "            # FAQ should contain complete Q&A pairs\n",
    "            has_question = 'Q:' in text or '?' in text\n",
    "            has_answer = 'A:' in text or len(text.split()) > 10\n",
    "            context_scores.append(1.0 if (has_question and has_answer) else 0.5)\n",
    "\n",
    "        elif 'pdf' in chunk_type.lower():\n",
    "            # Policy docs should have complete sections\n",
    "            word_count = len(text.split())\n",
    "            if word_count < 30:  # Too fragmented\n",
    "                context_scores.append(0.3)\n",
    "            elif word_count > 200:  # Good context\n",
    "                context_scores.append(1.0)\n",
    "            else:\n",
    "                context_scores.append(0.7)\n",
    "\n",
    "        else:  # Web content\n",
    "            word_count = len(text.split())\n",
    "            if word_count < 20:\n",
    "                context_scores.append(0.4)\n",
    "            elif word_count > 100:\n",
    "                context_scores.append(0.9)\n",
    "            else:\n",
    "                context_scores.append(0.7)\n",
    "\n",
    "    return np.mean(context_scores) if context_scores else 0.0\n",
    "\n",
    "def evaluate_information_density(self, chunks: List[Dict]) -> float:\n",
    "    \"\"\"Measure information value per token\"\"\"\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "\n",
    "    density_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get('content', chunk.get('text', ''))\n",
    "        tokens = chunk.get('token_count', len(text.split()))\n",
    "\n",
    "        # Count informative elements\n",
    "        keywords = count_jiopay_keywords(text)\n",
    "        numbers = len(re.findall(r'\\d+', text))\n",
    "        procedures = count_procedure_words(text)\n",
    "\n",
    "        info_elements = keywords + numbers + procedures\n",
    "        density = info_elements / max(tokens, 1)\n",
    "\n",
    "        # Normalize to 0-1 scale\n",
    "        density_scores.append(min(1.0, density * 10))\n",
    "\n",
    "    return np.mean(density_scores) if density_scores else 0.0\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.evaluate_semantic_coherence = evaluate_semantic_coherence\n",
    "UnifiedChunkingEvaluator.evaluate_context_completeness = evaluate_context_completeness\n",
    "UnifiedChunkingEvaluator.evaluate_information_density = evaluate_information_density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0e23a",
   "metadata": {},
   "source": [
    "## Core Evaluator Class - Part 3 (Domain Coverage Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0fc3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_coverage(self, chunks: List[Dict]) -> float:\n",
    "    \"\"\"Evaluate how well chunks cover different JioPay topics\"\"\"\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "\n",
    "    # JioPay topic categories\n",
    "    topics = {\n",
    "        'payments': ['payment', 'transaction', 'money', 'pay', 'refund'],\n",
    "        'app_usage': ['app', 'download', 'login', 'mobile'],\n",
    "        'business': ['merchant', 'business', 'kyc', 'verification'],\n",
    "        'support': ['help', 'support', 'issue', 'problem'],\n",
    "        'features': ['collect link', 'qr code', 'voicebox', 'settlement']\n",
    "    }\n",
    "\n",
    "    topic_coverage = {topic: 0 for topic in topics}\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get('content', chunk.get('text', '')).lower()\n",
    "        for topic, keywords in topics.items():\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                topic_coverage[topic] += 1\n",
    "\n",
    "    # Calculate coverage score\n",
    "    total_chunks = len(chunks)\n",
    "    coverage_ratios = [min(1.0, count / max(total_chunks * 0.1, 1)) for count in topic_coverage.values()]\n",
    "\n",
    "    return np.mean(coverage_ratios)\n",
    "\n",
    "def evaluate_faq_grouping(self, chunks: List[Dict]) -> float:\n",
    "    \"\"\"Evaluate FAQ grouping quality for JioPay domain\"\"\"\n",
    "    faq_chunks = [c for c in chunks if 'faq' in c.get('type', '').lower()]\n",
    "    if not faq_chunks:\n",
    "        return 1.0  # No FAQs to evaluate\n",
    "\n",
    "    grouping_scores = []\n",
    "\n",
    "    for chunk in faq_chunks:\n",
    "        text = chunk.get('content', chunk.get('text', '')).lower()\n",
    "\n",
    "        # Check for related topic grouping\n",
    "        payment_terms = ['payment', 'transaction', 'refund', 'settlement', 'money']\n",
    "        app_terms = ['app', 'login', 'password', 'download', 'mobile', 'install']\n",
    "        business_terms = ['merchant', 'business', 'kyc', 'documents', 'verification']\n",
    "        support_terms = ['help', 'support', 'issue', 'problem', 'error']\n",
    "        feature_terms = ['collect link', 'qr code', 'voicebox', 'voice box']\n",
    "\n",
    "        topic_groups = [payment_terms, app_terms, business_terms, support_terms, feature_terms]\n",
    "\n",
    "        max_topic_score = 0\n",
    "        for topic_group in topic_groups:\n",
    "            topic_score = sum(1 for term in topic_group if term in text)\n",
    "            max_topic_score = max(max_topic_score, topic_score)\n",
    "\n",
    "        # Score based on topic coherence\n",
    "        if max_topic_score >= 3:\n",
    "            grouping_scores.append(1.0)  # Excellent grouping\n",
    "        elif max_topic_score >= 2:\n",
    "            grouping_scores.append(0.8)  # Good grouping\n",
    "        elif max_topic_score == 1:\n",
    "            grouping_scores.append(0.6)  # Partial grouping\n",
    "        else:\n",
    "            grouping_scores.append(0.3)  # Poor grouping\n",
    "\n",
    "    return np.mean(grouping_scores) if grouping_scores else 1.0\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.evaluate_topic_coverage = evaluate_topic_coverage\n",
    "UnifiedChunkingEvaluator.evaluate_faq_grouping = evaluate_faq_grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefcbf1",
   "metadata": {},
   "source": [
    "## Core Evaluator Class - Part 4 (Retrieval Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d45f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_retrieval(self, query: str, chunks: List[Dict], top_k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "    \"\"\"Simulate retrieval using TF-IDF similarity\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Extract content from chunks\n",
    "        chunk_contents = [chunk.get('content', chunk.get('text', '')) for chunk in chunks]\n",
    "        all_texts = [query] + chunk_contents\n",
    "\n",
    "        # Calculate TF-IDF vectors\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        # Calculate similarities\n",
    "        query_vector = tfidf_matrix[0]\n",
    "        chunk_vectors = tfidf_matrix[1:]\n",
    "        similarities = cosine_similarity(query_vector, chunk_vectors).flatten()\n",
    "\n",
    "        # Get top-k chunks with scores\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        results = [(chunks[i], similarities[i]) for i in top_indices if similarities[i] > 0]\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval simulation: {e}\")\n",
    "        return []\n",
    "\n",
    "def evaluate_retrieval_metrics(self, query_info: Dict, chunks: List[Dict], top_k: int = 5) -> Dict:\n",
    "    \"\"\"Evaluate retrieval metrics (P@1, Recall@k, MRR)\"\"\"\n",
    "    query = query_info['query']\n",
    "    expected_topics = set(query_info['expected_topics'])\n",
    "\n",
    "    # Simulate retrieval\n",
    "    retrieved = self.simulate_retrieval(query, chunks, top_k)\n",
    "\n",
    "    if not retrieved:\n",
    "        return {\n",
    "            'precision_at_1': 0.0,\n",
    "            'precision_at_k': 0.0,\n",
    "            'recall_at_k': 0.0,\n",
    "            'mrr': 0.0,\n",
    "            'retrieved_count': 0\n",
    "        }\n",
    "\n",
    "    # Calculate relevance for each retrieved chunk\n",
    "    relevance_scores = []\n",
    "    for chunk, score in retrieved:\n",
    "        content = chunk.get('content', chunk.get('text', '')).lower()\n",
    "\n",
    "        # Simple relevance calculation based on topic keyword presence\n",
    "        matches = sum(1 for topic in expected_topics if topic in content)\n",
    "        relevance = matches / len(expected_topics) if expected_topics else 0\n",
    "        relevance_scores.append(relevance > 0.3)  # Threshold for relevance\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision_at_1 = relevance_scores[0] if relevance_scores else 0.0\n",
    "    precision_at_k = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0\n",
    "\n",
    "    # For recall, assume perfect system would retrieve all relevant chunks\n",
    "    recall_at_k = min(1.0, sum(relevance_scores) / max(1, len(expected_topics)))\n",
    "\n",
    "    # Mean Reciprocal Rank\n",
    "    mrr = 0.0\n",
    "    for i, relevant in enumerate(relevance_scores):\n",
    "        if relevant:\n",
    "            mrr = 1.0 / (i + 1)\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'precision_at_1': precision_at_1,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'mrr': mrr,\n",
    "        'retrieved_count': len(retrieved)\n",
    "    }\n",
    "\n",
    "def benchmark_performance(self, chunks: List[Dict], num_queries: int = 50) -> Dict:\n",
    "    \"\"\"Benchmark retrieval performance (latency, throughput)\"\"\"\n",
    "    if not chunks:\n",
    "        return {'error': 'No chunks provided'}\n",
    "\n",
    "    # Generate sample queries\n",
    "    sample_queries = [q['query'] for q in self.test_queries[:min(num_queries, len(self.test_queries))]]\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    for i, query in enumerate(sample_queries):\n",
    "        start_time = time.time()\n",
    "        self.simulate_retrieval(query, chunks, top_k=5)\n",
    "        latency = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        latencies.append(latency)\n",
    "\n",
    "    return {\n",
    "        'mean_latency_ms': np.mean(latencies),\n",
    "        'median_latency_ms': np.median(latencies),\n",
    "        'p95_latency_ms': np.percentile(latencies, 95),\n",
    "        'p99_latency_ms': np.percentile(latencies, 99),\n",
    "        'queries_per_second': 1000 / np.mean(latencies) if latencies else 0\n",
    "    }\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.simulate_retrieval = simulate_retrieval\n",
    "UnifiedChunkingEvaluator.evaluate_retrieval_metrics = evaluate_retrieval_metrics\n",
    "UnifiedChunkingEvaluator.benchmark_performance = benchmark_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e69b6c",
   "metadata": {},
   "source": [
    "## Core Evaluator Class - Part 5 (Main Evaluation Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7671e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(self, strategy_name: str, chunks: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation combining both approaches\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {strategy_name}...\")\n",
    "    \n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks to evaluate\"}\n",
    "    \n",
    "    results = {\n",
    "        'strategy_name': strategy_name,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Basic statistics\n",
    "    token_counts = [chunk.get('token_count', len(chunk.get('content', chunk.get('text', '')).split())) for chunk in chunks]\n",
    "    char_counts = [chunk.get('char_count', len(chunk.get('content', chunk.get('text', '')))) for chunk in chunks]\n",
    "    \n",
    "    results['basic_stats'] = {\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_tokens': np.mean(token_counts) if token_counts else 0,\n",
    "        'median_tokens': np.median(token_counts) if token_counts else 0,\n",
    "        'std_tokens': np.std(token_counts) if token_counts else 0,\n",
    "        'min_tokens': np.min(token_counts) if token_counts else 0,\n",
    "        'max_tokens': np.max(token_counts) if token_counts else 0,\n",
    "        'total_tokens': sum(token_counts),\n",
    "        'total_chars': sum(char_counts)\n",
    "    }\n",
    "    \n",
    "    # 1. DOMAIN-SPECIFIC RAG METRICS (40% weight)\n",
    "    print(\"  Evaluating domain-specific RAG metrics...\")\n",
    "    rag_metrics = {\n",
    "        'semantic_coherence': self.evaluate_semantic_coherence(chunks),\n",
    "        'context_completeness': self.evaluate_context_completeness(chunks),\n",
    "        'information_density': self.evaluate_information_density(chunks),\n",
    "        'topic_coverage': self.evaluate_topic_coverage(chunks),\n",
    "        'faq_grouping': self.evaluate_faq_grouping(chunks)\n",
    "    }\n",
    "    \n",
    "    results['rag_quality'] = np.mean([\n",
    "        rag_metrics['semantic_coherence'],\n",
    "        rag_metrics['context_completeness'],\n",
    "        rag_metrics['information_density'],\n",
    "        rag_metrics['topic_coverage']\n",
    "    ])\n",
    "    \n",
    "    # 2. RETRIEVAL PERFORMANCE METRICS (35% weight)\n",
    "    print(\"  Evaluating retrieval performance...\")\n",
    "    retrieval_results = []\n",
    "    for query_info in self.test_queries:\n",
    "        metrics = self.evaluate_retrieval_metrics(query_info, chunks)\n",
    "        metrics['query_id'] = query_info['id']\n",
    "        metrics['category'] = query_info['category']\n",
    "        retrieval_results.append(metrics)\n",
    "    \n",
    "    results['retrieval_metrics'] = retrieval_results\n",
    "    \n",
    "    # Aggregate retrieval metrics\n",
    "    avg_precision_at_1 = np.mean([m['precision_at_1'] for m in retrieval_results])\n",
    "    avg_precision_at_k = np.mean([m['precision_at_k'] for m in retrieval_results])\n",
    "    avg_recall_at_k = np.mean([m['recall_at_k'] for m in retrieval_results])\n",
    "    avg_mrr = np.mean([m['mrr'] for m in retrieval_results])\n",
    "    \n",
    "    results['retrieval_performance'] = (avg_precision_at_1 + avg_precision_at_k + avg_recall_at_k + avg_mrr) / 4\n",
    "    \n",
    "    # 3. SIZE OPTIMIZATION METRICS (15% weight)\n",
    "    print(\"  Evaluating size optimization...\")\n",
    "    excellent_range = (80, 400)\n",
    "    good_range = (50, 600)\n",
    "    size_scores = []\n",
    "    for chunk in chunks:\n",
    "        tokens = chunk.get('token_count', len(chunk.get('content', chunk.get('text', '')).split()))\n",
    "        if excellent_range[0] <= tokens <= excellent_range[1]:\n",
    "            score = 1.0\n",
    "        elif good_range[0] <= tokens <= good_range[1]:\n",
    "            score = 0.8\n",
    "        elif tokens < good_range[0]:\n",
    "            score = max(0.6, tokens / good_range[0])\n",
    "        else:\n",
    "            score = max(0.4, good_range[1] / tokens)\n",
    "        size_scores.append(score)\n",
    "    \n",
    "    results['size_optimization'] = np.mean(size_scores) if size_scores else 0.0\n",
    "    \n",
    "    # 4. PERFORMANCE METRICS (10% weight)\n",
    "    print(\"  Benchmarking performance...\")\n",
    "    performance = self.benchmark_performance(chunks)\n",
    "    results['performance_benchmark'] = performance\n",
    "    \n",
    "    # Normalize performance score (higher QPS is better, lower latency is better)\n",
    "    qps = performance.get('queries_per_second', 0)\n",
    "    latency = performance.get('mean_latency_ms', 1000)\n",
    "    results['performance_score'] = min(1.0, qps / 100) * (1000 / max(latency, 1)) * 0.001\n",
    "    \n",
    "    # CALCULATE WEIGHTED FINAL SCORE\n",
    "    results['final_score'] = (\n",
    "        results['rag_quality'] * 0.45 +\n",
    "        results['retrieval_performance'] * 0.40 +\n",
    "        results['size_optimization'] * 0.08 +\n",
    "        results['performance_score'] * 0.07\n",
    "    )\n",
    "    \n",
    "    # Store detailed metrics\n",
    "    results['detailed_metrics'] = {\n",
    "        **rag_metrics,\n",
    "        'avg_precision_at_1': avg_precision_at_1,\n",
    "        'avg_precision_at_k': avg_precision_at_k,\n",
    "        'avg_recall_at_k': avg_recall_at_k,\n",
    "        'avg_mrr': avg_mrr\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_strategies(self, strategies_data: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"Compare multiple chunking strategies using unified evaluation\"\"\"\n",
    "    print(\"\\nRunning Unified Chunking Strategy Comparison...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for strategy_name, chunks in strategies_data.items():\n",
    "        print(f\"\\nProcessing {strategy_name}...\")\n",
    "        results = self.evaluate_strategy(strategy_name, chunks)\n",
    "        \n",
    "        if 'error' in results:\n",
    "            print(f\"Skipping {strategy_name} due to error: {results['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Store detailed results\n",
    "        self.evaluation_results[strategy_name] = results\n",
    "        \n",
    "        # Extract key metrics for comparison table\n",
    "        basic_stats = results.get('basic_stats', {})\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        performance = results.get('performance_benchmark', {})\n",
    "        \n",
    "        row = {\n",
    "            'Strategy': strategy_name,\n",
    "            'Final_Score': f\"{results.get('final_score', 0):.3f}\",\n",
    "            'RAG_Quality': f\"{results.get('rag_quality', 0):.3f}\",\n",
    "            'Retrieval_Perf': f\"{results.get('retrieval_performance', 0):.3f}\",\n",
    "            'Size_Opt': f\"{results.get('size_optimization', 0):.3f}\",\n",
    "            'Performance': f\"{results.get('performance_score', 0):.3f}\",\n",
    "            'Total_Chunks': basic_stats.get('total_chunks', 0),\n",
    "            'Avg_Tokens': f\"{basic_stats.get('avg_tokens', 0):.1f}\",\n",
    "            'Semantic_Coh': f\"{detailed.get('semantic_coherence', 0):.3f}\",\n",
    "            'Context_Comp': f\"{detailed.get('context_completeness', 0):.3f}\",\n",
    "            'Info_Density': f\"{detailed.get('information_density', 0):.3f}\",\n",
    "            'Topic_Cov': f\"{detailed.get('topic_coverage', 0):.3f}\",\n",
    "            'Precision@1': f\"{detailed.get('avg_precision_at_1', 0):.3f}\",\n",
    "            'Recall@5': f\"{detailed.get('avg_recall_at_k', 0):.3f}\",\n",
    "            'MRR': f\"{detailed.get('avg_mrr', 0):.3f}\",\n",
    "            'Latency_ms': f\"{performance.get('mean_latency_ms', 0):.2f}\",\n",
    "            'QPS': f\"{performance.get('queries_per_second', 0):.1f}\"\n",
    "        }\n",
    "        \n",
    "        comparison_results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "def analyze_chunk_sizes(self, strategies_data: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"Analyze chunk size statistics for all strategies\"\"\"\n",
    "    size_analysis = []\n",
    "    \n",
    "    for strategy_name, chunks in strategies_data.items():\n",
    "        if not chunks:\n",
    "            continue\n",
    "            \n",
    "        token_counts = [chunk.get('token_count', len(chunk.get('content', chunk.get('text', '')).split())) for chunk in chunks]\n",
    "        \n",
    "        size_stats = {\n",
    "            'Strategy': strategy_name,\n",
    "            'Total_Chunks': len(chunks),\n",
    "            'Min_Tokens': np.min(token_counts),\n",
    "            'Max_Tokens': np.max(token_counts),\n",
    "            'Avg_Tokens': np.mean(token_counts),\n",
    "            'Median_Tokens': np.median(token_counts),\n",
    "            'Std_Tokens': np.std(token_counts)\n",
    "        }\n",
    "        size_analysis.append(size_stats)\n",
    "    \n",
    "    return pd.DataFrame(size_analysis)\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.evaluate_strategy = evaluate_strategy\n",
    "UnifiedChunkingEvaluator.compare_strategies = compare_strategies\n",
    "UnifiedChunkingEvaluator.analyze_chunk_sizes = analyze_chunk_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b725b6",
   "metadata": {},
   "source": [
    "## Visualization Functions - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df56307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unified_plots(self, output_dir: str):\n",
    "    \"\"\"Generate unified performance plots\"\"\"\n",
    "    strategies = list(self.evaluation_results.keys())\n",
    "    \n",
    "    # Create comprehensive metrics dashboard\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('Unified Chunking Strategy Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract all metrics\n",
    "    metrics_data = {}\n",
    "    for strategy in strategies:\n",
    "        results = self.evaluation_results[strategy]\n",
    "        basic_stats = results.get('basic_stats', {})\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        performance = results.get('performance_benchmark', {})\n",
    "        \n",
    "        metrics_data[strategy] = {\n",
    "            'Final Score': results.get('final_score', 0),\n",
    "            'RAG Quality': results.get('rag_quality', 0),\n",
    "            'Retrieval Perf': results.get('retrieval_performance', 0),\n",
    "            'Semantic Coherence': detailed.get('semantic_coherence', 0),\n",
    "            'Context Completeness': detailed.get('context_completeness', 0),\n",
    "            'Information Density': detailed.get('information_density', 0),\n",
    "            'Topic Coverage': detailed.get('topic_coverage', 0),\n",
    "            'Precision@1': detailed.get('avg_precision_at_1', 0),\n",
    "            'Size Optimization': results.get('size_optimization', 0)\n",
    "        }\n",
    "    \n",
    "    # Plot each metric\n",
    "    plot_configs = [\n",
    "        ('Final Score', (0, 0)),\n",
    "        ('RAG Quality', (0, 1)),\n",
    "        ('Retrieval Perf', (0, 2)),\n",
    "        ('Semantic Coherence', (1, 0)),\n",
    "        ('Context Completeness', (1, 1)),\n",
    "        ('Information Density', (1, 2)),\n",
    "        ('Topic Coverage', (2, 0)),\n",
    "        ('Precision@1', (2, 1)),\n",
    "        ('Size Optimization', (2, 2))\n",
    "    ]\n",
    "    \n",
    "    for metric_name, (row, col) in plot_configs:\n",
    "        ax = axes[row, col]\n",
    "        values = [metrics_data[strategy][metric_name] for strategy in strategies]\n",
    "        \n",
    "        bars = ax.bar(strategies, values, color=plt.cm.viridis(np.linspace(0, 1, len(strategies))))\n",
    "        ax.set_title(metric_name, fontweight='bold')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/unified_performance_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create radar chart comparison\n",
    "    self.create_radar_chart(output_dir, metrics_data)\n",
    "\n",
    "def create_radar_chart(self, output_dir: str, metrics_data: Dict):\n",
    "    \"\"\"Create radar chart for strategy comparison\"\"\"\n",
    "    # Select key metrics for radar chart\n",
    "    radar_metrics = ['RAG Quality', 'Retrieval Perf', 'Semantic Coherence', \n",
    "                    'Context Completeness', 'Information Density', 'Size Optimization']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(metrics_data)))\n",
    "    \n",
    "    for i, (strategy, data) in enumerate(metrics_data.items()):\n",
    "        values = [data[metric] for metric in radar_metrics]\n",
    "        values += [values[0]]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=strategy, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(radar_metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Strategy Comparison Radar Chart', fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/strategy_radar_chart.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.generate_unified_plots = generate_unified_plots\n",
    "UnifiedChunkingEvaluator.create_radar_chart = create_radar_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591aec8d",
   "metadata": {},
   "source": [
    "## Visualization Functions - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57aebfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain_analysis(self, output_dir: str):\n",
    "    \"\"\"Generate domain-specific analysis\"\"\"\n",
    "    # Analyze domain coverage by strategy\n",
    "    domain_analysis = defaultdict(dict)\n",
    "    \n",
    "    for strategy_name, results in self.evaluation_results.items():\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        domain_analysis[strategy_name] = {\n",
    "            'Semantic Coherence': detailed.get('semantic_coherence', 0),\n",
    "            'Context Completeness': detailed.get('context_completeness', 0),\n",
    "            'Information Density': detailed.get('information_density', 0),\n",
    "            'Topic Coverage': detailed.get('topic_coverage', 0),\n",
    "            'FAQ Grouping': detailed.get('faq_grouping', 0)\n",
    "        }\n",
    "    \n",
    "    # Create domain analysis DataFrame\n",
    "    domain_df = pd.DataFrame(domain_analysis).T\n",
    "    domain_df.to_csv(f\"{output_dir}/domain_specific_analysis.csv\")\n",
    "    \n",
    "    # Create domain performance heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(domain_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "               cbar_kws={'label': 'Score'})\n",
    "    plt.title('Domain-Specific Performance Analysis', fontweight='bold')\n",
    "    plt.ylabel('Strategy')\n",
    "    plt.xlabel('Domain Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/domain_performance_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_retrieval_analysis(self, output_dir: str):\n",
    "    \"\"\"Generate retrieval performance analysis\"\"\"\n",
    "    # Analyze retrieval metrics by category\n",
    "    category_analysis = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for strategy_name, results in self.evaluation_results.items():\n",
    "        retrieval_metrics = results.get('retrieval_metrics', [])\n",
    "        for metric in retrieval_metrics:\n",
    "            category = metric['category']\n",
    "            category_analysis[category]['precision_at_1'].append(metric['precision_at_1'])\n",
    "            category_analysis[category]['recall_at_k'].append(metric['recall_at_k'])\n",
    "            category_analysis[category]['mrr'].append(metric['mrr'])\n",
    "    \n",
    "    # Create category performance summary\n",
    "    category_summary = []\n",
    "    for category, metrics in category_analysis.items():\n",
    "        category_summary.append({\n",
    "            'Category': category,\n",
    "            'Avg_Precision@1': np.mean(metrics['precision_at_1']),\n",
    "            'Avg_Recall@5': np.mean(metrics['recall_at_k']),\n",
    "            'Avg_MRR': np.mean(metrics['mrr']),\n",
    "            'Std_Precision@1': np.std(metrics['precision_at_1']),\n",
    "            'Query_Count': len(metrics['precision_at_1'])\n",
    "        })\n",
    "    \n",
    "    category_df = pd.DataFrame(category_summary)\n",
    "    category_df.to_csv(f\"{output_dir}/retrieval_by_category.csv\", index=False)\n",
    "    \n",
    "    # Create category performance plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    categories = category_df['Category']\n",
    "    metrics_to_plot = ['Avg_Precision@1', 'Avg_Recall@5', 'Avg_MRR']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        axes[i].bar(categories, category_df[metric])\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].set_ylabel('Score')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Retrieval Performance by Query Category', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/retrieval_by_category.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.generate_domain_analysis = generate_domain_analysis\n",
    "UnifiedChunkingEvaluator.generate_retrieval_analysis = generate_retrieval_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ade0bc",
   "metadata": {},
   "source": [
    "## Report Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8bc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unified_summary_report(self, output_dir: str):\n",
    "    \"\"\"Generate comprehensive markdown summary report\"\"\"\n",
    "    report_lines = []\n",
    "    report_lines.append(\"# UNIFIED CHUNKING EVALUATION REPORT\")\n",
    "    report_lines.append(\"=\" * 60)\n",
    "    report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report_lines.append(f\"Strategies Evaluated: {len(self.evaluation_results)}\")\n",
    "    report_lines.append(f\"Test Queries: {len(self.test_queries)}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report_lines.append(\"## EXECUTIVE SUMMARY\")\n",
    "    report_lines.append(\"-\" * 30)\n",
    "    \n",
    "    # Find best strategy\n",
    "    best_strategy = max(self.evaluation_results.items(), \n",
    "                      key=lambda x: x[1].get('final_score', 0))\n",
    "    \n",
    "    if best_strategy:\n",
    "        name, results = best_strategy\n",
    "        report_lines.append(f\"**Best Overall Strategy:** {name}\")\n",
    "        report_lines.append(f\"- Final Score: {results.get('final_score', 0):.3f}\")\n",
    "        report_lines.append(f\"- RAG Quality: {results.get('rag_quality', 0):.3f}\")\n",
    "        report_lines.append(f\"- Retrieval Performance: {results.get('retrieval_performance', 0):.3f}\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    # Evaluation Methodology\n",
    "    report_lines.append(\"## EVALUATION METHODOLOGY\")\n",
    "    report_lines.append(\"-\" * 30)\n",
    "    report_lines.append(\"This evaluation combines two complementary approaches:\")\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"### Weighting Scheme:\")\n",
    "    report_lines.append(\"- **RAG Quality (40%)**: Domain-specific content quality metrics\")\n",
    "    report_lines.append(\"  - Semantic coherence, context completeness, information density, topic coverage\")\n",
    "    report_lines.append(\"- **Retrieval Performance (35%)**: Actual retrieval effectiveness\")\n",
    "    report_lines.append(\"  - Precision@1, Precision@K, Recall@K, Mean Reciprocal Rank\")\n",
    "    report_lines.append(\"- **Size Optimization (15%)**: Chunk size appropriateness\")\n",
    "    report_lines.append(\"  - Optimal size distribution for embeddings (150-600 tokens)\")\n",
    "    report_lines.append(\"- **Performance (10%)**: Processing efficiency\")\n",
    "    report_lines.append(\"  - Query latency and throughput\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Strategy Rankings\n",
    "    report_lines.append(\"## STRATEGY RANKINGS\")\n",
    "    report_lines.append(\"-\" * 30)\n",
    "    \n",
    "    rankings = sorted(self.evaluation_results.items(), \n",
    "                     key=lambda x: x[1].get('final_score', 0), reverse=True)\n",
    "    \n",
    "    for i, (strategy, results) in enumerate(rankings, 1):\n",
    "        score = results.get('final_score', 0)\n",
    "        report_lines.append(f\"{i}. **{strategy}**: {score:.3f}\")\n",
    "    \n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Detailed Analysis\n",
    "    report_lines.append(\"## DETAILED ANALYSIS\")\n",
    "    report_lines.append(\"-\" * 30)\n",
    "    \n",
    "    for strategy_name, results in rankings:\n",
    "        report_lines.append(f\"### {strategy_name}\")\n",
    "        \n",
    "        basic_stats = results.get('basic_stats', {})\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        performance = results.get('performance_benchmark', {})\n",
    "        \n",
    "        report_lines.append(\"**Core Metrics:**\")\n",
    "        report_lines.append(f\"- Final Score: {results.get('final_score', 0):.3f}\")\n",
    "        report_lines.append(f\"- RAG Quality: {results.get('rag_quality', 0):.3f}\")\n",
    "        report_lines.append(f\"- Retrieval Performance: {results.get('retrieval_performance', 0):.3f}\")\n",
    "        report_lines.append(f\"- Size Optimization: {results.get('size_optimization', 0):.3f}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"**Chunk Statistics:**\")\n",
    "        report_lines.append(f\"- Total Chunks: {basic_stats.get('total_chunks', 0)}\")\n",
    "        report_lines.append(f\"- Avg Tokens: {basic_stats.get('avg_tokens', 0):.1f}\")\n",
    "        report_lines.append(f\"- Token Std Dev: {basic_stats.get('std_tokens', 0):.1f}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"**Domain-Specific Metrics:**\")\n",
    "        report_lines.append(f\"- Semantic Coherence: {detailed.get('semantic_coherence', 0):.3f}\")\n",
    "        report_lines.append(f\"- Context Completeness: {detailed.get('context_completeness', 0):.3f}\")\n",
    "        report_lines.append(f\"- Information Density: {detailed.get('information_density', 0):.3f}\")\n",
    "        report_lines.append(f\"- Topic Coverage: {detailed.get('topic_coverage', 0):.3f}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"**Retrieval Metrics:**\")\n",
    "        report_lines.append(f\"- Precision@1: {detailed.get('avg_precision_at_1', 0):.3f}\")\n",
    "        report_lines.append(f\"- Recall@5: {detailed.get('avg_recall_at_k', 0):.3f}\")\n",
    "        report_lines.append(f\"- MRR: {detailed.get('avg_mrr', 0):.3f}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"**Performance:**\")\n",
    "        report_lines.append(f\"- Avg Latency: {performance.get('mean_latency_ms', 0):.2f} ms\")\n",
    "        report_lines.append(f\"- Queries/Second: {performance.get('queries_per_second', 0):.1f}\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report_lines.append(\"## RECOMMENDATIONS\")\n",
    "    report_lines.append(\"-\" * 30)\n",
    "    \n",
    "    if best_strategy:\n",
    "        name, results = best_strategy\n",
    "        report_lines.append(f\"### Primary Recommendation: {name}\")\n",
    "        report_lines.append(f\"Use **{name}** as your primary chunking strategy for the JioPay RAG chatbot.\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Analyze strengths\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        strengths = []\n",
    "        if detailed.get('semantic_coherence', 0) > 0.8:\n",
    "            strengths.append(\"Excellent semantic coherence\")\n",
    "        if detailed.get('avg_precision_at_1', 0) > 0.7:\n",
    "            strengths.append(\"High precision retrieval\")\n",
    "        if detailed.get('topic_coverage', 0) > 0.8:\n",
    "            strengths.append(\"Comprehensive topic coverage\")\n",
    "        \n",
    "        if strengths:\n",
    "            report_lines.append(\"**Key Strengths:**\")\n",
    "            for strength in strengths:\n",
    "                report_lines.append(f\"- {strength}\")\n",
    "            report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"### Implementation Guidelines:\")\n",
    "    report_lines.append(\"- Monitor retrieval performance in production\")\n",
    "    report_lines.append(\"- Consider A/B testing with top 2-3 strategies\")\n",
    "    report_lines.append(\"- Regularly evaluate with domain-specific queries\")\n",
    "    report_lines.append(\"- Adjust chunk sizes based on embedding model performance\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Save report\n",
    "    with open(f\"{output_dir}/unified_evaluation_summary.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "\n",
    "def prepare_production_ready_chunks(self, output_dir: str):\n",
    "    \"\"\"Prepare best performing chunks for production deployment\"\"\"\n",
    "    if not self.evaluation_results:\n",
    "        return\n",
    "    \n",
    "    # Find best strategy\n",
    "    best_strategy = max(self.evaluation_results.items(), \n",
    "                      key=lambda x: x[1].get('final_score', 0))\n",
    "    \n",
    "    if not best_strategy:\n",
    "        return\n",
    "    \n",
    "    strategy_name, results = best_strategy\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    production_metadata = {\n",
    "        \"deployment_info\": {\n",
    "            \"best_strategy\": strategy_name,\n",
    "            \"evaluation_score\": results.get('final_score', 0),\n",
    "            \"evaluation_timestamp\": timestamp,\n",
    "            \"recommended_for_production\": True\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"rag_quality_score\": results.get('rag_quality', 0),\n",
    "            \"retrieval_performance\": results.get('retrieval_performance', 0),\n",
    "            \"size_optimization\": results.get('size_optimization', 0),\n",
    "            \"avg_latency_ms\": results.get('performance_benchmark', {}).get('mean_latency_ms', 0),\n",
    "            \"queries_per_second\": results.get('performance_benchmark', {}).get('queries_per_second', 0)\n",
    "        },\n",
    "        \"chunk_statistics\": results.get('basic_stats', {}),\n",
    "        \"detailed_metrics\": results.get('detailed_metrics', {}),\n",
    "        \"deployment_notes\": [\n",
    "            f\"Evaluated against {len(self.test_queries)} test queries\",\n",
    "            f\"Optimized for JioPay domain-specific content\",\n",
    "            f\"Balanced approach: 40% RAG quality, 35% retrieval, 15% size, 10% performance\",\n",
    "            f\"Ready for vector database ingestion and RAG deployment\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save production metadata\n",
    "    with open(f\"{output_dir}/production_deployment_metadata.json\", 'w') as f:\n",
    "        json.dump(production_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Production metadata prepared for: {strategy_name}\")\n",
    "    print(f\"   Score: {results.get('final_score', 0):.3f}\")\n",
    "    print(f\"   File: {output_dir}/production_deployment_metadata.json\")\n",
    "\n",
    "def generate_comprehensive_report(self, output_dir: str = \"results/unified_evaluation\"):\n",
    "    \"\"\"Generate comprehensive evaluation report with all metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not self.evaluation_results:\n",
    "        print(\"No evaluation results to report\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nGenerating comprehensive reports in {output_dir}...\")\n",
    "    \n",
    "    # 1. Save detailed results as JSON\n",
    "    with open(f\"{output_dir}/unified_evaluation_results.json\", 'w') as f:\n",
    "        json.dump(self.evaluation_results, f, indent=2, default=str)\n",
    "    \n",
    "    # 2. Generate performance comparison plots\n",
    "    self.generate_unified_plots(output_dir)\n",
    "    \n",
    "    # 3. Generate domain-specific analysis\n",
    "    self.generate_domain_analysis(output_dir)\n",
    "    \n",
    "    # 4. Generate retrieval analysis\n",
    "    self.generate_retrieval_analysis(output_dir)\n",
    "    \n",
    "    # 5. Generate comprehensive summary report\n",
    "    self.generate_unified_summary_report(output_dir)\n",
    "    \n",
    "    # 6. Prepare best chunks for production\n",
    "    self.prepare_production_ready_chunks(output_dir)\n",
    "    \n",
    "    print(\"Comprehensive reports generated successfully!\")\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.generate_unified_summary_report = generate_unified_summary_report\n",
    "UnifiedChunkingEvaluator.prepare_production_ready_chunks = prepare_production_ready_chunks\n",
    "UnifiedChunkingEvaluator.generate_comprehensive_report = generate_comprehensive_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbf7f8",
   "metadata": {},
   "source": [
    "## Data Loading and Strategy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ace62ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 642 chunks for Fixed_256_0\n",
      "Loaded 415 chunks for Fixed_512_64\n",
      "Loaded 262 chunks for Fixed_1024_128\n",
      "Loaded 1486 chunks for Semantic_High_Sim\n",
      "Loaded 1065 chunks for Semantic_Low_Sim\n",
      "Loaded 1320 chunks for Semantic_Med_Sim\n",
      "Loaded 368 chunks for Structural_Balanced\n",
      "Loaded 380 chunks for Structural_Hierarchical\n",
      "Loaded 489 chunks for Structural_Large\n",
      "Loaded 619 chunks for Recursive_Balanced\n",
      "Loaded 600 chunks for Recursive_Large\n",
      "Loaded 654 chunks for Recursive_Small\n",
      "Loaded 584 chunks for LLM_smallChunks\n",
      "Loaded 13 strategies successfully\n"
     ]
    }
   ],
   "source": [
    "# Define chunking strategies and their file paths\n",
    "strategies = {\n",
    "    'Fixed_256_0': r'Data\\chunks\\fixed\\Fixed_256_0.json',\n",
    "    'Fixed_512_64': r'Data\\chunks\\fixed\\Fixed_512_64.json',\n",
    "    'Fixed_1024_128': r'Data\\chunks\\fixed\\Fixed_1024_128.json',\n",
    "    'Semantic_High_Sim': r'Data\\chunks\\semantic\\Semantic_High_Sim.json',\n",
    "    'Semantic_Low_Sim': r'Data\\chunks\\semantic\\Semantic_Low_Sim.json',\n",
    "    'Semantic_Med_Sim': r'Data\\chunks\\semantic\\Semantic_Med_Sim.json',\n",
    "    'Structural_Balanced': r'Data\\chunks\\structural\\Structural_Balanced.json',\n",
    "    'Structural_Hierarchical': r'Data\\chunks\\structural\\Structural_Hierarchical.json',\n",
    "    'Structural_Large': r'Data\\chunks\\structural\\Structural_Large.json',\n",
    "    'Recursive_Balanced': r'Data\\chunks\\recursive\\Recursive_Balanced.json',\n",
    "    'Recursive_Large': r'Data\\chunks\\recursive\\Recursive_Large.json',\n",
    "    'Recursive_Small': r'Data\\chunks\\recursive\\Recursive_Small.json',\n",
    "    'LLM_smallChunks': r'Data\\chunks\\llm_based\\LLM_Small_Chunks.json'\n",
    "}\n",
    "\n",
    "# Load all chunking strategies data\n",
    "strategies_data = {}\n",
    "for strategy_name, file_path in strategies.items():\n",
    "    chunks = evaluator.load_chunked_data(strategy_name, file_path)\n",
    "    if chunks:\n",
    "        strategies_data[strategy_name] = chunks\n",
    "    else:\n",
    "        print(f\"Warning: No data loaded for {strategy_name}\")\n",
    "\n",
    "print(f\"Loaded {len(strategies_data)} strategies successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847a5ed",
   "metadata": {},
   "source": [
    "## Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e879792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation...\n",
      "\n",
      "Running Unified Chunking Strategy Comparison...\n",
      "======================================================================\n",
      "\n",
      "Processing Fixed_256_0...\n",
      "Evaluating Fixed_256_0...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Fixed_512_64...\n",
      "Evaluating Fixed_512_64...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Fixed_1024_128...\n",
      "Evaluating Fixed_1024_128...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Semantic_High_Sim...\n",
      "Evaluating Semantic_High_Sim...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Semantic_Low_Sim...\n",
      "Evaluating Semantic_Low_Sim...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Semantic_Med_Sim...\n",
      "Evaluating Semantic_Med_Sim...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Structural_Balanced...\n",
      "Evaluating Structural_Balanced...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Structural_Hierarchical...\n",
      "Evaluating Structural_Hierarchical...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Structural_Large...\n",
      "Evaluating Structural_Large...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Recursive_Balanced...\n",
      "Evaluating Recursive_Balanced...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Recursive_Large...\n",
      "Evaluating Recursive_Large...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing Recursive_Small...\n",
      "Evaluating Recursive_Small...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "Processing LLM_smallChunks...\n",
      "Evaluating LLM_smallChunks...\n",
      "  Evaluating domain-specific RAG metrics...\n",
      "  Evaluating retrieval performance...\n",
      "  Evaluating size optimization...\n",
      "  Benchmarking performance...\n",
      "\n",
      "================================================================================\n",
      "UNIFIED CHUNKING STRATEGY EVALUATION RESULTS\n",
      "================================================================================\n",
      "               Strategy Final_Score RAG_Quality Retrieval_Perf Size_Opt Performance  Total_Chunks Avg_Tokens Semantic_Coh Context_Comp Info_Density Topic_Cov Precision@1 Recall@5   MRR Latency_ms  QPS\n",
      "            Fixed_256_0       0.704       0.641          0.843    0.970       0.001           642      219.9        0.252        0.859        0.454     1.000       0.733    0.944 0.856     104.74  9.5\n",
      "           Fixed_512_64       0.695       0.634          0.860    0.826       0.001           415      382.8        0.242        0.847        0.446     1.000       0.800    0.928 0.872     115.55  8.7\n",
      "         Fixed_1024_128       0.686       0.636          0.854    0.722       0.001           262      599.0        0.277        0.827        0.441     1.000       0.800    0.906 0.883     122.73  8.1\n",
      "      Semantic_High_Sim       0.680       0.647          0.799    0.866       0.001          1486       93.3        0.459        0.707        0.420     1.000       0.667    0.944 0.811     116.44  8.6\n",
      "       Semantic_Low_Sim       0.681       0.644          0.801    0.880       0.001          1065      130.2        0.429        0.735        0.412     1.000       0.667    0.939 0.811     112.28  8.9\n",
      "       Semantic_Med_Sim       0.692       0.647          0.828    0.875       0.001          1320      105.1        0.448        0.722        0.418     1.000       0.733    0.922 0.856     112.17  8.9\n",
      "    Structural_Balanced       0.724       0.671          0.902    0.758       0.001           368      235.1        0.356        0.720        0.608     1.000       0.867    0.967 0.922      90.56 11.0\n",
      "Structural_Hierarchical       0.725       0.670          0.906    0.766       0.001           380      227.7        0.348        0.726        0.606     1.000       0.867    0.967 0.922      94.61 10.6\n",
      "       Structural_Large       0.710       0.658          0.889    0.723       0.001           489      175.8        0.413        0.624        0.595     1.000       0.867    0.944 0.933      84.87 11.8\n",
      "     Recursive_Balanced       0.705       0.666          0.856    0.787       0.001           619      158.3        0.348        0.700        0.619     1.000       0.800    0.939 0.900     104.62  9.6\n",
      "        Recursive_Large       0.706       0.668          0.856    0.784       0.001           600      163.3        0.350        0.694        0.628     1.000       0.800    0.939 0.900     106.44  9.4\n",
      "        Recursive_Small       0.708       0.668          0.856    0.814       0.001           654      150.0        0.348        0.710        0.614     1.000       0.800    0.939 0.900     100.41 10.0\n",
      "        LLM_smallChunks       0.741       0.680          0.925    0.815       0.002           584      107.1        0.477        0.702        0.542     1.000       0.867    0.967 0.933      69.10 14.5\n",
      "\n",
      "Results saved to: results/unified_evaluation/unified_strategy_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Run unified comprehensive comparison\n",
    "if strategies_data:\n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    comparison_df = evaluator.compare_strategies(strategies_data)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"UNIFIED CHUNKING STRATEGY EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_df.to_csv('results/unified_evaluation/unified_strategy_comparison.csv', index=False)\n",
    "    print(\"\\nResults saved to: results/unified_evaluation/unified_strategy_comparison.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"No chunking data loaded. Please ensure chunk files exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c4b7b",
   "metadata": {},
   "source": [
    "## Analyze Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790465c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating comprehensive reports in results/unified_evaluation...\n",
      "Production metadata prepared for: LLM_smallChunks\n",
      "   Score: 0.741\n",
      "   File: results/unified_evaluation/production_deployment_metadata.json\n",
      "Comprehensive reports generated successfully!\n",
      "\n",
      "BEST STRATEGY: LLM_smallChunks\n",
      "Final Score: 0.741\n",
      "RAG Quality: 0.680\n",
      "Retrieval Performance: 0.925\n",
      "\n",
      "Reports generated in: results/unified_evaluation/\n",
      "Evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive reports\n",
    "if evaluator.evaluation_results:\n",
    "    evaluator.generate_comprehensive_report()\n",
    "    \n",
    "    # Find and display best strategy\n",
    "    best_strategy = max(evaluator.evaluation_results.items(), \n",
    "                      key=lambda x: x[1].get('final_score', 0))\n",
    "    \n",
    "    if best_strategy:\n",
    "        name, results = best_strategy\n",
    "        print(f\"\\nBEST STRATEGY: {name}\")\n",
    "        print(f\"Final Score: {results.get('final_score', 0):.3f}\")\n",
    "        print(f\"RAG Quality: {results.get('rag_quality', 0):.3f}\")\n",
    "        print(f\"Retrieval Performance: {results.get('retrieval_performance', 0):.3f}\")\n",
    "    \n",
    "    print(f\"\\nReports generated in: results/unified_evaluation/\")\n",
    "    print(\"Evaluation completed successfully!\")\n",
    "else:\n",
    "    print(\"No evaluation results available for report generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e649cde",
   "metadata": {},
   "source": [
    "##  Generate Comprehensive Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67291e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain_analysis(self, output_dir: str):\n",
    "    \"\"\"Generate domain-specific analysis\"\"\"\n",
    "    # Analyze domain coverage by strategy\n",
    "    domain_analysis = defaultdict(dict)\n",
    "    \n",
    "    for strategy_name, results in self.evaluation_results.items():\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        domain_analysis[strategy_name] = {\n",
    "            'Semantic Coherence': detailed.get('semantic_coherence', 0),\n",
    "            'Context Completeness': detailed.get('context_completeness', 0),\n",
    "            'Information Density': detailed.get('information_density', 0),\n",
    "            'Topic Coverage': detailed.get('topic_coverage', 0),\n",
    "            'FAQ Grouping': detailed.get('faq_grouping', 0)\n",
    "        }\n",
    "    \n",
    "    # Create domain analysis DataFrame\n",
    "    domain_df = pd.DataFrame(domain_analysis).T\n",
    "    domain_df.to_csv(f\"{output_dir}/domain_specific_analysis.csv\")\n",
    "    \n",
    "    # Create domain performance heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(domain_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "               cbar_kws={'label': 'Score'})\n",
    "    plt.title('Domain-Specific Performance Analysis', fontweight='bold')\n",
    "    plt.ylabel('Strategy')\n",
    "    plt.xlabel('Domain Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/domain_performance_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_retrieval_analysis(self, output_dir: str):\n",
    "    \"\"\"Generate retrieval performance analysis\"\"\"\n",
    "    # Analyze retrieval metrics by category\n",
    "    category_analysis = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for strategy_name, results in self.evaluation_results.items():\n",
    "        retrieval_metrics = results.get('retrieval_metrics', [])\n",
    "        for metric in retrieval_metrics:\n",
    "            category = metric['category']\n",
    "            category_analysis[category]['precision_at_1'].append(metric['precision_at_1'])\n",
    "            category_analysis[category]['recall_at_k'].append(metric['recall_at_k'])\n",
    "            category_analysis[category]['mrr'].append(metric['mrr'])\n",
    "    \n",
    "    # Create category performance summary\n",
    "    category_summary = []\n",
    "    for category, metrics in category_analysis.items():\n",
    "        category_summary.append({\n",
    "            'Category': category,\n",
    "            'Avg_Precision@1': np.mean(metrics['precision_at_1']),\n",
    "            'Avg_Recall@5': np.mean(metrics['recall_at_k']),\n",
    "            'Avg_MRR': np.mean(metrics['mrr']),\n",
    "            'Std_Precision@1': np.std(metrics['precision_at_1']),\n",
    "            'Query_Count': len(metrics['precision_at_1'])\n",
    "        })\n",
    "    \n",
    "    category_df = pd.DataFrame(category_summary)\n",
    "    category_df.to_csv(f\"{output_dir}/retrieval_by_category.csv\", index=False)\n",
    "    \n",
    "    # Create category performance plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    categories = category_df['Category']\n",
    "    metrics_to_plot = ['Avg_Precision@1', 'Avg_Recall@5', 'Avg_MRR']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        axes[i].bar(categories, category_df[metric])\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].set_ylabel('Score')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Retrieval Performance by Query Category', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/retrieval_by_category.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Add these methods to the evaluator class\n",
    "UnifiedChunkingEvaluator.generate_domain_analysis = generate_domain_analysis\n",
    "UnifiedChunkingEvaluator.generate_retrieval_analysis = generate_retrieval_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbd594",
   "metadata": {},
   "source": [
    "## Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722d7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance monitoring functions ready!\n",
      "Use monitor_strategy_performance() to view dashboard\n",
      "Use export_results_summary() to create executive summary\n"
     ]
    }
   ],
   "source": [
    "def monitor_strategy_performance():\n",
    "    \"\"\"Monitor and display key performance indicators\"\"\"\n",
    "    if not evaluator.evaluation_results:\n",
    "        print(\"No evaluation results available\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== PERFORMANCE MONITORING DASHBOARD ===\")\n",
    "    \n",
    "    # Overall rankings\n",
    "    rankings = sorted(evaluator.evaluation_results.items(), \n",
    "                     key=lambda x: x[1].get('final_score', 0), reverse=True)\n",
    "    \n",
    "    print(\"\\nStrategy Rankings:\")\n",
    "    for i, (strategy, results) in enumerate(rankings, 1):\n",
    "        score = results.get('final_score', 0)\n",
    "        status = \"ðŸŸ¢\" if score > 0.7 else \"ðŸŸ¡\" if score > 0.5 else \"ðŸ”´\"\n",
    "        print(f\"{i:2d}. {status} {strategy:<20} {score:.3f}\")\n",
    "    \n",
    "    # Performance alerts\n",
    "    print(\"\\n=== PERFORMANCE ALERTS ===\")\n",
    "    alerts = []\n",
    "    \n",
    "    for strategy, results in evaluator.evaluation_results.items():\n",
    "        basic_stats = results.get('basic_stats', {})\n",
    "        detailed = results.get('detailed_metrics', {})\n",
    "        performance = results.get('performance_benchmark', {})\n",
    "        \n",
    "        # Check for issues\n",
    "        if basic_stats.get('avg_tokens', 0) < 50:\n",
    "            alerts.append(f\"âš ï¸  {strategy}: Chunks too small (avg {basic_stats.get('avg_tokens', 0):.0f} tokens)\")\n",
    "        \n",
    "        if basic_stats.get('avg_tokens', 0) > 800:\n",
    "            alerts.append(f\"âš ï¸  {strategy}: Chunks too large (avg {basic_stats.get('avg_tokens', 0):.0f} tokens)\")\n",
    "        \n",
    "        if detailed.get('semantic_coherence', 0) < 0.5:\n",
    "            alerts.append(f\"âš ï¸  {strategy}: Low semantic coherence ({detailed.get('semantic_coherence', 0):.3f})\")\n",
    "        \n",
    "        if detailed.get('avg_precision_at_1', 0) < 0.3:\n",
    "            alerts.append(f\"âš ï¸  {strategy}: Poor retrieval precision ({detailed.get('avg_precision_at_1', 0):.3f})\")\n",
    "        \n",
    "        if performance.get('mean_latency_ms', 0) > 100:\n",
    "            alerts.append(f\"âš ï¸  {strategy}: High latency ({performance.get('mean_latency_ms', 0):.1f}ms)\")\n",
    "    \n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            print(alert)\n",
    "    else:\n",
    "        print(\"âœ… No performance issues detected\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    best_strategy = rankings[0] if rankings else None\n",
    "    \n",
    "    if best_strategy:\n",
    "        name, results = best_strategy\n",
    "        print(f\"âœ… Recommended Strategy: {name}\")\n",
    "        print(f\"   Final Score: {results.get('final_score', 0):.3f}\")\n",
    "        print(f\"   Ready for production deployment\")\n",
    "        \n",
    "        # Check if score is concerning\n",
    "        if results.get('final_score', 0) < 0.6:\n",
    "            print(\"âš ï¸  Note: Best strategy score is below 0.6 - consider strategy refinement\")\n",
    "\n",
    "def export_results_summary():\n",
    "    \"\"\"Export a concise summary for stakeholders\"\"\"\n",
    "    if not evaluator.evaluation_results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for strategy, results in evaluator.evaluation_results.items():\n",
    "        basic_stats = results.get('basic_stats', {})\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Strategy': strategy,\n",
    "            'Final_Score': round(results.get('final_score', 0), 3),\n",
    "            'RAG_Quality': round(results.get('rag_quality', 0), 3),\n",
    "            'Retrieval_Performance': round(results.get('retrieval_performance', 0), 3),\n",
    "            'Total_Chunks': basic_stats.get('total_chunks', 0),\n",
    "            'Avg_Tokens': round(basic_stats.get('avg_tokens', 0), 1),\n",
    "            'Recommendation': 'Recommended' if results.get('final_score', 0) == max([r.get('final_score', 0) for r in evaluator.evaluation_results.values()]) else 'Not Recommended'\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Final_Score', ascending=False)\n",
    "    \n",
    "    # Export to CSV\n",
    "    summary_df.to_csv('results/unified_evaluation/executive_summary.csv', index=False)\n",
    "    print(\"Executive summary exported to: results/unified_evaluation/executive_summary.csv\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "print(\"Performance monitoring functions ready!\")\n",
    "print(\"Use monitor_strategy_performance() to view dashboard\")\n",
    "print(\"Use export_results_summary() to create executive summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc96f73",
   "metadata": {},
   "source": [
    "## Final Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c40e92ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete evaluation pipeline ready!\n",
      "Run run_complete_evaluation() to execute the entire pipeline\n",
      "\n",
      "Or execute individual cells for step-by-step analysis:\n",
      "- Cell 13: Load data\n",
      "- Cell 14: Run evaluation\n",
      "- Cell 15: Analyze chunk sizes\n",
      "- Cell 16: Generate reports\n",
      "- Cell 17-19: Interactive analysis\n"
     ]
    }
   ],
   "source": [
    "def run_complete_evaluation():\n",
    "    \"\"\"Run the complete evaluation pipeline\"\"\"\n",
    "    print(\"Starting complete evaluation pipeline...\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"Step 1: Loading strategy data...\")\n",
    "    if not strategies_data:\n",
    "        print(\"âŒ No data loaded. Please run Cell 13 first.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Run evaluation\n",
    "    print(\"Step 2: Running comprehensive evaluation...\")\n",
    "    comparison_df = evaluator.compare_strategies(strategies_data)\n",
    "    \n",
    "    # Step 3: Analyze results\n",
    "    print(\"Step 3: Analyzing chunk sizes...\")\n",
    "    size_analysis_df = evaluator.analyze_chunk_sizes(strategies_data)\n",
    "    \n",
    "    # Step 4: Generate reports\n",
    "    print(\"Step 4: Generating comprehensive reports...\")\n",
    "    evaluator.generate_comprehensive_report()\n",
    "    \n",
    "    # Step 5: Performance monitoring\n",
    "    print(\"Step 5: Performance monitoring...\")\n",
    "    monitor_strategy_performance()\n",
    "    \n",
    "    # Step 6: Export summary\n",
    "    print(\"Step 6: Exporting executive summary...\")\n",
    "    summary_df = export_results_summary()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION PIPELINE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display key results\n",
    "    print(\"\\nTOP 3 STRATEGIES:\")\n",
    "    top_strategies = summary_df.head(3)\n",
    "    for idx, row in top_strategies.iterrows():\n",
    "        print(f\"{row.name + 1}. {row['Strategy']}: {row['Final_Score']}\")\n",
    "    \n",
    "    print(f\"\\nAll results saved in: results/unified_evaluation/\")\n",
    "    print(\"âœ… Pipeline completed successfully!\")\n",
    "\n",
    "# Optional: Run everything at once\n",
    "# run_complete_evaluation()\n",
    "\n",
    "print(\"Complete evaluation pipeline ready!\")\n",
    "print(\"Run run_complete_evaluation() to execute the entire pipeline\")\n",
    "print(\"\\nOr execute individual cells for step-by-step analysis:\")\n",
    "print(\"- Cell 13: Load data\")\n",
    "print(\"- Cell 14: Run evaluation\") \n",
    "print(\"- Cell 15: Analyze chunk sizes\")\n",
    "print(\"- Cell 16: Generate reports\")\n",
    "print(\"- Cell 17-19: Interactive analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacf7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
