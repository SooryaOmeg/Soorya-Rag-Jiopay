\begin{abstract}
	This study presents a comprehensive evaluation framework for knowledge-intensive question answering systems, integrating document chunking, embedding generation, and retrieval-augmented generation (RAG). We compared multiple chunking strategies and identified Structural Balanced and Structural Hierarchical approaches as optimal for maintaining retrieval quality and index efficiency. Three embedding models—Qwen3-0.6B, Gemma-300M, and Voyage-3.5—were benchmarked, assessing recall, precision, MRR, latency, and index size. Subsequently, RAG evaluation with top-k retrieval, rerankers, and guardrails was conducted to measure answer quality, semantic similarity, faithfulness, and relevance. The results highlight the trade-offs between embedding quality, computational performance, and retrieval accuracy, providing actionable insights for system design and deployment. This work establishes a robust methodology for systematically evaluating and optimizing retrieval and generation pipelines in large-scale knowledge-based applications.
\end{abstract}

\begin{IEEEkeywords}
	RAG, Chunking Strategy, Embedding Models, Recall@k, Precision@k, MRR, Latency, Index Size, Top-k Retrieval, Reranker, Guardrails, Semantic Similarity, Faithfulness, Completeness.
\end{IEEEkeywords}